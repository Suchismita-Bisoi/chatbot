{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c6ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"data.pdf\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8766a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
      "     ---------------------------------------- 0.0/303.4 kB ? eta -:--:--\n",
      "     -------------------------------------  297.0/303.4 kB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 303.4/303.4 kB 6.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=4.0 in d:\\ai projects\\rag baesd chatbot\\myenv\\lib\\site-packages (from pypdf) (4.13.2)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15b7161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 0, 'page_label': '1'}, page_content=''),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 1, 'page_label': '2'}, page_content='Artificial Intelligence  \\n  i \\nAbout the T utorial \\nThis tutorial provides introductory knowledge on Artificial Intelligence. It would come to a \\ngreat help if you are about to select Artificial Intelligence as a course subject. You can briefly \\nknow about the areas of AI in which research is prospering.  \\n \\n \\nAudience \\nThis tutorial is prepared for the students at beginner level who aspire to learn Artificial \\nIntelligence.  \\n \\n \\nPrerequisites \\nThe basic knowledge of Computer Science is mandatory. The knowledge of Mathematics, \\nLanguages, Science, Mechanical or Electrical engineering is a plus.  \\n \\nDisclaimer & Copyright \\n\\uf0e3 Copyright 2015 by Tutorials Point (I) Pvt. Ltd.  \\nAll the content and graphics published in this e -book are the property of Tutorials Point (I) \\nPvt. Ltd. The user of this e -book is prohibited to reuse, retain, copy, distribute or republish \\nany contents or a part of contents of this e -book in any manner without wri tten consent of \\nthe publisher.  \\nWe strive to update the contents of our website and tutorials as timely and as precisely as  \\npossible, however, the contents may contain inaccuracies or errors. Tutorials Point (I) Pvt. \\nLtd. provides no guarantee regarding the accuracy, timeliness or completeness of our website \\nor its contents including this tutorial. If you discover any errors o n our website or in this \\ntutorial, please notify us at contact@tutorialspoint.com.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 2, 'page_label': '3'}, page_content='Artificial Intelligence  \\n  ii \\nT able of Contents  \\nAbout the Tutorial ......................................................................................................................................... i \\nAudience ....................................................................................................................................................... i \\nPrerequisites ................................................................................................................................................. i \\nDisclaimer & Copyright .................................................................................................................................. i \\nTable of Contents ......................................................................................................................................... ii \\n1. OVERVIEW OF AI ................................ ................................ ................................ ...................... 1 \\nWhat is Artificial Intelligence? ...................................................................................................................... 1 \\nPhilosophy of AI ........................................................................................................................................... 1 \\nGoals of AI .................................................................................................................................................... 1 \\nWhat Contributes to AI? ............................................................................................................................... 2 \\nProgramming Without and With AI .............................................................................................................. 2 \\nWhat is AI Technique? .................................................................................................................................. 3 \\nApplications of AI ......................................................................................................................................... 3 \\nHistory of AI ................................................................................................................................................. 4 \\n2. INTELLIGENT SYSTEMS ................................ ................................ ................................ ............. 6 \\nWhat is Intelligence? .................................................................................................................................... 6 \\nTypes of Intelligence..................................................................................................................................... 6 \\nWhat is Intelligence Composed of? .............................................................................................................. 7 \\nDifference between Human and Machine Intelligence ................................................................................. 9 \\n3. RESEARCH AREAS OF AI ................................ ................................ ................................ .......... 10 \\nReal Life Applications of Research Areas .................................................................................................... 11 \\nTask Classification of AI .............................................................................................................................. 12'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 3, 'page_label': '4'}, page_content='Artificial Intelligence  \\n  iii \\n4. AGENTS AND ENVIRONMENTS ................................ ................................ ...............................  14 \\nWhat are Agent and Environment? ............................................................................................................ 14 \\nAgents Terminology ................................................................................................................................... 14 \\nRationality .................................................................................................................................................. 15 \\nWhat is Ideal Rational Agent? .................................................................................................................... 15 \\nThe Structure of Intelligent Agents ............................................................................................................. 15 \\nThe Nature of Environments ...................................................................................................................... 18 \\nProperties of Environment ......................................................................................................................... 19 \\n5. POPULAR SEARCH ALGORITHMS ................................ ................................ ............................  20 \\nSingle Agent Pathfinding Problems............................................................................................................. 20 \\nSearch Terminology .................................................................................................................................... 20 \\nBrute-Force Search Strategies .................................................................................................................... 20 \\nInformed (Heuristic) Search Strategies ....................................................................................................... 23 \\nLocal Search Algorithms ............................................................................................................................. 24 \\n6. FUZZY LOGIC SYSTEMS ................................ ................................ ................................ ........... 27 \\nWhat is Fuzzy Logic? ................................................................................................................................... 27 \\nWhy Fuzzy Logic? ........................................................................................................................................ 27 \\nFuzzy Logic Systems Architecture ............................................................................................................... 28 \\nExample of a Fuzzy Logic System ................................................................................................................ 29 \\nApplication Areas of Fuzzy Logic ................................................................................................................. 32 \\nAdvantages of FLSs ..................................................................................................................................... 33 \\nDisadvantages of FLSs ................................................................................................................................ 33'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 4, 'page_label': '5'}, page_content='Artificial Intelligence  \\n  iv \\n7. NATURAL LANGUAGE PROCESSING ................................ ................................ ........................  34 \\nComponents of NLP .................................................................................................................................... 34 \\nDifficulties in NLU ....................................................................................................................................... 34 \\nNLP Terminology ........................................................................................................................................ 35 \\nSteps in NLP................................................................................................................................................ 35 \\nImplementation Aspects of Syntactic Analysis............................................................................................ 36 \\n8. EXPERT SYSTEMS................................ ................................ ................................ .................... 40 \\nWhat are Expert Systems? .......................................................................................................................... 40 \\nCapabilities of Expert Systems .................................................................................................................... 40 \\nComponents of Expert Systems .................................................................................................................. 41 \\nKnowledge Base ......................................................................................................................................... 41 \\nInference Engine ......................................................................................................................................... 42 \\nUser Interface ............................................................................................................................................. 43 \\nExpert Systems Limitations......................................................................................................................... 44 \\nApplications of Expert System .................................................................................................................... 44 \\nExpert System Technology .......................................................................................................................... 45 \\nDevelopment of Expert Systems: General Steps ......................................................................................... 45 \\nBenefits of Expert Systems ......................................................................................................................... 46 \\n9. ROBOTICS ................................ ................................ ................................ ..............................  47 \\nWhat are Robots? ....................................................................................................................................... 47 \\nWhat is Robotics? ....................................................................................................................................... 47 \\nDifference in Robot System and Other AI Program ..................................................................................... 47 \\nRobot Locomotion ...................................................................................................................................... 48 \\nComponents of a Robot .............................................................................................................................. 50'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 5, 'page_label': '6'}, page_content='Artificial Intelligence  \\n  v \\nComputer Vision ......................................................................................................................................... 50 \\nTasks of Computer Vision ........................................................................................................................... 50 \\nApplication Domains of Computer Vision ................................................................................................... 51 \\nApplications of Robotics ............................................................................................................................. 51 \\n10. NEURAL NETWORKS ................................ ................................ ................................ ............... 53 \\nWhat are Artificial Neural Networks (ANNs)? ............................................................................................. 53 \\nBasic Structure of ANNs .............................................................................................................................. 53 \\nTypes of Artificial Neural Networks ............................................................................................................ 54 \\nWorking of ANNs ........................................................................................................................................ 55 \\nMachine Learning in ANNs ......................................................................................................................... 55 \\nBayesian Networks (BN) ............................................................................................................................. 56 \\nApplications of Neural Networks ................................................................................................................ 59 \\n11. AI ISSUES ................................ ................................ ................................ ................................  61 \\n12. AI TERMINOLOGY ................................ ................................ ................................ ................... 62'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 6, 'page_label': '7'}, page_content='Artificial Intelligence  \\n  1 \\nSince the invention of computers or machines, their capability to perform various tasks went \\non growing exponentially. Humans have developed the power of computer systems in terms \\nof their diverse working domains, their increasing speed, and reducing size with respect to \\ntime.  \\nA branch of Computer Science named Artificial Intelligence pursues creating the computers or \\nmachines as intelligent as human beings. \\nWhat is Artificial Intelligence? \\nAccording to  the father of A rtificial Intelligence John McCarthy , it is “The science and \\nengineering of making intelligent machines, especially intelligent computer programs”. \\nArtificial Intelligence is a way of making a computer, a computer-controlled robot, or a \\nsoftware think intelligently, in the similar manner the intelligent humans think.  \\nAI is accomplished by studying how human brain thinks, and how humans learn, decide, and \\nwork while trying to solve a problem, and then using the outcomes of this study as a basis of \\ndeveloping intelligent software and systems. \\nPhilosophy of AI \\nWhile exploiting the power of the computer systems, the  curiosity of human, lead him to \\nwonder, “Can a machine think and behave like humans do?”  \\nThus, the development of AI started with the intention of creating similar intelligence in \\nmachines that we find and regard high in humans.    \\nGoals of AI \\n\\uf0b7 To Create Expert Systems: The systems whi ch exhibit intelligent behavior, learn, \\ndemonstrate, explain, and advice its users. \\n\\uf0b7 To Implement Human Intelligence  in Machines : Creating systems that \\nunderstand, think, learn, and behave like humans. \\n \\n \\n1. Overview of AI'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 7, 'page_label': '8'}, page_content='Artificial Intelligence  \\n  2 \\nWhat Contributes to AI? \\nArtificial intelligence is a science and technology based on disciplines such as Computer \\nScience, Biology, Psychology, Linguistics, Mathematics, and Engineering. A major thrust of AI \\nis in the development of computer function s associated with human intelligence, such as \\nreasoning, learning, and problem solving. \\nOut of the following areas, one or multiple areas can contribute to build an intelligent system. \\n \\nProgramming Without and With AI  \\nThe programming without and with AI is different in following ways: \\nProgramming Without AI Programming With AI \\nA computer program without AI can \\nanswer the specific questions it is meant \\nto solve. \\nA computer program with  AI can answer the \\ngeneric questions it is meant to solve. \\nModification in the program leads to \\nchange in its structure. \\nAI programs can absorb new modifications by \\nputting highly independent pieces of \\ninformation together. Hence you can modify \\neven a minute piece of information of program \\nwithout affecting its structure. \\nModification is not quick  and easy. It may \\nlead to affecting the program adversely. Quick and Easy program modification.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 8, 'page_label': '9'}, page_content='Artificial Intelligence  \\n  3 \\nWhat is AI T echnique? \\nIn the real world, the knowledge has some unwelcomed properties: \\n\\uf0b7 Its volume is huge, next to unimaginable. \\n\\uf0b7 It is not well-organized or well-formatted. \\n\\uf0b7 It keeps changing constantly. \\nAI Technique is a manner to organize and use the knowledge efficiently in such a way that: \\n\\uf0b7 It should be perceivable by the people who provide it. \\n\\uf0b7 It should be easily modifiable to correct errors. \\n\\uf0b7 It should be useful in many situations though it is incomplete or inaccurate. \\nAI techniques elevate the speed of execution of the complex program it is equipped with.  \\nApplications of AI \\nAI has been dominant in various fields such as: \\n\\uf0b7 Gaming \\nAI plays crucial role in strategic games such as chess, poker, tic -tac-toe, etc., where \\nmachine can think of large number of possible positions based on heuristic knowledge.   \\n\\uf0b7 Natural Language Processing \\nIt is possible to interact with the computer that understa nds natural language spoken \\nby humans.  \\n\\uf0b7 Expert Systems \\nThere are some applications which integrate machine, software , and special \\ninformation to impart reasoning and advising. They provide explanation and advice to \\nthe users.  \\n\\uf0b7 Vision Systems \\nThese systems understand, interpret, and comprehend visual input on the computer . \\nFor example,  \\no A s pying aeroplane takes photographs which are used to figure out spatial \\ninformation or map of the areas. \\no Doctors use clinical expert system to diagnose the patient. \\no Police use computer software that can recognize the face of criminal with the \\nstored portrait made by forensic artist.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 9, 'page_label': '10'}, page_content='Artificial Intelligence  \\n  4 \\n\\uf0b7 Speech Recognition \\nSome intelligent systems are capable of hearing and comprehending the language in \\nterms of sentences and their meanings while a human talks to it. It can handle different \\naccents, slang words, noise in the background, change in human’s noise due to cold, \\netc.  \\n \\n\\uf0b7 Handwriting Recognition \\nThe handwriting recognition software reads the text written on paper by a pen or on \\nscreen by a stylus. It can recognize the shapes of the letters and convert it into editable \\ntext. \\n \\n\\uf0b7 Intelligent Robots \\nRobots are able to perform t he tasks given by a human. They have sensors to detect \\nphysical data from the real world such as light, heat, temperature, movement, sound, \\nbump, and pressure . They have efficient processors, multiple sensors and huge \\nmemory, to exhibit i ntelligence. In addition, they are capable of learning from their \\nmistakes and they can adapt to the new environment. \\nHistory of AI \\nHere is the history of AI during 20th century:  \\nYear Milestone / Innovation \\n1923 Karel Čapek’s play named “Rossum\\'s Universal Robots” (RUR) opens in London, \\nfirst use of the word \"robot\" in English. \\n1943 Foundations for neural networks laid. \\n1945 Isaac Asimov, a Columbia University alumni, coined the term Robotics. \\n1950 \\nAlan Turing introduced Turing Test for evaluation of intelligence and published \\nComputing Machinery and Intelligence.  Claude Shannon published Detailed \\nAnalysis of Chess Playing as a search. \\n1956 John McCarthy coined the term Artificial Intelligence. Demonstration of the first \\nrunning AI program at Carnegie Mellon University. \\n1958 John McCarthy invents LISP programming language for AI. \\n1964 Danny Bobrow\\'s dissertation at MIT showed that computers can understand \\nnatural language well enough to solve algebra word problems correctly. \\n1965 Joseph Weizenbaum at MIT built ELIZA, an interactive problem that carries on \\na dialogue in English. \\n1969 Scientists at Stanford Research Institute Developed Shakey, a robot, equipped \\nwith locomotion, perception, and problem solving.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 10, 'page_label': '11'}, page_content='Artificial Intelligence  \\n  5 \\n1973 The Assembly Robotics group at Edinburgh University built Freddy, the Famous \\nScottish Robot, capable of using vision to locate and assemble models. \\n1979 The first computer-controlled autonomous vehicle, Stanford Cart, was built. \\n1985 \\n Harold Cohen created and demonstrated the drawing program, Aaron. \\n1990 \\nMajor advances in all areas of AI:  \\n\\uf0b7 Significant demonstrations in machine learning  \\n\\uf0b7 Case-based reasoning  \\n\\uf0b7 Multi-agent planning  \\n\\uf0b7 Scheduling  \\n\\uf0b7 Data mining, Web Crawler \\n\\uf0b7 natural language understanding and translation \\n\\uf0b7 Vision, Virtual Reality  \\n\\uf0b7 Games \\n1997 The Deep Blue Chess Program beats the then world chess champion, Garry \\nKasparov. \\n2000 \\nInteractive robot pets become commercially available. MIT displays Kismet, a \\nrobot with a face that expresses emotion s. The robot Nomad explores remote \\nregions of Antarctica and locates meteorites.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 11, 'page_label': '12'}, page_content='Artificial Intelligence  \\n  6 \\nWhile studying artificially intelligence, you need to know what intelligence  is. This chapter \\ncovers Idea of intelligence, types, and components of intelligence.  \\nWhat is Intelligence? \\nThe ability of a system to calculate, reason, perceive relationships and analogies, learn from \\nexperience, store and retrieve information from memory, solve problems, comprehend \\ncomplex ideas, use natural language fluently, classify, generalize, and adapt new situations. \\nT ypes of Intelligence \\nAs described by Howard Gardner, an American developmental psychologist, the Intelligence \\ncomes in multifold: \\nIntelligence Description Example \\nLinguistic \\nintelligence \\nThe ability to speak, recognize, and use mechanisms \\nof phonology (speech sounds), syntax (grammar), \\nand semantics (meaning). \\nNarrators, Orators \\nMusical intelligence \\nThe ability to create, communicate  with, and \\nunderstand meanings made of sound, understanding \\nof pitch, rhythm. \\nMusicians, \\nSingers, \\nComposers \\nLogical-\\nmathematical \\nintelligence \\nThe ability of use and understand relationships in the \\nabsence of action or objects. Understanding complex \\nand abstract ideas. \\nMathematicians, \\nScientists \\nSpatial intelligence \\nThe ability to perceive visual or spatial information, \\nchange it, and re -create visual images without \\nreference to the objects,  construct 3D images, and \\nto move and rotate them. \\nMap readers , \\nAstronauts, \\nPhysicists \\nBodily-Kinesthetic \\nintelligence \\nThe ability to use complete or part of the  body to \\nsolve problems or fashion products, control over fine \\nand coarse motor skills, and manipulate the objects. \\nPlayers, Dancers \\nIntra-personal \\nintelligence \\nThe ability to distinguish among one’s own feelings, \\nintentions, and motivations. Gautam Buddha \\n2. IntelligenT Systems'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 12, 'page_label': '13'}, page_content='Artificial Intelligence  \\n  7 \\nInterpersonal \\nintelligence \\nThe ability to recognize and make distinctions among \\nother people’s feelings, beliefs, and intentions. \\nMass \\nCommunicators, \\nInterviewers \\n \\nYou can say a machine or a system is artificially intelligent when it is equipped with at least \\none and at most all intelligences in it.  \\nWhat is Intelligence Composed of? \\nThe intelligence is intangible. It is composed of: \\n1. Reasoning \\n2. Learning \\n3. Problem Solving \\n4. Perception \\n5. Linguistic Intelligence \\n \\n \\nLet us go through all the components briefly: \\n1. Reasoning: It is the set of processes that enables us to provide basis for judgement, \\nmaking decisions, and prediction. There are broadly two types: \\nInductive Reasoning Deductive Reasoning \\nIt conducts specific observations to makes \\nbroad general statements. \\nIt starts with a general statement and \\nexamines the possibilities to reach a specific, \\nlogical conclusion. \\nEven if all of the premises are true in a \\nstatement, inductive reasoning allows for the \\nconclusion to be false. \\nIf something is true of a class of things in \\ngeneral, it is also true for all members of that \\nclass.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 13, 'page_label': '14'}, page_content='Artificial Intelligence  \\n  8 \\nExample: \\n“Nita is a teacher.  \\nAll teachers are studious.  \\nTherefore, Nita is studious.” \\nExample:  \\n\"All women of age above 60 years  are \\ngrandmothers.  \\nShalini is 65 years.  \\nTherefore, Shalini is a grandmother.\"  \\n \\n2. Learning: It is the activity of gaining knowledge or skill by studying, practising, being \\ntaught, or experiencing something . Learning enhances the awareness of the subjects \\nof the study.  \\n    The ability of lea rning is possessed by humans, some animals, and AI -enabled   \\nsystems. Learning is categorized as: \\no Auditory Learning: It is learning by listening and hearing. For example, students \\nlistening to recorded audio lectures. \\n \\no Episodic Learning: To learn by remembering sequences of events that one has \\nwitnessed or experienced. This is linear and orderly. \\n \\no Motor Learning: It is learning by precise movement of muscles. For example, \\npicking objects, Writing, etc. \\n \\no Observational Learning: To learn by watching and imitating others. For example, \\nchild tries to learn by mimicking her parent.   \\n \\no Perceptual Learning: It is learning to recognize stimuli that one has seen before. \\nFor example, identifying and classifying objects and situations. \\n \\no Relational Learning: It involves learning to differentiate among various stimuli \\non the basis of relational properties, rather than absolute properties. For Example, \\nAdding ‘little less’ salt at the time of cooking potatoes that came up salty last time, \\nwhen cooked with adding say a tablespoon of salt.  \\n \\no Spatial learning : It is learning through visual stimuli such as images, colors, \\nmaps, etc. For Example,  A person can create roadmap in mind before actually \\nfollowing the road.  \\n \\no Stimulus-Response Learning: It is l earning to perform a part icular behavior \\nwhen a certain stimulus is present . For example, a dog raises its ear on hearing \\ndoorbell.  \\n3. Problem solving: It is the process in which one perceives and tries to arrive at a \\ndesired solution from a present situation by taking some path, wh ich is blocked by \\nknown or unknown hurdles. \\nProblem solving also includes decision making, which is the process of selecting the \\nbest suitable alternative out of multiple alternatives to reach the desired goal are \\navailable.  \\n4. Perception: It is the process of acquiring, interpreting, selecting, and organizing  \\nsensory information.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 14, 'page_label': '15'}, page_content='Artificial Intelligence  \\n  9 \\nPerception presumes sensing. In humans, perception is aided by sensory organs. In \\nthe domain of AI, perception mechanism puts the data acquired by the sensors \\ntogether in a meaningful manner.  \\n5. Linguistic Intelligence: It is one’s ability to use, comprehend, speak, and write the \\nverbal and written language. It is important in interpersonal communication. \\nDifference between Human and Machine Intelligence \\n\\uf0b7 Humans perceive by patterns whereas the machines perceive by set of rules and data. \\n\\uf0b7 Humans store and recall information by patterns, machines do it by searching \\nalgorithms. For example, the number 40404040 is easy to remember, store and recall \\nas its pattern is simple. \\n \\n\\uf0b7 Humans can figur e out the complete object even if some part of it is missing or \\ndistorted; whereas the machines cannot correctly.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 15, 'page_label': '16'}, page_content='Artificial Intelligence  \\n  10 \\nThe domain of artificial intelligence is huge in breadth and width. W hile proceeding, w e \\nconsider the broadly common and prospering research areas in the domain of AI: \\n \\nSpeech and Voice Recognition \\nThese both terms are common in robotics, expert systems and natural language processing. \\nThough these terms are used interchangeably, their objectives are different. \\nSpeech Recognition Voice Recognition \\nThe s peech recognition aims at \\nunderstanding and comprehend ing WHAT \\nwas spoken. \\nThe objective of voice recognition is to \\nrecognize WHO is speaking. \\nIt is used in hand -free computing, map or \\nmenu navigation \\nIt analyzes person’s t one, voice pitch, and \\naccent, etc., to identify a person. \\nMachine does not need training as it is not \\nspeaker dependent. \\nThe recognition system needs training as it \\nis person-oriented. \\n3. Research Areas of AI'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 16, 'page_label': '17'}, page_content='Artificial Intelligence  \\n  11 \\nSpeaker independent Speech Recognition \\nsystems are difficult to develop. \\nSpeaker-dependent Speech Recognition \\nsystems are comparatively easy to develop. \\nWorking of Speech and Voice Recognition Systems  \\nThe user input spoken at a microphone goes to sound card of the system. The converter turns \\nthe analog signal into equivalent digital signal for the speech processing. The database is used \\nto compare the patterns to recognize the words. Finally , a reverse feedback is given to the \\ndatabase. \\nThis source-language text becomes input to the Translation Engine, which converts it to  the \\ntarget language text. They are supported with interactive GUI, large database of vocabulary \\netc. \\nReal Life Applications of Research Areas \\nThere is a large array of applications where AI is serving common people in their day -to-day \\nlives: \\nSr. No. Research Area Real Life Application  \\n1 \\nExpert Systems \\n \\nExamples: Flight-tracking systems, Clinical \\nsystems \\n \\n2 \\nNatural Language Processing \\n \\nExamples: Google Now feature , speech \\nrecognition, Automatic voice output \\n \\n3 \\nNeural Networks \\n \\nExamples: Pattern recogn ition system s such as  \\nface recognition, character recognition, handwriting \\nrecognition.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 17, 'page_label': '18'}, page_content='Artificial Intelligence  \\n  12 \\n4 \\nRobotics \\n \\nExamples: Industrial robots for moving, spraying, \\npainting, precision checking, drilling, cleaning, \\ncoating, carving etc. \\n \\n5 \\nFuzzy Logic \\n \\nExamples: Consumer electronics, automobiles , \\netc. \\n \\nT ask Classification of AI \\nThe domain of AI is classified into Formal tasks, Mundane tasks, and Expert tasks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 18, 'page_label': '19'}, page_content='Artificial Intelligence  \\n  13 \\nTask Domains of Artificial Intelligence \\nMundane (Ordinary) Tasks Formal Tasks Expert Tasks \\nPerception \\n\\uf0b7 Computer Vision \\n\\uf0b7 Speech, Voice \\n\\uf0b7 Mathematics \\n\\uf0b7 Geometry \\n\\uf0b7 Logic \\n\\uf0b7 Integration and \\nDifferentiation \\n\\uf0b7 Engineering \\n\\uf0b7 Fault finding \\n\\uf0b7 Manufacturing \\n\\uf0b7 Monitoring \\nNatural Language Processing \\n\\uf0b7 Understanding \\n\\uf0b7 Language Generation \\n\\uf0b7 Language Translation \\nGames \\n\\uf0b7 Go \\n\\uf0b7 Chess (Deep Blue) \\n\\uf0b7 Checkers \\nScientific Analysis \\n \\nCommon Sense Verification Financial Analysis \\nReasoning Theorem Proving Medical Diagnosis \\nPlanning  Creativity \\nRobotics \\n\\uf0b7 Locomotive \\n  \\n \\nHumans learn mundane (ordinary) tasks  since their birth. They learn by perce ption, \\nspeaking, using language, and locomotives. They learn Formal Tasks and Expert Tasks later, \\nin that order.  \\nFor humans, the mundane tasks are easiest to learn. The same was considered true before \\ntrying to implement mundane tasks in machines. Earlier , all work of AI was concentrated in \\nthe mundane task domain.  \\nLater, it turned out that the machine requires more knowledge, complex knowledge \\nrepresentation, and complicated algorithms for handling mundane tasks. This is the reason \\nwhy AI work is more pr ospering in the  Expert Task domain now, as the expert task \\ndomain needs expert knowledge without common sense, which can be easier to represent and \\nhandle.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 19, 'page_label': '20'}, page_content='Artificial Intelligence  \\n  14 \\nAn AI system is composed of an agent and its environment. The agents ac t in their \\nenvironment. The environment may contain other agents. \\nWhat are Agent and Environment? \\nAn agent is anything that can perceive its environment through sensors and acts upon that \\nenvironment through effectors.  \\n\\uf0b7 A human agent has sensory organs such as eyes, ears, nose, tongue and skin parallel \\nto the sensors, and other organs such as hands, legs, mouth, for effectors.  \\n\\uf0b7 A robotic agent  replaces cameras and infrared range finders for the sensors , and \\nvarious motors and actuators for effectors.  \\n\\uf0b7 A software agent has encoded bit strings as its programs and actions. \\n \\nAgents T erminology \\n\\uf0b7 Performance Measure of Agent: It is the criteria, which determines how successful \\nan agent is. \\n\\uf0b7 Behavior of Agent: It is the action that agent performs after any given seq uence of \\npercepts. \\n\\uf0b7 Percept: It is agent’s perceptual inputs at a given instance.  \\n\\uf0b7 Percept Sequence: It is the history of all that an agent has perceived till date.  \\n4. Agents and Environments'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 20, 'page_label': '21'}, page_content='Artificial Intelligence  \\n  15 \\n\\uf0b7 Agent Function: It is a map from the precept sequence to an action. \\nRationality \\nRationality is nothing but status of being reasonable, sensible, and having good sense of \\njudgment. \\nRationality is concerned with expected actions and results depending upon what the agent has \\nperceived. Performing actions with the aim of  obtaining useful information is an important \\npart of rationality. \\nWhat is Ideal Rational Agent? \\nAn ideal rational agent is the one, which is capable of doing expected actions to maximize its \\nperformance measure, on the basis of: \\n\\uf0b7 Its percept sequence  \\n\\uf0b7 Its built-in knowledge base \\nRationality of an agent depends on the following:  \\n1. The performance measures, which determine the degree of success.  \\n2. Agent’s Percept Sequence till now.  \\n3. The agent’s prior knowledge about the environment.  \\n4. The actions that the agent can carry out.  \\nA rational agent always performs right action, where the right action means the action that \\ncauses the agent to be most successful in the given percept sequence. The problem the agent \\nsolves is characterized by Performance Measure, Environment, Actuators, and Sensors \\n(PEAS). \\nThe Structure of Intelligent Agents \\nAgent’s structure can be viewed as: \\n• Agent = Architecture + Agent Program \\n• Architecture = the machinery that an agent executes on. \\n• Agent Program = an implementation of an agent function. \\nSimple Reflex Agents \\n\\uf0b7 They choose actions only based on the current percept.  \\n\\uf0b7 They are rational only if a correct decision is made only on the basis of current precept.  \\n\\uf0b7 Their environment is completely observable. \\nCondition-Action Rule – It is a rule that maps a state (condition) to an action.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 21, 'page_label': '22'}, page_content='Artificial Intelligence  \\n  16 \\n \\nModel-Based Reflex Agents  \\nThey use a model of the world to choose their actions. They maintain an internal state.  \\nModel: knowledge about “how the things happen in the world”. \\nInternal State: It is a representation of unobserved aspects of current state depending on \\npercept history. \\nUpdating state requires the information about \\n\\uf0b7 How the world evolves. \\n\\uf0b7 How the agent’s actions affect the world.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 22, 'page_label': '23'}, page_content='Artificial Intelligence  \\n  17 \\n \\nGoal-Based Agents \\nThey choose their actions in order to achieve goals. Goal-based approach is more flexible than \\nreflex agent since the knowledge supporting a decision is explicitly modeled, thereby allowing \\nfor modifications. \\n\\uf0b7 Goal: It is the description of desirable situations. \\n \\nUtility-Based Agents  \\nThey choose actions based on a preference (utility) for each state.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 23, 'page_label': '24'}, page_content='Artificial Intelligence  \\n  18 \\nGoals are inadequate when: \\n\\uf0b7 There are conflicting goals only some of which can be achieved. \\n\\uf0b7 Goals have some uncertainty of being achieved and one needs to  weigh likelihood of \\nsuccess against the importance of a goal. \\n \\n \\nThe Nature of Environments \\nSome programs operate in the entirely artificial environment confined to keyboard input, \\ndatabase, computer file systems and character output on a screen. \\nIn contrast, some software agents (software robots or softbots) exist in rich, unlimited \\nsoftbots domains. The simulator has a very detailed, complex environment. The software \\nagent needs to choose from a long array of actions in real time. A softbot designed to scan  \\nthe online preferences of the customer and show interesting items to the customer works in \\nthe real as well as an artificial environment. \\nThe most famous artificial environment is the Turing Test environment , in which one \\nreal and other artificial agents are tested on equal ground. This is a very ch allenging \\nenvironment as it is highly difficult for a software agent to perform as well as a human. \\nTuring Test  \\nThe success of an intelligent behavior of a system can be measured with Turing Test.  \\nTwo persons and a machine to be evaluated participate in the test.  Out of the two persons, \\none plays the role of the tester. Each of them sits in different rooms. The tester is unaware of \\nwho is machine and who is a human. He interrogates the questions by typing and sending \\nthem to both intelligences, to which he receives typed responses.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 24, 'page_label': '25'}, page_content='Artificial Intelligence  \\n  19 \\nThis test aims at fooling the tester. If the tester fails to determine machine’s response from \\nthe human response, then the machine is said to be intelligent.   \\nProperties of Environment \\nThe environment has multifold properties: \\n\\uf0b7 Discrete / Continuous:  If there are a limited number of distinct, clearly defined, \\nstates of the environment, the environment is discrete (For example, chess); otherwise \\nit is continuous (For example, driving).  \\n \\n\\uf0b7 Observable / Partially Observable: If it is possible to determine the complete state \\nof the environment at each time point from the percepts it is observable; otherwise it \\nis only partially observable.  \\n \\n\\uf0b7 Static / Dynamic: If the environment does not change while an agent is acting, then \\nit is static; otherwise it is dynamic. \\n \\n\\uf0b7 Single agent / Multiple agents : The environment may contain other agents which \\nmay be of the same or different kind as that of the agent. \\n \\n\\uf0b7 Accessible vs. inaccessible: If the agent’s sensory apparatus can have access to the \\ncomplete state of the environment, then the environment is accessible to that agent. \\n \\n\\uf0b7 Deterministic vs. Non-deterministic: If the next state of the environment is \\ncompletely determined by the current state and the actions of the agent, then the \\nenvironment is deterministic; otherwise it is non-deterministic. \\n \\n\\uf0b7 Episodic vs. Non-episodic: In an episodic environment, each episode consists of the \\nagent perceiving and then acting. The quality of its action depends just on the episode \\nitself. Subsequent episodes do not depend on the actions in the previous episodes. \\nEpisodic environments are much simpler because the agent does not need to think \\nahead.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 25, 'page_label': '26'}, page_content='Artificial Intelligence  \\n  20 \\nSearching is the universal technique of problem solving in AI. There are some single-player \\ngames such as tile games, Sudoku, crossword, etc. The search algorithms help you to search \\nfor a particular position in such games.  \\nSingle Agent Pathfinding Problems  \\nThe games such as 3X3 eight-tile, 4X4 fifteen-tile, and 5X5 twenty four tile puzzles are single-\\nagent-path-finding challenges. They consist of a matrix of tiles with a blank tile. The player is \\nrequired to arrange the tiles by sliding a tile either vertically or horizontally into a blank space \\nwith the aim of accomplishing some objective.  \\nThe other examples of single agent  pathfinding problems are Travelling Salesman Problem, \\nRubik’s Cube, and Theorem Proving. \\nSearch T erminology \\nProblem Space: It is the environment in which the search takes place. ( A set of states and \\nset of operators to change those states) \\nProblem Instance: It is Initial state + Goal state \\nProblem Space Graph : It represents problem state. States are shown by nodes and \\noperators are shown by edges. \\nDepth of a problem: Length of a shortest path or shortest sequence of operators from Initial \\nState to goal state.  \\nSpace Complexity: The maximum number of nodes that are stored in memory. \\nTime Complexity: The maximum number of nodes that are created. \\nAdmissibility: A property of an algorithm to always find an optimal solution. \\nBranching Factor: The average number of child nodes in the problem space graph. \\nDepth: Length of the shortest path from initial state to goal state. \\nBrute-Force Search Strategies \\nThey are most simple, as they do not need any domain -specific knowledge. They work fine \\nwith small number of possible states. \\nRequirements –  \\n\\uf0b7 State description \\n5. Popular Search Algorithms'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 26, 'page_label': '27'}, page_content='Artificial Intelligence  \\n  21 \\n\\uf0b7 A set of valid operators \\n\\uf0b7 Initial state \\n\\uf0b7 Goal state description \\nBreadth-First Search \\nIt starts from the root node, explores the neighboring nodes first and moves towards the next \\nlevel neighbors.  It generates one  tree at a time until the solution is found. It can be \\nimplemented using FIFO queue data structure. This method provides shortest path to the \\nsolution.  \\nIf branching factor (average number of child nodes for a given node) = b and depth = d, \\nthen number of nodes at level d = bd. \\nThe total no of nodes created in worst case is b + b2 + b3 + … + bd. \\nDisadvantage: Since each level of nodes is saved for creating next one, it consumes a lot of \\nmemory space. Space requirement to store nodes is exponential. \\nIts complexity depends on the number of nodes. It can check duplicate nodes. \\n \\nDepth-First Search \\nIt is implemented in recursion with LIFO stack data structure. It creates the same set of nodes \\nas Breadth-First method, only in the different order. \\nAs the nodes on the single path are stored in each iteration from root to leaf node, the space \\nrequirement to store nodes is linear.  With branching factor b and depth as m, the storage \\nspace is bm. \\nDisadvantage: This algorithm may not terminate and go on infinitely on one  path. The \\nsolution to this issue is to choose a cut -off depth. If the ideal cut -off is d, and if chosen cut-\\noff is lesser than d, then this algo rithm may fail. If chosen cut -off is more  than d, then \\nexecution time increases. \\nIts complexity depends on the number of paths. It cannot check duplicate nodes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 27, 'page_label': '28'}, page_content='Artificial Intelligence  \\n  22 \\n \\nBidirectional Search \\nIt searches forward from initial state and backward from goal state till both meet to identify \\na common state.  \\nThe path from initial state is concatenated with the inverse path from the goal state. Each \\nsearch is done only up to half of the total path. \\nUniform Cost Search \\nSorting is done in increasing cost of the path to a node. It always expands the least cost node. \\nIt is identical to Breadth First search if each transition has the same cost. \\nIt explores paths in the increasing order of cost.  \\nDisadvantage: There can be multiple long paths with the cost ≤ C*. Uniform Cost search \\nmust explore them all. \\nIterative Deepening Depth-First Search \\nIt performs depth-first search to level 1, st arts over, executes a complete depth -first search \\nto level 2, and continues in such way till the solution is found.  \\nIt never creates a node until all lower nodes are generated. It only saves a stack of nodes. \\nThe algorithm ends when it finds a solution at depth d. The number of nodes created at depth \\nd is bd and at depth d-1 is bd-1.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 28, 'page_label': '29'}, page_content='Artificial Intelligence  \\n  23 \\n \\nComparison of Various Algorithms Complexities \\nLet us see the performance of algorithms based on various criteria:  \\nCriterion Breadth \\nFirst \\nDepth \\nFirst Bidirectional Uniform \\nCost \\nIterative \\nDeepening \\nTime bd bm b d/2 bd bd \\nSpace bd bm b d/2 bd bd \\nOptimality Y N Y Y Y \\nCompleteness Y N Y Y Y \\nInformed (Heuristic) Search Strategies \\nTo solve large problems with large number of possible states, problem-specific knowledge \\nneeds to be added to increase the efficiency of search algorithms. \\nHeuristic Evaluation Functions \\nThey calculate the cost of optimal path between two states. A heuristic function for sliding -\\ntiles games is computed by counting number of moves that each tile makes from its goal state \\nand adding these number of moves for all tiles. \\nPure Heuristic Search \\nIt expands nodes in the order of their heuristic values. It creates two lists, a closed list for the \\nalready expanded nodes and an open list for the created but unexpanded nodes. \\nIn each iteration, a node with a minimum heuristic value is expanded, all its child nodes are \\ncreated and placed in the closed list. Then, the heuristic function is applied to the child nodes \\nand they are placed in the open list according to  their heuristic value. The shorter paths are \\nsaved and the longer ones are disposed.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 29, 'page_label': '30'}, page_content='Artificial Intelligence  \\n  24 \\nA* Search \\nIt is best -known form of Best First search. It avoids expanding paths that are already \\nexpensive, but expands most promising paths first.  \\nf(n) = g(n) + h(n), where  \\n• g(n) the cost (so far) to reach the node  \\n• h(n) estimated cost to get from the node to the goal  \\n• f(n) estimated total cost of path through n to goal. It is implemented using priority queue \\nby increasing f(n). \\nGreedy Best First Search  \\nIt expands the node that is estimated to be closest to goal. It expands nodes based on f(n) = \\nh(n). It is implemented using priority queue.  \\nDisadvantage: It can get stuck in loops. It is not optimal. \\nLocal Search Algorithms   \\nThey start from a prospective solution and then move to a  neighboring solution. They can \\nreturn a valid solution even if it is interrupted at any time before they end. \\nHill-Climbing Search  \\nIt is an iterative algorithm that starts with an arbitrary solution to a problem and attempts to \\nfind a better solution by changing a single element of the solution incrementally. If the change \\nproduces a better solution, an incremental change is taken as a new solution. This process is \\nrepeated until there are no further improvements. \\nfunction Hill-Climbing (problem), returns a state that is a local maximum. \\ninputs: problem, a problem  \\nlocal variables: current, a node  \\n                      neighbor, a node  \\ncurrent ←Make_Node(Initial-State[problem])  \\nloop  \\n    do neighbor ← a highest_valued successor of current  \\n         if Value[neighbor] ≤ Value[current] then  \\n         return State[current]  \\n         current ← neighbor  \\nend \\nDisadvantage: This algorithm is neither complete, nor optimal.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 30, 'page_label': '31'}, page_content=\"Artificial Intelligence  \\n  25 \\nLocal Beam Search  \\nIn this algorithm, it holds k number of states at any given time. At the start, these states are \\ngenerated randomly. The successors of these k states are computed with the help of objective \\nfunction. If any of these successors is the maximum value of the objective function, then the \\nalgorithm stops.  \\nOtherwise the (initial k states and k number of successors of the states = 2k) states are placed \\nin a pool. The pool is then sorted numerically. The highest k states are selected as new initial \\nstates. This process continues until a maximum value is reached.  \\nfunction BeamSearch( problem, k), returns a solution state.  \\nstart with k randomly generated states  \\nloop  \\n     generate all successors of all k states  \\n     if any of the states = solution, then return the state \\n     else select the k best successors \\nend \\nSimulated Annealing \\nAnnealing is the process of heating and cooling a metal to change its internal structure for \\nmodifying its physical properties. When the metal cools, its new structure is seized, and the \\nmetal retains its newly obtained properties. In simulated annealing process, the temperature \\nis kept variable.  \\nWe initially set the temperature high and then allow it to ‘cool' slowly as the algorithm \\nproceeds. When the temperature is high , the algorithm is allowed to accept worse solutions \\nwith high frequency.  \\nStart \\n5. Initialize k = 0; L = integer number of variables; \\n6. From i -> j, search the performance difference ∆. \\n7. If ∆ <= 0 then accept else if exp(-\\uf044/T(k)) > random(0,1) then accept; \\n8. Repeat steps 1 and 2 for L(k) steps. \\n9. k = k + 1; \\nRepeat steps 1 through 4 till the criteria is met. \\nEnd \\nTravelling Salesman Problem \\nIn this algorithm, the objective is to find a low-cost tour that starts from a city, visits all cities \\nen-route exactly once and ends at the same starting city. \\nStart \\n   Find out all (n -1)! Possible solutions, where n is the total number of cities.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 31, 'page_label': '32'}, page_content='Artificial Intelligence  \\n  26 \\n   Determine the minimum cost by finding out the cost of each of these (n -1)!     \\n   solutions. \\n   Finally, keep the one with the minimum cost. \\nend'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 32, 'page_label': '33'}, page_content='Artificial Intelligence  \\n  27 \\nFuzzy Logic Systems (FLS) produce acceptable but definite output in response to incomplete, \\nambiguous, distorted, or inaccurate (fuzzy) input. \\nWhat is Fuzzy Logic? \\nFuzzy Logic (FL) is a method of reasoning that resembles human reasoning . The approach of \\nFL imitates the way of decision making in humans  that involves all intermediate possibilities \\nbetween digital values YES and NO.  \\nThe conventional logic block that a computer can understand takes precise input and produces \\na definite output as TRUE or FALSE, which is equivalent to human’s YES or NO.  \\nThe inventor of fuzzy logic, Lotfi Zadeh, observed that unlike computers, the human decision \\nmaking includes a range of possibilities between YES and NO, such as: \\nCERTAINLY YES \\nPOSSIBLY YES \\nCANNOT SAY \\nPOSSIBLY NO \\nCERTAINLY NO \\n \\nThe fuzzy logic works on the levels of possibilities of input to achieve the definite output.  \\nImplementation \\n\\uf0b7 It can be implemented in systems with various sizes and capabilities ranging from small \\nmicro-controllers to large, networked, workstation-based control systems.  \\n\\uf0b7 It can be implemented in hardware, software, or a combination of both.  \\nWhy Fuzzy Logic? \\nFuzzy logic is useful for commercial and practical purposes. \\n\\uf0b7 It can control machines and consumer products. \\n\\uf0b7 It may not give accurate reasoning, but acceptable reasoning. \\n\\uf0b7 Fuzzy logic helps to deal with the uncertainty in engineering.  \\n6. Fuzzy Logic Systems'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 33, 'page_label': '34'}, page_content='Artificial Intelligence  \\n  28 \\nFuzzy Logic Systems Architecture \\nIt has four main parts as shown: \\n1. Fuzzification Module: transforms the system inputs, which are crisp numbers, into \\nfuzzy sets.  \\nIt splits the input signal into five steps such as: \\nLP x is Large Positive  \\nMP x is Medium Positive \\nS x is Small \\nMN x is Medium Negative \\nLN x is Large Negative \\n2. Knowledge Base: It stores IF-THEN rules provided by experts. \\n3. Inference E ngine: It simulates the human reasoning process by making fuzzy \\ninference on the inputs and IF-THEN rules. \\n4. Defuzzification Module: It transforms the fuzzy set obtained by the inference engine \\ninto a crisp value. \\n \\nThese membership functions work on fuzzy sets of variables. \\nMembership Functions \\nMembership functions allow you  to quantify linguistic term and represent a fuzzy set \\ngraphically. A membership function  for a fuzzy set A on the universe of discourse X is \\ndefined as µA:X → [0,1].'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 34, 'page_label': '35'}, page_content='Artificial Intelligence  \\n  29 \\nHere, each element of X is mapped to a value between 0 and 1. It is called membership \\nvalue or degree of membership. It quantifies the degree of membership of the element in X \\nto the fuzzy set A. \\n\\uf0b7 x axis represents the universe of discourse. \\n\\uf0b7 y axis represents the degrees of membership in the [0, 1] interval. \\nThere can be multiple membership functions applicabl e to fuzzify a nume rical value. Simple \\nmembership functions are used as use of complex functions does not add more precision  in \\nthe output. \\nAll membership functions for LP, MP, S, MN, and LN are shown as below: \\n \\nThe triangular membership function shapes are most common amon g various other \\nmembership function shapes such as trapezoidal, singleton, and Gaussian. \\nHere, the input to 5-level fuzzifier varies from -10 volts to +10 volts. Hence the corresponding \\noutput also changes. \\nExample of a Fuzzy Logic System \\nLet us consider an air conditioning system with 5-lvel fuzzy logic system. This system adjusts \\nthe temperature of air conditioner by comparing the room temperature and the target \\ntemperature value.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 35, 'page_label': '36'}, page_content='Artificial Intelligence  \\n  30 \\n \\nAlgorithm  \\n1. Define linguistic Variables and terms (start) \\n2. Construct membership functions for them. (start)  \\n3. Construct knowledge base of rules (start) \\n4. Convert crisp data into fuzzy data sets using membership functions (fuzzification) \\n5. Evaluate rules in the rule base (inference engine) \\n6. Combine results from each rule (inference engine) \\n7. Convert output data into non-fuzzy values. (defuzzification) \\nDevelopment \\nStep 1: Define linguistic variables and terms \\nLinguistic variables are input and output variables in the form of simple words or sentences. \\nFor room temperature, cold, warm, hot, etc., are linguistic terms. \\nTemperature (t) = {very-cold, cold, warm, very-warm, hot} \\nEvery member of this set is a linguistic term and it can cover some portion of overall \\ntemperature values. \\nStep 2: Construct membership functions for them \\nThe membership functions of temperature variable are as shown:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 36, 'page_label': '37'}, page_content='Artificial Intelligence  \\n  31 \\n \\nStep3: Construct knowledge base rules \\nCreate a matrix of room temperature values versus target temperature values that an air \\nconditioning system is expected to provide. \\nRoomTemp/Target Very_Cold Cold Warm Hot Very_Hot \\nVery_Cold No_Change Heat Heat Heat Heat \\nCold Cool No_Change Heat Heat Heat \\nWarm Cool Cool No_Change Heat Heat \\nHot Cool Cool Cool No_Change Heat \\nVery_Hot Cool Cool Cool Cool No_Change \\n \\nBuild a set of rules into the knowledge base in the form of IF-THEN-ELSE structures. \\nSr. No. Condition Action \\n1 IF temperature=(Cold OR Very_Cold) AND target=Warm THEN HEAT \\n2 IF temperature=(Hot OR Very_Hot) AND target=Warm THEN COOL \\n3 IF (temperature=Warm) AND (target=Warm) THEN NOCHANGE \\n \\nStep5'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 37, 'page_label': '38'}, page_content='Artificial Intelligence  \\n  32 \\nFuzzy set operations perform evaluation of rules. The operations used for OR and AND are \\nMax and Min respectively.  All results of evaluation are combined to form a final result. This \\nresult is a fuzzy value. \\nStep 6 \\nDefuzzification is then performed according to membership function for output variable. \\n \\nApplication Areas of Fuzzy Logic \\nThe key application areas of fuzzy logic are as given: \\nAutomotive Systems \\n\\uf0b7 Automatic Gearboxes \\n\\uf0b7 Four-Wheel Steering \\n\\uf0b7 Vehicle environment control \\nConsumer Electronics \\n\\uf0b7 Hi-Fi Systems \\n\\uf0b7 Photocopiers \\n\\uf0b7 Still and Video Cameras \\n\\uf0b7 Television \\nDomestic Goods \\n\\uf0b7 Microwave Ovens \\n\\uf0b7 Refrigerators \\n\\uf0b7 Toasters \\n\\uf0b7 Vacuum Cleaners'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 38, 'page_label': '39'}, page_content='Artificial Intelligence  \\n  33 \\n\\uf0b7 Washing Machines \\nEnvironment Control \\n\\uf0b7 Air Conditioners/Dryers/Heaters \\n\\uf0b7 Humidifiers \\nAdvantages of FLSs \\n\\uf0b7 Mathematical concepts within fuzzy reasoning are very simple. \\n\\uf0b7 You can modify a FIS by just adding or deleting rules due to flexibility of fuzzy logic. \\n\\uf0b7 Fuzzy logic Systems can take imprecise, distorted, noisy input information. \\n\\uf0b7 FLSs are easy to construct and understand. \\n\\uf0b7 Fuzzy logic is a solution to complex problems in all fields of life, including medicine, as \\nit resembles human reasoning and decision making.  \\nDisadvantages of FLSs \\n\\uf0b7 There is no systematic approach to fuzzy system designing. \\n\\uf0b7 They are understandable only when simple. \\n\\uf0b7 They are suitable for the problems which do not need high accuracy.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 39, 'page_label': '40'}, page_content='Artificial Intelligence  \\n  34 \\nNatural Language Processing (NLP) refers to AI method of communicating with an intelligent \\nsystems using a natural language such as English. \\nProcessing of Natural Language is required when you want an intelligent system like robot to \\nperform as per your instructions, when you want to hear decision from a dialogue based \\nclinical expert system, etc. \\nThe field of NLP involves making computers to perform useful tasks with the natural languages \\nhumans use. The input and output of an NLP system can be: \\n\\uf0b7 Speech \\n\\uf0b7 Written Text \\nComponents of NLP \\nThere are two components of NLP as given: \\nNatural Language Understanding (NLU) \\nUnderstanding involves the following tasks: \\n\\uf0b7 Mapping the given input in natural language into useful representations. \\n\\uf0b7 Analyzing different aspects of the language. \\nNatural Language Generation (NLG) \\nIt is the process of producing meaningful phrases and sentences in the form of natural \\nlanguage from some internal representation.  \\nIt involves: \\n\\uf0b7 Text planning: It includes retrieving the relevant content from knowledge base.  \\n\\uf0b7 Sentence planning:  It includes choosing required words, forming meaningful \\nphrases, setting tone of the sentence. \\n\\uf0b7 Text Realization: It is mapping sentence plan into sentence structure. \\nThe NLU is harder than NLG. \\nDifficulties in NLU \\n\\uf0b7 NL has an extremely rich form and structure. \\n\\uf0b7 It is very ambiguous. There can be different levels of ambiguity: \\no Lexical ambiguity: It is at very primitive level such as word-level.  \\n7. Natural Language Processing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 40, 'page_label': '41'}, page_content='Artificial Intelligence  \\n  35 \\no For example, treating the word “board” as noun or verb? \\no Syntax Level ambiguity: A sentence can be parsed in different ways.  \\no For example, “He lifted the beetle with red cap.” – Did he use cap to lift the \\nbeetle or he lifted a beetle that had red cap? \\no Referential ambiguity: Referring to something using pronouns. For example, \\nRima went to Gauri. She said, “I am tired.” - Exactly who is tired? \\no One input can mean different meanings. \\no Many inputs can mean the same thing. \\nNLP T erminology \\n\\uf0b7 Phonology: It is study of organizing sound systematically. \\n\\uf0b7 Morphology: It is a study of construction of words from primitive meaningful units. \\n\\uf0b7 Morpheme: It is primitive unit of meaning in a language. \\n\\uf0b7 Syntax: It refers to arranging words to make a sentence. It also involves determining \\nthe structural role of words in the sentence and in phrases. \\n\\uf0b7 Semantics: It is concerned with the meaning of words and how to combine words into \\nmeaningful phrases and sentences. \\n\\uf0b7 Pragmatics: It deals with using and understanding sentences in different situations \\nand how the interpretation of the sentence is affected. \\n\\uf0b7 Discourse: It deals with how the immediately preceding sentence can affect the \\ninterpretation of the next sentence. \\n\\uf0b7 World Knowledge: It includes the general knowledge about the world. \\nSteps in NLP \\nThere are general five steps:  \\n1. Lexical Analysis \\nIt involves identifying and analyzing the structure of words. Lexicon of a language \\nmeans the collection of words and phrases in a language. Lexical analysis is dividing \\nthe whole chunk of txt into paragraphs, sentences, and words. \\n2. Syntactic Analysis (Parsing) \\nIt involves analysis of words in the sentence for grammar and arranging words in a \\nmanner that shows the relationship among the words. The sentence such as “The \\nschool goes to boy” is rejected by English syntactic analyzer.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 41, 'page_label': '42'}, page_content='Artificial Intelligence  \\n  36 \\n \\n3. Semantic Analysis \\nIt draws the exact meaning or the dictionary meaning from the text. The text is \\nchecked for meaningfulness. It is done by mapping syntactic structures and objects in \\nthe task domain. The semantic analyzer disregards sentence such as “hot ice-cream”. \\n4. Discourse Integration \\nThe meaning of any sentence depends upon the meaning of the sentence just before \\nit. In addition, it also brings about the meaning of immediately succeeding sentence. \\n5. Pragmatic Analysis \\nDuring this, w hat was said is re -interpreted on what it actually meant. It involves \\nderiving those aspects of language which require real world knowledge. \\nImplementation Aspects of Syntactic Analysis \\nThere are a number of algorithms researchers have developed for syntactic analysis, but we \\nconsider only the following simple methods: \\n\\uf0b7 Context-Free Grammar \\n\\uf0b7 Top-Down Parser \\nLet us see them in detail: \\nContext-Free Grammar  \\nIt is the grammar that consists rules with a single symbol on the left-hand side of the rewrite \\nrules. Let us create grammar to parse a sentence –  \\n“The bird pecks the grains” \\nArticles (DET): a | an | the.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 42, 'page_label': '43'}, page_content='Artificial Intelligence  \\n  37 \\nNouns: bird | birds | grain | grains \\nNoun Phrase (NP): Article + Noun | Article + Adjective + Noun  \\n                              = DET N | DET ADJ N \\nVerbs: pecks | pecking | pecked  \\nVerb Phrase (VP): NP V | V NP \\nAdjectives (ADJ): beautiful | small | chirping \\nThe parse tree breaks down the sentence into structured parts so that the computer can easily \\nunderstand and process it. In order for the pars ing algorithm to construct this parse tree, a \\nset of rewrite rules, which describe what tree structures are legal, need to be constructed. \\nThese rules say that a certain symbol may be expanded in the tree by a sequence of other \\nsymbols. According to first order logic rule, ff there are two strings Noun Phrase (NP) and \\nVerb Phrase (VP), then the string combined by NP followed by VP is a sentence. The rewrite \\nrules for the sentence are as follows: \\n \\nS -> NP VP \\nNP -> DET N | DET ADJ N \\nVP -> V NP \\nLexocon: \\nDET -> a | the \\nADJ -> beautiful | perching \\nN -> bird | birds | grain | grains \\nV -> peck | pecks | pecking  \\n \\nThe parse tree can be created as shown:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 43, 'page_label': '44'}, page_content='Artificial Intelligence  \\n  38 \\n \\nNow consider the above rewrite rules. Since  V can be replaced by both, \"peck\" or \"pecks\", \\nsentences such as \"The bird peck the grains\" can be wrongly permitted. i. e. the subject-verb \\nagreement error is approved as correct.  \\nMerit: The simplest style of grammar, therefore widely used one. \\nDemerits: \\n\\uf0b7 They are not highly precise. For example, “The grains peck the bird”, is a syntactically \\ncorrect according to parser, but even if it makes no sense, parser takes it as a correct \\nsentence. \\n\\uf0b7 To bring out  high precision, multiple sets of grammar need to be prepared.  It may \\nrequire a completely different sets of rules for parsing singular and plural variations, \\npassive sentences, etc., which can lead to  creation of huge set of rules that are  \\nunmanageable.  \\nTop-Down Parser \\nHere, the parser starts with the  S symbol and attempts to rewrite it into a sequence \\nof terminal symbols  that matches the classes of the words in the input sentence until it \\nconsists entirely of terminal symbols.  \\nThese are then checked with the input sentence to see if it matched. If not, the process is \\nstarted over again with a different set of rules. This is  repeated until a specific rule is found \\nwhich describes the structure of the sentence.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 44, 'page_label': '45'}, page_content='Artificial Intelligence  \\n  39 \\nMerit: It is simple to implement. \\nDemerits: \\n\\uf0b7 It is inefficient, as the search process has to be repeated if an error occurs.  \\n\\uf0b7 Slow speed of working.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 45, 'page_label': '46'}, page_content='Artificial Intelligence  \\n  40 \\nExpert systems (ES) are one of the prominent research domains of AI. It is introduced by the \\nresearchers at Stanford University, Computer Science Department. \\nWhat are Expert Systems? \\nThe expert systems are the computer applications developed to solve complex problems in a \\nparticular domain, at the level of extra-ordinary human intelligence and expertise.  \\nCharacteristics of Expert Systems \\n\\uf0b7 High performance \\n\\uf0b7 Understandable \\n\\uf0b7 Reliable \\n\\uf0b7 Highly responsive \\nCapabilities of Expert Systems \\nThe expert systems are capable of: \\n\\uf0b7 Advising  \\n\\uf0b7 Instructing and assisting human in decision making \\n\\uf0b7 Demonstrating  \\n\\uf0b7 Deriving a solution \\n\\uf0b7 Diagnosing \\n\\uf0b7 Explaining  \\n\\uf0b7 Interpreting input \\n\\uf0b7 Predicting results \\n\\uf0b7 Justifying the conclusion \\n\\uf0b7 Suggesting alternative options to a problem \\nThey are incapable of: \\n\\uf0b7 Substituting human decision makers \\n\\uf0b7 Possessing human capabilities \\n\\uf0b7 Producing accurate output for inadequate knowledge base \\n\\uf0b7 Refining their own knowledge \\n8. Expert Systems'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 46, 'page_label': '47'}, page_content='Artificial Intelligence  \\n  41 \\nComponents of Expert Systems \\nThe components of ES include: \\n\\uf0b7 Knowledge Base  \\n\\uf0b7 Inference Engine  \\n\\uf0b7 User Interface  \\nLet us see them one by one briefly: \\n \\nKnowledge Base  \\nIt contains domain-specific and high-quality knowledge.  \\nKnowledge is required to exhibit intelligence. The success of any ES majorly depends upon \\nthe collection of highly accurate and precise knowledge. \\nWhat is Knowledge? \\nThe data is collection of facts. The information is organized as data and facts about the task \\ndomain. Data, information, and past experience  combined together are termed as \\nknowledge.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 47, 'page_label': '48'}, page_content='Artificial Intelligence  \\n  42 \\nComponents of Knowledge Base \\nThe knowledge base of an ES is a store of both, factual and heuristic knowledge.  \\n\\uf0b7 Factual Knowledge  – It is the information widely accepted by the Knowledge \\nEngineers and scholars in the task domain. \\n\\uf0b7 Heuristic Knowledge – It is about practice, accurate judgment, one’s ability of \\nevaluation, and guessing. \\nKnowledge representation \\nIt is the method used to organize and formalize the knowledge in the knowledge base. It is in \\nthe form of IF-THEN-ELSE rules.   \\nKnowledge Acquisition \\nThe success of any expert system majorly depends on the quality, completeness, and accuracy \\nof the information stored in the knowledge base.  \\nThe knowledge base is formed by readings from various experts, scholars, and the \\nKnowledge Engineers. The knowledge engineer is a person with the qualities of empathy , \\nquick learning, and case analyzing skills.  \\nHe acquires information from subject expert by recording, interviewing, and observing him at \\nwork, etc. He then categorizes and organizes the information in a meaningful way, in the form \\nof IF-THEN-ELSE rules, to be used by interference machine. The knowledge engineer also \\nmonitors the development of the ES. \\nInference Engine \\nUse of efficient procedures and rules by the Inf erence Engine is essential in deducting a \\ncorrect, flawless solution.  \\nIn case of knowledge-based ES, the Inference Engine acquires and manipulates the knowledge \\nfrom the knowledge base to arrive at a particular solution.  \\nIn case of rule based ES, it: \\n\\uf0b7 Applies rules repeatedly to the facts, which are obtained from earlier rule application. \\n\\uf0b7 Adds new knowledge into the knowledge base if required. \\n\\uf0b7 Resolves rules conflict when multiple rules are applicable to a particular case  \\nTo recommend a solution, the inference engine uses the following strategies: \\n\\uf0b7 Forward Chaining \\n\\uf0b7 Backward Chaining \\nForward Chaining \\nIt is a strategy of an expert system to answer the question, “What can happen next?”'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 48, 'page_label': '49'}, page_content='Artificial Intelligence  \\n  43 \\nHere, the inferance engine follows the chain of conditions and derivations and finally deduces \\nthe outcome. It considers all the facts and rules, and sorts them before  concluding to a \\nsolution.  \\nThis strategy is followed for working on conclusion, result, or effect.  For example, prediction \\nof share market status as an effect of changes in interest rates.  \\n \\nBackward Chaining \\nWith this strategy, an expert system finds ou t the answer to the question , “Why this \\nhappened?” \\nOn the basis of what has already happen ed, the in ference engine tries to find out which \\nconditions could have happened in the past for this result. This strategy is followed for finding \\nout cause or reason. For example, diagnosis of blood cancer in humans.  \\n \\nUser Interface \\nUser interface provides interaction between user of the ES and the ES itself. It is generally \\nNatural Language Processing so as to be used by the user who is well -versed in the task \\ndomain. The user of the ES need not be necessarily an expert in Artificial Intelligence.  \\nIt explains how the ES has arrived at a particular recommendation. The explanation may in \\nthe following forms: \\n\\uf0b7 Natural language displayed on screen \\n\\uf0b7 Verbal narrations in natural language'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 49, 'page_label': '50'}, page_content='Artificial Intelligence  \\n  44 \\n\\uf0b7 Listing of rule numbers displayed on the screen.  \\nThe user interface makes it easy to trace the credibility of the deductions. \\nRequirements of Efficient ES User Interface  \\n\\uf0b7 It should help users to accomplish their goals in shortest possible ay. \\n\\uf0b7 It should be designed to work for user’s existing or desired work practices. \\n\\uf0b7 Its technology should be adaptable to user’s requirements; not the other way round.  \\n\\uf0b7 It should make efficient use of user input. \\nExpert Systems Limitations \\nNo technology can offer easy and complete solution. Large systems are costly , require \\nsignificant development time , and computer resources. ESs have their limitations which \\ninclude: \\n\\uf0b7 Limitations of the technology \\n\\uf0b7 Difficult knowledge acquisition  \\n\\uf0b7 ES are Difficult to maintain \\n\\uf0b7 High Development costs \\nApplications of Expert System  \\nThe following table shows where ES can be applied.  \\nApplication Description \\nDesign Domain Camera lens design, automobile design. \\nMedical Domain Diagnosis Systems to deduce cause of  disease from observ ed \\ndata, conduction medical operations on humans. \\nMonitoring Systems  \\nComparing data continu ously with observed system or with  \\nprescribed behavior such as leakage monitoring in long \\npetroleum pipeline. \\nProcess Control Systems Controlling a physical process based on monitoring. \\nKnowledge Domain Finding out faults in vehicles, computers. \\nFinance/Commerce Detection of possible fraud, suspicious transactions, stock \\nmarket trading, Airline scheduling, cargo scheduling.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 50, 'page_label': '51'}, page_content='Artificial Intelligence  \\n  45 \\nExpert System T echnology  \\nThere are several levels of ES technologies available. Expert systems technologies include:  \\n1. Expert System Development Environment : The ES development environment \\nincludes hardware and tools. They are: \\no Workstations, minicomputers, mainframes  \\no High level Symbolic Programming Languages such as LISt Programming (LISP) \\nand PROgrammation en LOGique (PROLOG). \\no Large databases \\n2. Tools: They reduce the effort and cost involved in developing an expert system to \\nlarge extent. \\no Powerful editors and debugging tools with multi-windows. \\no They provide rapid prototyping \\no Have Inbuilt definitions of model, knowledge representation , and inference \\ndesign. \\n1. Shells: A shell is nothing but an  expert system without knowledge base. A shell \\nprovides the developers with knowledge acquisition, inference engine, user interface, \\nand explanation facility. For example, few shells are given below:  \\no Java Expert System Shell (JESS) that provides fully developed Java API for \\ncreating an expert system. \\no Vidwan, a shell developed at the National Centre for Softw are Te chnology, \\nMumbai in 1993. It enables knowledge encoding in the form of IF-THEN rules.  \\nDevelopment of Expert Systems: General Steps  \\nThe process of ES development is iterative. Steps in developing the ES include: \\n1. Identify Problem Domain \\n\\uf0b7 The problem must be suitable for an expert system to solve it. \\n\\uf0b7 Find the experts in task domain for the ES project. \\n\\uf0b7 Establish cost-effectiveness of the system. \\n2. Design the System \\n\\uf0b7 Identify the ES Technology. \\n\\uf0b7 Know and establish the degree of integration with the other systems and databases. \\n\\uf0b7 Realize how the concepts can represent the domain knowledge best.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 51, 'page_label': '52'}, page_content='Artificial Intelligence  \\n  46 \\n3. Develop the Prototype \\n    Form Knowledge Base: The knowledge engineer works to: \\n\\uf0b7     Acquire domain knowledge from the expert. \\n\\uf0b7     Represent it in the form of If-THEN-ELSE rules. \\n4. Test and Refine the Prototype \\n\\uf0b7 The knowledge engineer uses sample cases to test the prototype for any deficiencies \\nin performance.  \\n\\uf0b7 End users test the prototypes of the ES. \\n5. Develop and Complete the ES \\n \\n\\uf0b7 Test and ensure the interaction of the  ES with all elements of its environment,  including end users, \\ndatabases, and other information systems. \\n\\uf0b7 Document the ES project well. \\n\\uf0b7 Train the user to use ES. \\n6. Maintain the System \\n\\uf0b7 Keep the knowledge base up-to-date by regular review and update. \\n\\uf0b7 Cater for new interfaces with other information systems, as those systems evolve. \\nBenefits of Expert Systems \\n\\uf0b7 Availability: They are easily available due to mass production of software. \\n\\uf0b7 Less Production Cost: Production cost is reasonable. This makes them affordable. \\n\\uf0b7 Speed: They offer great speed. They reduce the amount of work an individual puts in. \\n\\uf0b7 Less Error Rate: Error rate is low as compared to human errors. \\n\\uf0b7 Reducing Risk: They can work in the environment dangerous to humans. \\n\\uf0b7 Steady response: They work steadily without getting motional, tensed or fatigued.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 52, 'page_label': '53'}, page_content='Artificial Intelligence  \\n  47 \\nRobotics is a domain in artificial intelligence that deals with the study of creating intelligent \\nand efficient robots. \\nWhat are Robots? \\nRobots are the artificial agents acting in real world environment.  \\nObjective \\nRobots are aimed at manipulating the objects by perceiving, picking, moving, modifying the \\nphysical properties of object, destroying it, or to have an effect thereby freeing manpower \\nfrom doing repetitive functions without getting bored, distracted, or exhausted.  \\nWhat is Robotics? \\nRobotics is a branch of AI, which is composed of Electrical Engineering, Mechanical \\nEngineering, and Computer Science for designing, construction, and application of robots.  \\nAspects of Robotics \\n\\uf0b7 The robots have mechanical construction, form, or shape designed to accomplish a \\nparticular task. \\n\\uf0b7 They have electrical components which power and control the machinery. \\n\\uf0b7 They contain some level of computer program that determines what, when and how \\na robot does something. \\nDifference in Robot System and Other AI Program \\nHere is the difference between the two: \\nAI Programs Robots \\nThey usually operate in computer-stimulated \\nworlds. They operate in real physical world \\nThe input to an AI program is in symbols and \\nrules. \\nInputs to robots is analog signal in the form of \\nspeech waveform or images \\nThey need general purpose computers to \\noperate on. \\nThey need special hardware with sensors and \\neffectors. \\n9. Robotics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 53, 'page_label': '54'}, page_content='Artificial Intelligence  \\n  48 \\nRobot Locomotion \\nLocomotion is the mechanism that makes a robot capable of moving in its environment. There \\nare various types of locomotions: \\n\\uf0b7 Legged \\n\\uf0b7 Wheeled \\n\\uf0b7 Combination of Legged and Wheeled Locomotion \\n\\uf0b7 Tracked slip/skid  \\nLegged Locomotion \\n\\uf0b7 This type of locomotion consumes more power while demonstrating walk, jump, trot, \\nhop, climb up or down, etc.  \\n\\uf0b7 It requires more number of motors to accomplish a movement. It is suited for rough \\nas well as smooth terrain where irregular or too smooth surface makes it consume \\nmore power for a wheeled locomotion. It is little difficult to implement be cause of \\nstability issues. \\n\\uf0b7 It comes with the variety of one, two, four, and six legs.  If a robot has multiple legs \\nthen leg coordination is necessary for locomotion.  \\nThe total number of possible gaits (a periodic sequence of lift and release events for ea ch of \\nthe total legs) a robot can travel depends upon the number of its legs.  \\nIf a robot has k legs, then the number of possible events N = (2k-1)!. \\nIn case of a two-legged robot (k=2), the number of possible events is N = (2k-1)!  \\n= (2*2-1)! = 3! = 6.  \\nHence there are six possible different events: \\n1. Lifting the Left leg  \\n2. Releasing the Left leg  \\n3. Lifting the Right leg  \\n4. Releasing the Right leg  \\n5. Lifting both the legs together  \\n6. Releasing both the legs together. \\nIn case of k=6 legs , there are 39916800 possible events. Hence the complexity of robots is \\ndirectly proportional to the number of legs.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 54, 'page_label': '55'}, page_content='Artificial Intelligence  \\n  49 \\n \\nWheeled Locomotion \\nIt requires fewer number of motors to accomplish a movement.  It is little easy to implement \\nas there are less stability issues in c ase of more number of wheels. It is power efficient as \\ncompared to legged locomotion. \\n\\uf0b7 Standard wheel: Rotates around the wheel axle and around the contact \\n\\uf0b7 Castor wheel: Rotates around the wheel axle and the offset steering joint  \\n\\uf0b7 Swedish 45° and Swedish 90 ° wheels: Omni-wheel, rotates around the contact \\npoint, around the wheel axle, and around the rollers.  \\n\\uf0b7 Ball or spherical wheel: Omnidirectional wheel, technically difficult to implement. \\n \\nSlip/Skid Locomotion \\nIn this type, the vehicles use tracks as in a  tank. The robot is steered by moving the tracks \\nwith different speeds in the same or opposite direction.  It offers stability because of large \\ncontact area of track and ground.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 55, 'page_label': '56'}, page_content='Artificial Intelligence  \\n  50 \\nComponents of a Robot \\nRobots are constructed with the following: \\n\\uf0b7 Power Supply : The robots are powered by batteries, solar power, hydraulic, or \\npneumatic power sources.  \\n\\uf0b7 Actuators: They convert energy into movement. \\n\\uf0b7 Electric motors (AC/DC): They are required for rotational movement. \\n\\uf0b7 Pneumatic Air Muscles: They contract almost 40% when air is sucked in them. \\n\\uf0b7 Muscle Wires: They contract by 5% when electric current is passed through them. \\n\\uf0b7 Piezo Motors and Ultrasonic Motors: Best for industrial robots. \\n\\uf0b7 Sensors: They provide knowledge of real time information on the task  environment. \\nRobots are equipped with vision sensors  to be to compute the depth in the \\nenvironment. A tactile sensor imitates the mechanical properties of touch receptors of \\nhuman fingertips.  \\nComputer Vision \\nThis is a technology of AI with which the robots can see. The computer vision plays vital role \\nin the domains of safety, security, health, access, and entertainment. \\nComputer vision automatically extracts, analyzes, and comprehends useful information from \\na single image or an array of images. This process involves development of algorithm s to \\naccomplish automatic visual comprehension. \\nHardware of Computer Vision System \\nThis involves: \\n\\uf0b7 Power supply \\n\\uf0b7 Image acquisition device such as camera \\n\\uf0b7 a processor  \\n\\uf0b7 a software  \\n\\uf0b7 A display device for monitoring the system \\n\\uf0b7 Accessories such as camera stands, cables, and connectors \\nT asks of Computer Vision \\nOCR: In the domain of computers, Optical Character Reader, a software  to convert scanned \\ndocuments into editable text, which accompanies a scanner.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 56, 'page_label': '57'}, page_content='Artificial Intelligence  \\n  51 \\nFace Detection:  Many state-of-the-art cameras come with this feature , which enables to \\nread the face and take the picture of that perfect expression . It is used to let a user access \\nthe software on correct match.  \\nObject Recognition: They are installed in supermarkets, cameras , high-end cars such as \\nBMW, GM, and Volvo. \\nEstimating Position:  It is estimating position of a n object with respect to camera as in \\nposition of tumor in human’s body. \\nApplication Domains of Computer Vision \\n\\uf0b7 agriculture \\n\\uf0b7 autonomous vehicles \\n\\uf0b7 biometrics \\n\\uf0b7 character recognition \\n\\uf0b7 forensics, security, and surveillance \\n\\uf0b7 industrial quality inspection \\n\\uf0b7 face recognition \\n\\uf0b7 gesture analysis \\n\\uf0b7 geoscience \\n\\uf0b7 medical imagery \\n\\uf0b7 pollution monitoring \\n\\uf0b7 process control  \\n\\uf0b7 remote sensing \\n\\uf0b7 robotics \\n\\uf0b7 transport \\nApplications of Robotics \\nThe robotics has been instrumental in the various domains such as: \\n\\uf0b7 Industries: Robots are used for h andling material, cutting, welding, color coating, \\ndrilling, polishing, etc. \\n\\uf0b7 Military: Autonomous robots can reach inaccessible and hazardous zones during war. \\nA robot named Daksh, developed by Defense Research and Development Organization \\n(DRDO), is in function to destroy life-threatening objects safely. \\n\\uf0b7 Medicine: The robots are capable of carrying out hundreds of clinical tests \\nsimultaneously, rehabilitating permanently disabled people, and performing complex \\nsurgeries such as brain tumors.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 57, 'page_label': '58'}, page_content='Artificial Intelligence  \\n  52 \\n\\uf0b7 Exploration: The robot rock climbers used for space exploration, underwater drones \\nused for ocean exploration are to name a few. \\n\\uf0b7 Entertainment: Disney’s engineers have created hundreds of robots for movie \\nmaking.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 58, 'page_label': '59'}, page_content='Artificial Intelligence  \\n  53 \\nYet another research area in AI, neural networks, is inspired from the natural neural network \\nof human nervous system.  \\nWhat are Artificial Neural Networks (ANNs)? \\nThe inventor of the first neurocomputer, Dr. Robert Hecht -Nielsen, defines a neural network \\nas: \\n\"...a computing system made up of a number of simple, highly interconnected processing \\nelements, which process information by their dynamic state response to external \\ninputs.” \\nBasic Structure of ANNs \\nThe idea of ANNs is  based on the belief that working of human brain by making the right \\nconnections, can be imitated using silicon and wires as living neurons and dendrites. \\nThe human brain is composed of 100 billion nerve cells  called neurons. They are connected \\nto other thousand cells by Axons. Stimuli from external environment or inputs from sensory \\norgans are accepted by dendrites. These inputs create electric impulses, which quickly travel \\nthrough the neural network. A neuron can then send the message to other neuron to handle  \\nthe issue or does not send it forward. \\n \\nANNs are composed of multiple nodes, which imitate biological neurons of human brain. The \\nneurons are connected by links and they interact with each other. The nodes can take input \\n10. Neural Networks'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 59, 'page_label': '60'}, page_content='Artificial Intelligence  \\n  54 \\ndata and perform simple operation s on the data. The result of these operations is passed t o \\nother neurons. The output at each node is called its activation or node value. \\nEach link is associated with weight. ANNs are capable of learning, which takes place by \\naltering weight values. The following illustration shows a simple ANN: \\n \\nT ypes of Artificial Neural Networks \\nThere are two Artificial Neural Network topologies: FeedForward and Feedback. \\nFeedForward ANN \\nIn this ANN, the information flow is unidirectional. A unit sends information to other unit from \\nwhich it does not receive any information. There are no feedback loops. They are used in \\npattern generation/recognition/classification. They have fixed inputs and outputs. \\n \\nFeedback ANN \\nHere, feedback loops are allowed. They are used in content addressable memories.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 60, 'page_label': '61'}, page_content='Artificial Intelligence  \\n  55 \\n \\nWorking of ANNs  \\nIn the topology diagrams shown , each arrow represents a connection between two neurons \\nand indicates the pathway for the flow of information. Each connection has a weight, an integer \\nnumber that controls the signal between the two neurons.  \\nIf the network generates a “good or desired” output, there is no need to adjust the weights. \\nHowever, if the network generates a “poor or undesired” output or an error, then the system \\nalters the weights in order to improve subsequent results. \\nMachine Learning in ANNs \\nANNs are capable of learning  and they need to be trained . There are several learning \\nstrategies: \\n\\uf0b7 Supervised Learning: It involves a teacher that is scholar than the ANN itself. For \\nexample, the teacher feeds some example data about which the teacher already knows \\nthe answers.  \\nFor example, pattern recognizing. The ANN comes up with guesses while recognizing. \\nThen the teacher provides the ANN with the answers. The network then compares it \\nguesses with the teacher’s “correct” answers and makes adjustments according to \\nerrors.  \\n\\uf0b7 Unsupervised Learning: It is required when there is no example data set with known \\nanswers. For example, searching for a hidden pattern . In this case, clustering i.e.  \\ndividing a set of elements into groups according to some unknown pattern  is carried \\nout based on the existing data sets present.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 61, 'page_label': '62'}, page_content='Artificial Intelligence  \\n  56 \\n\\uf0b7 Reinforcement Learning : This strategy built on observation. The ANN  makes a \\ndecision by observing its environment . If  the observation is negative, the ne twork \\nadjusts its weights to be able to make a different required decision the next time.  \\nBack Propagation Algorithm \\nIt is the training or learning algorithm. It learns by example. If you submit to the algorithm \\nthe example of what you want the network to do, it changes the network’s weights so that it \\ncan produce desired output for a particular input on finishing the training.  \\nBack Propagation networks are ideal for simple Pattern Recognition and Mapping Tasks . \\nBayesian Networks (BN) \\nThese are the graphi cal structures used to represent the probabilistic relationship among a \\nset of random variables. Bayesian networks are also called Belief Networks or Bayes Nets. \\nBNs reason about uncertain domain. \\nIn these networks, each node represents a random variable  with specific propositions . For \\nexample, in a medical diagnosis domain, the node Cancer represents the proposition that a \\npatient has cancer.  \\nThe edges connecting the nodes represent probabilistic dependencies among those random \\nvariables. If out of two nodes, one is affecting the other then they must be directly connected \\nin the directions of the effect. The strength of the relationship between variables is quantified \\nby the probability associated with each node. \\nThere is an only constraint on the arcs in a BN that you cannot return to a node simply by \\nfollowing directed arcs. Hence the BNs are called Directed Acyclic Graphs (DAGs). \\nBNs are capable of handling multivalued variables simultaneously. The BN variables are \\ncomposed of two dimensions:  \\n1. Range of prepositions \\n2. Probability assigned to each of the prepositions. \\nConsider a finite set X = {X1, X2, …,Xn} of discrete random variables, where each variable  Xi \\nmay take values from a finite set, denoted by  Val(Xi). If there is a directed link from \\nvariable Xi to variable, Xj, then variable  Xi will be a parent of variable  Xj showing direct \\ndependencies between the variables.  \\nThe structure of BN is ideal for combining prior knowledge and observed data. BN can be used \\nto learn the causal relationships and underst and various problem domains and to predict \\nfuture events, even in case of missing data. \\nBuilding a Bayesian Network \\nA knowledge engineer can build a Bayesian network. There are a number of steps th e \\nknowledge engineer needs to take while building it.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 62, 'page_label': '63'}, page_content='Artificial Intelligence  \\n  57 \\nExample problem: Lung cancer. A patient has been suffering from breathlessness. He visits \\nthe doctor, suspecting he has lung cancer. The doctor knows that barring lung cancer, there \\nare various other possible diseases the patient might have such as tuberculosis and bronchitis.  \\nGather Relevant Information of Problem \\n\\uf0b7 Is the patient a smoker? If yes, then high chances of cancer and bronchitis.  \\n\\uf0b7 Is the patient exposed to air pollution? If yes, what sort of air pollution?  \\n\\uf0b7 Take an X-Ray positive X-ray would indicate either TB or lung cancer. \\nIdentify Interesting Variables \\nThe knowledge engineer tries to answer the questions:  \\n\\uf0b7 Which nodes to represent?  \\n\\uf0b7 What values can they take? In which state can they be?  \\nFor now let us consider nodes, with only discrete values. The variable must take on exactly \\none of these values at a time.  \\nCommon types of discrete nodes are:  \\n• Boolean nodes: They represent propositions, taking binary values TRUE (T) and FALSE \\n(F).  \\n• Ordered values: A node Pollution might represent and take values from {low, medium, \\nhigh} describing degree of a patient’s exposure to pollution.  \\n• Integral values: A node called Age might represent patient’s age with possible values \\nfrom 1 to 120. Even at this early stage, modeling choices are being made.  \\nPossible nodes and values for the lung cancer example: \\nNode Name Type Value \\n \\nPollution Binary {LOW, HIGH, MEDIUM} \\nSmoker Boolean {TRUE, FASLE} \\nLung-Cancer Boolean {TRUE, FASLE} \\nX-Ray Binary {Positive, Negative} \\n \\nCreate Arcs between Nodes \\nTopology of the network should capture qualitative relationships between variables.  \\nFor example, what causes a pa tient to have lung cancer? - Pollution and smoking. Then add \\narcs from node Pollution and node Smoker to node Lung-Cancer.  \\nSimilarly if patient has lung cancer, then X-ray result will be positive . Then add arcs from \\nLung-Cancer to X-Ray.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 63, 'page_label': '64'}, page_content='Artificial Intelligence  \\n  58 \\n \\nSpecify Topology \\nConventionally, BNs are laid out so that the arcs point from top to bottom. The set of parent \\nnodes of a node X is given by Parents(X). \\nThe Lung-Cancer node has two parents  (reasons or causes) :  Pollution and Smoker, while \\nnode Smoker is an ancestor of node X-Ray. Similarly, X-Ray is a child (consequence or \\neffects) of node Lung-Cancer and successor of nodes Smoker and Pollution.  \\nConditional Probabilities  \\nNow quantify the relation ships between connected nodes: this is done by specifying a \\nconditional probability distribution for each node. As only discrete variables are considered \\nhere, this takes the form of a Conditional Probability Table (CPT).  \\nFirst, for each node we need to look at all the possible combinations of values of those parent \\nnodes. Each such combination is called an instantiation of the parent set. For each distinct \\ninstantiation of parent node values, we need to specify the probability that the child will take.  \\nFor ex ample, the Lung-Cancer node’s parents are Pollution and Smoking. They take the \\npossible values = { (H,T), ( H,F), (L,T), (L,F)}. The CPT specifies the probability of cancer for \\neach of these cases as <0.05, 0.02, 0.03, 0.001> respectively. \\nEach node will have conditional probability associated as follows:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 64, 'page_label': '65'}, page_content='Artificial Intelligence  \\n  59 \\n \\nApplications of Neural Networks \\nThey can perform tasks that are easy for a human but difficult for a machine: \\n\\uf0b7 Aerospace: Autopilot aircrafts, aircraft fault detection.  \\n\\uf0b7 Automotive: Automobile guidance systems. \\n\\uf0b7 Military: Weapon steering, target tracking, object discrimination, facial recognition, \\nsignal/image identification. \\n\\uf0b7 Electronics: Code sequence prediction, IC chip layout, chip failure analy sis, machine \\nvision, voice synthesis. \\n\\uf0b7 Financial: Real estate appraisal, loan advisor, mortgage screening, corporate bond \\nrating, portfolio trading program, corporate financial analysis, currency value \\nprediction, document readers, credit application evaluators. \\n\\uf0b7 Industrial: Manufacturing process control, product design and analysis, quality \\ninspection systems, welding quality analy sis, paper quality predict ion,  chemical \\nproduct design analy sis, dynamic modeling of chemical process systems, machine \\nmaintenance analysis, project bidding, planning, and management.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 65, 'page_label': '66'}, page_content='Artificial Intelligence  \\n  60 \\n\\uf0b7 Medical: Cancer cell analysis, EEG and ECG analysis, prosthe tic design, transplant \\ntime optimizer. \\n\\uf0b7 Speech: Speech recognition, speech classification, text to speech conversion. \\n\\uf0b7 Telecommunications: Image and data compression, automated information \\nservices, real-time spoken language translation. \\n\\uf0b7 Transportation: Truck brake diagnosis, vehicle scheduling, routing systems.  \\n\\uf0b7 Software: Pattern Recognition in facial recognition, optical character recognition, etc. \\n\\uf0b7 Time Series Predict ion: ANNs are used to make predictions on stocks and natural \\ncalamities. \\n\\uf0b7 Signal Processing: Neural networks can be trained to process an audio signal and \\nfilter it appropriately in the hearing aids. \\n\\uf0b7 Control: ANNs are often used to make steering decisions of physical vehicles. \\n\\uf0b7 Anomaly Detection: As ANNs are expert at recognizing patterns, they can also be \\ntrained to generate an output when something unusual occurs that misfits the pattern.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 66, 'page_label': '67'}, page_content='Artificial Intelligence  \\n  61 \\nAI is developing with such an incredible speed, sometimes  it seems magical. There is an \\nopinion among researchers and developers that AI could grow so immensely strong that it \\nwould be difficult for humans to control.  \\nHumans developed AI systems by introducing into them every possible intelligence they could, \\nfor which the humans themselves now seem threatened.  \\nThreat to Privacy \\nAn AI program that recognizes speech and understands natural language is theoretically \\ncapable of understanding each conversation on e-mails and telephones. \\nThreat to Human Dignity \\nAI systems have already started replacing the human beings  in few industries. It should not \\nreplace people in the sectors where they are holding dignified positions  which are pertaining \\nto ethics such as nursing, surgeon, judge, police officer, etc.  \\nThreat to Safety \\nThe self-improving AI systems can become so mighty than humans that could be very difficult \\nto stop from achieving their goals, which may lead to unintended consequences.  \\n  \\n11. AI Issues'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2016-10-10T13:06:13+05:30', 'title': 'Artificial Intelligence', 'author': 'Manisha Shejwal', 'moddate': '2016-10-10T13:06:13+05:30', 'source': 'data.pdf', 'total_pages': 68, 'page': 67, 'page_label': '68'}, page_content='Artificial Intelligence  \\n  62 \\nHere is the list of frequently used terms in the domain of AI: \\n \\nTerm Meaning \\nAgent Agents are systems or software programs capable of autonomous, \\npurposeful and reasoning directed towards one or more goals.  They \\nare also called assistants, brokers, bots, droids, intelligent agents, and \\nsoftware agents. \\nAutonomous Robot Robot free from external control or influence and able to control itself \\nindependently. \\nBackward Chaining Strategy of working backward for Reason/Cause of a problem. \\nBlackboard It is the memory inside computer, which is used for communic ation \\nbetween the cooperating expert systems. \\nEnvironment It is the part of real or computational world inhabited by the agent. \\nForward Chaining Strategy of working forward for conclusion/solution of a problem. \\nHeuristics It is the knowledge based on Tr ial-and-error, evaluations, and \\nexperimentation. \\nKnowledge \\nEngineering Acquiring knowledge from human experts and other resources. \\nPercepts It is the format in which the agent obtains information about the \\nenvironment. \\nPruning Overriding unnecessary and irrelevant considerations in AI systems. \\nRule It is a format of representing knowledge base in Expert System. It is in \\nthe form of IF-THEN-ELSE. \\nShell A shell is a software that helps in designing inference engine, \\nknowledge base, and user interface of an expert system. \\nTask It is the goal the agent is tries to accomplish. \\nTuring Test A test developed by Allan Turing to test the intelligence of a machine \\nas compared to human intelligence.   \\n  \\n \\n \\n \\n \\n \\n  \\n12. AI Terminology')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ff6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap =100)\n",
    "final_docs = splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87cf1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 0, 'page_label': '1'}, page_content='Data Science and Machine Learning\\nMathematical and Statistical Methods\\nDirk P. Kroese, Zdravko I. Botev, Thomas Taimre, Radislav Vaisman\\n22nd August 2024'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 1, 'page_label': '2'}, page_content='To my wife and daughters: Lesley, Elise, and Jessica\\n— DPK\\nTo Sarah, Sofia, and my parents\\n— ZIB\\nTo my grandparents: Arno, Harry, Juta, and Maila\\n— TT\\nTo Valerie\\n— RV'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 2, 'page_label': 'vii'}, page_content='CONTENTS\\nPreface xiii\\nNotation xvii\\n1 Importing, Summarizing, and Visualizing Data 1\\n1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n1.2 Structuring Features According to Type . . . . . . . . . . . . . . . . . . 3\\n1.3 Summary Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n1.4 Summary Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.5 Visualizing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.5.1 Plotting Qualitative Variables . . . . . . . . . . . . . . . . . . . . 9\\n1.5.2 Plotting Quantitative Variables . . . . . . . . . . . . . . . . . . . 9\\n1.5.3 Data Visualization in a Bivariate Setting . . . . . . . . . . . . . . 12\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2 Statistical Learning 19\\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 2, 'page_label': 'vii'}, page_content='2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.2 Supervised and Unsupervised Learning . . . . . . . . . . . . . . . . . . . 20\\n2.3 Training and Test Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n2.4 Tradeo ffs in Statistical Learning . . . . . . . . . . . . . . . . . . . . . . 31\\n2.5 Estimating Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.5.1 In-Sample Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.5.2 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n2.6 Modeling Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n2.7 Multivariate Normal Models . . . . . . . . . . . . . . . . . . . . . . . . 45\\n2.8 Normal Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n2.9 Bayesian Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 2, 'page_label': 'vii'}, page_content='Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n3 Monte Carlo Methods 67\\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n3.2 Monte Carlo Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n3.2.1 Generating Random Numbers . . . . . . . . . . . . . . . . . . . 68\\n3.2.2 Simulating Random Variables . . . . . . . . . . . . . . . . . . . 69\\n3.2.3 Simulating Random Vectors and Processes . . . . . . . . . . . . . 74\\n3.2.4 Resampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n3.2.5 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . 78\\n3.3 Monte Carlo Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\\nvii'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 3, 'page_label': 'viii'}, page_content='viii Contents\\n3.3.1 Crude Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . 85\\n3.3.2 Bootstrap Method . . . . . . . . . . . . . . . . . . . . . . . . . . 88\\n3.3.3 Variance Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n3.4 Monte Carlo for Optimization . . . . . . . . . . . . . . . . . . . . . . . . 96\\n3.4.1 Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . . 96\\n3.4.2 Cross-Entropy Method . . . . . . . . . . . . . . . . . . . . . . . 100\\n3.4.3 Splitting for Optimization . . . . . . . . . . . . . . . . . . . . . . 103\\n3.4.4 Noisy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . 106\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n4 Unsupervised Learning 121\\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\n4.2 Risk and Loss in Unsupervised Learning . . . . . . . . . . . . . . . . . . 122'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 3, 'page_label': 'viii'}, page_content='4.2 Risk and Loss in Unsupervised Learning . . . . . . . . . . . . . . . . . . 122\\n4.3 Expectation–Maximization (EM) Algorithm . . . . . . . . . . . . . . . . 128\\n4.4 Empirical Distribution and Density Estimation . . . . . . . . . . . . . . . 131\\n4.5 Clustering via Mixture Models . . . . . . . . . . . . . . . . . . . . . . . 135\\n4.5.1 Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n4.5.2 EM Algorithm for Mixture Models . . . . . . . . . . . . . . . . . 137\\n4.6 Clustering via Vector Quantization . . . . . . . . . . . . . . . . . . . . . 142\\n4.6.1 K-Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\\n4.6.2 Clustering via Continuous Multiextremal Optimization . . . . . . 146\\n4.7 Hierarchical Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\\n4.8 Principal Component Analysis (PCA) . . . . . . . . . . . . . . . . . . . 153\\n4.8.1 Motivation: Principal Axes of an Ellipsoid . . . . . . . . . . . . . 154'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 3, 'page_label': 'viii'}, page_content='4.8.1 Motivation: Principal Axes of an Ellipsoid . . . . . . . . . . . . . 154\\n4.8.2 PCA and Singular Value Decomposition (SVD) . . . . . . . . . . 155\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n5 Regression 167\\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n5.2 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n5.3 Analysis via Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . 171\\n5.3.1 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . 171\\n5.3.2 Model Selection and Prediction . . . . . . . . . . . . . . . . . . . 172\\n5.3.3 Cross-Validation and Predictive Residual Sum of Squares . . . . . 173\\n5.3.4 In-Sample Risk and Akaike Information Criterion . . . . . . . . . 175\\n5.3.5 Categorical Features . . . . . . . . . . . . . . . . . . . . . . . . 177\\n5.3.6 Nested Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 180'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 3, 'page_label': 'viii'}, page_content='5.3.6 Nested Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\\n5.3.7 Coe fficient of Determination . . . . . . . . . . . . . . . . . . . . 181\\n5.4 Inference for Normal Linear Models . . . . . . . . . . . . . . . . . . . . 182\\n5.4.1 Comparing Two Normal Linear Models . . . . . . . . . . . . . . 183\\n5.4.2 Confidence and Prediction Intervals . . . . . . . . . . . . . . . . 186\\n5.5 Nonlinear Regression Models . . . . . . . . . . . . . . . . . . . . . . . . 188\\n5.6 Linear Models in Python . . . . . . . . . . . . . . . . . . . . . . . . . . 191\\n5.6.1 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\\n5.6.2 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\\n5.6.3 Analysis of Variance (ANOV A) . . . . . . . . . . . . . . . . . . 196'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 4, 'page_label': 'ix'}, page_content='Contents ix\\n5.6.4 Confidence and Prediction Intervals . . . . . . . . . . . . . . . . 198\\n5.6.5 Model Validation . . . . . . . . . . . . . . . . . . . . . . . . . . 199\\n5.6.6 Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 200\\n5.7 Generalized Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . 204\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\\n6 Regularization and Kernel Methods 215\\n6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\\n6.2 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\\n6.3 Reproducing Kernel Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . 222\\n6.4 Construction of Reproducing Kernels . . . . . . . . . . . . . . . . . . . . 224\\n6.4.1 Reproducing Kernels via Feature Mapping . . . . . . . . . . . . . 224\\n6.4.2 Kernels from Characteristic Functions . . . . . . . . . . . . . . . 225'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 4, 'page_label': 'ix'}, page_content='6.4.2 Kernels from Characteristic Functions . . . . . . . . . . . . . . . 225\\n6.4.3 Reproducing Kernels Using Orthonormal Features . . . . . . . . 227\\n6.4.4 Kernels from Kernels . . . . . . . . . . . . . . . . . . . . . . . . 229\\n6.5 Representer Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\\n6.6 Smoothing Cubic Splines . . . . . . . . . . . . . . . . . . . . . . . . . . 235\\n6.7 Gaussian Process Regression . . . . . . . . . . . . . . . . . . . . . . . . 238\\n6.8 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\\n7 Classification 251\\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\\n7.2 Classification Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\\n7.3 Classification via Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . 257'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 4, 'page_label': 'ix'}, page_content='7.3 Classification via Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . 257\\n7.4 Linear and Quadratic Discriminant Analysis . . . . . . . . . . . . . . . . 259\\n7.5 Logistic Regression and Softmax Classification . . . . . . . . . . . . . . 266\\n7.6 K-Nearest Neighbors Classification . . . . . . . . . . . . . . . . . . . . . 268\\n7.7 Support Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\\n7.8 Classification with Scikit-Learn . . . . . . . . . . . . . . . . . . . . . . . 277\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\\n8 Decision Trees and Ensemble Methods 287\\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\\n8.2 Top-Down Construction of Decision Trees . . . . . . . . . . . . . . . . . 289\\n8.2.1 Regional Prediction Functions . . . . . . . . . . . . . . . . . . . 290\\n8.2.2 Splitting Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . 291'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 4, 'page_label': 'ix'}, page_content='8.2.2 Splitting Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . 291\\n8.2.3 Termination Criterion . . . . . . . . . . . . . . . . . . . . . . . . 292\\n8.2.4 Basic Implementation . . . . . . . . . . . . . . . . . . . . . . . . 294\\n8.3 Additional Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . 298\\n8.3.1 Binary Versus Non-Binary Trees . . . . . . . . . . . . . . . . . . 298\\n8.3.2 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 298\\n8.3.3 Alternative Splitting Rules . . . . . . . . . . . . . . . . . . . . . 298\\n8.3.4 Categorical Variables . . . . . . . . . . . . . . . . . . . . . . . . 299\\n8.3.5 Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\\n8.4 Controlling the Tree Shape . . . . . . . . . . . . . . . . . . . . . . . . . 300\\n8.4.1 Cost-Complexity Pruning . . . . . . . . . . . . . . . . . . . . . . 303'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 5, 'page_label': 'x'}, page_content='x Contents\\n8.4.2 Advantages and Limitations of Decision Trees . . . . . . . . . . . 304\\n8.5 Bootstrap Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . 305\\n8.6 Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\\n8.7 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\\n9 Deep Learning 323\\n9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\\n9.2 Feed-Forward Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 326\\n9.3 Back-Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\\n9.4 Methods for Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n9.4.1 Steepest Descent . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n9.4.2 Levenberg–Marquardt Method . . . . . . . . . . . . . . . . . . . 336'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 5, 'page_label': 'x'}, page_content='9.4.2 Levenberg–Marquardt Method . . . . . . . . . . . . . . . . . . . 336\\n9.4.3 Limited-Memory BFGS Method . . . . . . . . . . . . . . . . . . 337\\n9.4.4 Adaptive Gradient Methods . . . . . . . . . . . . . . . . . . . . . 339\\n9.5 Examples in Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\\n9.5.1 Simple Polynomial Regression . . . . . . . . . . . . . . . . . . . 341\\n9.5.2 Image Classification . . . . . . . . . . . . . . . . . . . . . . . . 345\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\\nA Linear Algebra and Functional Analysis 355\\nA.1 Vector Spaces, Bases, and Matrices . . . . . . . . . . . . . . . . . . . . . 355\\nA.2 Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360\\nA.3 Complex Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . 361\\nA.4 Orthogonal Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . 362'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 5, 'page_label': 'x'}, page_content='A.4 Orthogonal Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\\nA.5 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . 363\\nA.5.1 Left- and Right-Eigenvectors . . . . . . . . . . . . . . . . . . . . 364\\nA.6 Matrix Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\\nA.6.1 (P)LU Decomposition . . . . . . . . . . . . . . . . . . . . . . . 368\\nA.6.2 Woodbury Identity . . . . . . . . . . . . . . . . . . . . . . . . . 370\\nA.6.3 Cholesky Decomposition . . . . . . . . . . . . . . . . . . . . . . 373\\nA.6.4 QR Decomposition and the Gram–Schmidt Procedure . . . . . . . 375\\nA.6.5 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . 376\\nA.6.6 Solving Structured Matrix Equations . . . . . . . . . . . . . . . . 379\\nA.7 Functional Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\\nA.8 Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 5, 'page_label': 'x'}, page_content='A.8 Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390\\nA.8.1 Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . 392\\nA.8.2 Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . 394\\nB Multivariate Di fferentiation and Optimization 397\\nB.1 Multivariate Di fferentiation . . . . . . . . . . . . . . . . . . . . . . . . . 397\\nB.1.1 Taylor Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . 400\\nB.1.2 Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\\nB.2 Optimization Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\\nB.2.1 Convexity and Optimization . . . . . . . . . . . . . . . . . . . . 403\\nB.2.2 Lagrangian Method . . . . . . . . . . . . . . . . . . . . . . . . . 406\\nB.2.3 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 6, 'page_label': 'xi'}, page_content='Contents xi\\nB.3 Numerical Root-Finding and Minimization . . . . . . . . . . . . . . . . . 408\\nB.3.1 Newton-Like Methods . . . . . . . . . . . . . . . . . . . . . . . 409\\nB.3.2 Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . . . . . 411\\nB.3.3 Normal Approximation Method . . . . . . . . . . . . . . . . . . 413\\nB.3.4 Nonlinear Least Squares . . . . . . . . . . . . . . . . . . . . . . 414\\nB.4 Constrained Minimization via Penalty Functions . . . . . . . . . . . . . . 415\\nC Probability and Statistics 421\\nC.1 Random Experiments and Probability Spaces . . . . . . . . . . . . . . . 421\\nC.2 Random Variables and Probability Distributions . . . . . . . . . . . . . . 422\\nC.3 Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426\\nC.4 Joint Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427\\nC.5 Conditioning and Independence . . . . . . . . . . . . . . . . . . . . . . . 428'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 6, 'page_label': 'xi'}, page_content='C.5 Conditioning and Independence . . . . . . . . . . . . . . . . . . . . . . . 428\\nC.5.1 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . 428\\nC.5.2 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428\\nC.5.3 Expectation and Covariance . . . . . . . . . . . . . . . . . . . . 429\\nC.5.4 Conditional Density and Conditional Expectation . . . . . . . . . 431\\nC.6 Functions of Random Variables . . . . . . . . . . . . . . . . . . . . . . . 431\\nC.7 Multivariate Normal Distribution . . . . . . . . . . . . . . . . . . . . . . 434\\nC.8 Convergence of Random Variables . . . . . . . . . . . . . . . . . . . . . 439\\nC.9 Law of Large Numbers and Central Limit Theorem . . . . . . . . . . . . 445\\nC.10 Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451\\nC.11 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453\\nC.12 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 6, 'page_label': 'xi'}, page_content='C.12 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454\\nC.12.1 Method of Moments . . . . . . . . . . . . . . . . . . . . . . . . 455\\nC.12.2 Maximum Likelihood Method . . . . . . . . . . . . . . . . . . . 456\\nC.13 Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\\nC.14 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458\\nD Python Primer 463\\nD.1 Getting Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463\\nD.2 Python Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\\nD.3 Types and Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466\\nD.4 Functions and Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 468\\nD.5 Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\\nD.6 Flow Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 6, 'page_label': 'xi'}, page_content='D.6 Flow Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471\\nD.7 Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472\\nD.8 Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474\\nD.9 Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476\\nD.10 NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478\\nD.10.1 Creating and Shaping Arrays . . . . . . . . . . . . . . . . . . . . 478\\nD.10.2 Slicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480\\nD.10.3 Array Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 481\\nD.10.4 Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 483\\nD.11 Matplotlib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483\\nD.11.1 Creating a Basic Plot . . . . . . . . . . . . . . . . . . . . . . . . 483'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 7, 'page_label': 'xii'}, page_content='xii Contents\\nD.12 Pandas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486\\nD.12.1 Series and DataFrame . . . . . . . . . . . . . . . . . . . . . . . . 486\\nD.12.2 Manipulating Data Frames . . . . . . . . . . . . . . . . . . . . . 487\\nD.12.3 Extracting Information . . . . . . . . . . . . . . . . . . . . . . . 489\\nD.12.4 Plotting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\\nD.13 Scikit-learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\\nD.13.1 Partitioning the Data . . . . . . . . . . . . . . . . . . . . . . . . 491\\nD.13.2 Standardization . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\\nD.13.3 Fitting and Prediction . . . . . . . . . . . . . . . . . . . . . . . . 493\\nD.13.4 Testing the Model . . . . . . . . . . . . . . . . . . . . . . . . . . 493\\nD.14 System Calls, URL Access, and Speed-Up . . . . . . . . . . . . . . . . . 494\\nBibliography 496\\nIndex 505'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 8, 'page_label': 'xiii'}, page_content='PREFACE\\nIn our present world of automation, cloud computing, algorithms, artificial intelligence,\\nand big data, few topics are as relevant asdata science and machine learning. Their recent\\npopularity lies not only in their applicability to real-life questions, but also in their natural\\nblending of many different disciplines, including mathematics, statistics, computer science,\\nengineering, science, and finance.\\nTo someone starting to learn these topics, the multitude of computational techniques\\nand mathematical ideas may seem overwhelming. Some may be satisfied with only learn-\\ning how to use off-the-shelf recipes to apply to practical situations. But what if the assump-\\ntions of the black-box recipe are violated? Can we still trust the results? How should the\\nalgorithm be adapted? To be able to truly understand data science and machine learning it\\nis important to appreciate the underlying mathematics and statistics, as well as the resulting\\nalgorithms.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 8, 'page_label': 'xiii'}, page_content='algorithms.\\nThe purpose of this book is to provide an accessible, yet comprehensive, account of\\ndata science and machine learning. It is intended for anyone interested in gaining a better\\nunderstanding of the mathematics and statistics that underpin the rich variety of ideas and\\nmachine learning algorithms in data science. Our viewpoint is that computer languages\\ncome and go, but the underlying key ideas and algorithms will remain forever and will\\nform the basis for future developments.\\nBefore we turn to a description of the topics in this book, we would like to say a\\nfew words about its philosophy. This book resulted from various courses in data science\\nand machine learning at the Universities of Queensland and New South Wales, Australia.\\nWhen we taught these courses, we noticed that students were eager to learn not only how\\nto apply algorithms but also to understand how these algorithms actually work. However,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 8, 'page_label': 'xiii'}, page_content='to apply algorithms but also to understand how these algorithms actually work. However,\\nmany existing textbooks assumed either too much background knowledge (e.g., measure\\ntheory and functional analysis) or too little (everything is a black box), and the information\\noverload from often disjointed and contradictory internet sources made it more difficult for\\nstudents to gradually build up their knowledge and understanding. We therefore wanted to\\nwrite a book about data science and machine learning that can be read as a linear story,\\nwith a substantial “backstory” in the appendices. The main narrative starts very simply and\\nbuilds up gradually to quite an advanced level. The backstory contains all the necessary\\nxiii'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 9, 'page_label': 'xiv'}, page_content='xiv Preface\\nbackground, as well as additional information, from linear algebra and functional analysis\\n(Appendix A), multivariate differentiation and optimization (Appendix B), and probability\\nand statistics (Appendix C). Moreover, to make the abstract ideas come alive, we believe\\nit is important that the reader sees actual implementations of the algorithms, directly trans-\\nlated from the theory. After some deliberation we have chosen Python as our programming\\nlanguage. It is freely available and has been adopted as the programming language of\\nchoice for many practitioners in data science and machine learning. It has many useful\\npackages for data manipulation (often ported from R) and has been designed to be easy to\\nprogram. A gentle introduction to Python is given in Appendix D.\\nTo keep the book manageable in size we had to be selective in our choice of topics.\\nImportant ideas and connections between various concepts are highlighted via keywords'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 9, 'page_label': 'xiv'}, page_content='Important ideas and connections between various concepts are highlighted via keywords\\nkeywords and page references (indicated by a ☞) in the margin. Key definitions and theorems are\\nhighlighted in boxes. Whenever feasible we provide proofs of theorems. Finally, we place\\ngreat importance on notation. It is often the case that once a consistent and concise system\\nof notation is in place, seemingly di fficult ideas suddenly become obvious. We use differ-\\nent fonts to distinguish between different types of objects. Vectors are denoted by letters in\\nboldface italics, x,X, and matrices by uppercase letters in boldface roman font, A,K. We\\nalso distinguish between random vectors and their values by using upper and lower case\\nletters, e.g., X (random vector) and x (its value or outcome). Sets are usually denoted by\\ncalligraphic letters G,H. The symbols for probability and expectation arePand E, respect-\\nively. Distributions are indicated by sans serif font, as in Bin and Gamma; exceptions are'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 9, 'page_label': 'xiv'}, page_content='ively. Distributions are indicated by sans serif font, as in Bin and Gamma; exceptions are\\nthe ubiquitous notations N and U for the normal and uniform distributions. A summary of\\nthe most important symbols and abbreviations is given on Pages xvii–xxi.☞xvii\\nData science provides the language and techniques necessary for understanding and\\ndealing with data. It involves the design, collection, analysis, and interpretation of nu-\\nmerical data, with the aim of extracting patterns and other useful information. Machine\\nlearning, which is closely related to data science, deals with the design of algorithms and\\ncomputer resources to learn from data. The organization of the book follows roughly the\\ntypical steps in a data science project: Gathering data to gain information about a research\\nquestion; cleaning, summarization, and visualization of the data; modeling and analysis of\\nthe data; translating decisions about the model into decisions and predictions about the re-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 9, 'page_label': 'xiv'}, page_content='the data; translating decisions about the model into decisions and predictions about the re-\\nsearch question. As this is a mathematics and statistics oriented book, most emphasis will\\nbe on modeling and analysis.\\nWe start in Chapter 1 with the reading, structuring, summarization, and visualization\\nof data using the data manipulation package pandas in Python. Although the material\\ncovered in this chapter requires no mathematical knowledge, it forms an obvious starting\\npoint for data science: to better understand the nature of the available data. In Chapter 2, we\\nintroduce the main ingredients of statistical learning. We distinguish between supervised\\nand unsupervised learning techniques, and discuss how we can assess the predictive per-\\nformance of (un)supervised learning methods. An important part of statistical learning is\\nthe modeling of data. We introduce various useful models in data science including linear,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 9, 'page_label': 'xiv'}, page_content='the modeling of data. We introduce various useful models in data science including linear,\\nmultivariate Gaussian, and Bayesian models. Many algorithms in machine learning and\\ndata science make use of Monte Carlo techniques, which is the topic of Chapter 3. Monte\\nCarlo can be used for simulation, estimation, and optimization. Chapter 4 is concerned\\nwith unsupervised learning, where we discuss techniques such as density estimation, clus-\\ntering, and principal component analysis. We then turn our attention to supervised learning'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 10, 'page_label': 'xv'}, page_content='Preface xv\\nin Chapter 5, and explain the ideas behind a broad class of regression models. Therein, we\\nalso describe how Python’sstatsmodels package can be used to define and analyze linear\\nmodels. Chapter 6 builds upon the previous regression chapter by developing the power-\\nful concepts of kernel methods and regularization, which allow the fundamental ideas of\\nChapter 5 to be expanded in an elegant way, using the theory of reproducing kernel Hilbert\\nspaces. In Chapter 7, we proceed with the classification task, which also belongs to the\\nsupervised learning framework, and consider various methods for classification, including\\nBayes classification, linear and quadratic discriminant analysis, K-nearest neighbors, and\\nsupport vector machines. In Chapter 8 we consider versatile methods for regression and\\nclassification that make use of tree structures. Finally, in Chapter 9, we consider the work-\\nings of neural networks and deep learning, and show that these learning algorithms have a'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 10, 'page_label': 'xv'}, page_content='ings of neural networks and deep learning, and show that these learning algorithms have a\\nsimple mathematical interpretation. An extensive range of exercises is provided at the end\\nof each chapter.\\nPython code and data sets for each chapter can be downloaded from the GitHub site:\\nhttps://github.com/DSML-book\\nAcknowledgments\\nSome of the Python code for Chapters 1 and 5 was adapted from [73]. We thank Benoit\\nLiquet for making this available, and Lauren Jones for translating the R code into Python.\\nWe thank all who through their comments, feedback, and suggestions have contributed\\nto this book, including Qibin Duan, Luke Taylor, Rémi Mouzayek, Harry Goodman, Bryce\\nStansfield, Ryan Tongs, Dillon Steyl, Bill Rudd, Nan Ye, Christian Hirsch, Chris van der\\nHeide, Sarat Moka, Aapeli Vuorinen, Joshua Ross, Giang Nguyen, and the anonymous\\nreferees. David Grubbs deserves a special accollade for his professionalism and attention\\nto detail in his role as Editor for this book.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 10, 'page_label': 'xv'}, page_content='to detail in his role as Editor for this book.\\nThe book was test-run during the 2019 Summer School of the Australian Mathemat-\\nical Sciences Institute. More than 80 bright upper-undergraduate (Honours) students used\\nthe book for the course Mathematical Methods for Machine Learning, taught by Zdravko\\nBotev. We are grateful for the valuable feedback that they provided.\\nOur special thanks go out to Robert Salomone, Liam Berry, Robin Carrick, and Sam\\nDaley, who commented in great detail on earlier versions of the entire book and wrote and\\nimproved our Python code. Their enthusiasm, perceptiveness, and kind assistance have\\nbeen invaluable.\\nOf course, none of this work would have been possible without the loving support,\\npatience, and encouragement from our families, and we thank them with all our hearts.\\nThis book was financially supported by the Australian Research Council Centre of\\nExcellence for Mathematical & Statistical Frontiers, under grant number CE140100049.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 10, 'page_label': 'xv'}, page_content='Excellence for Mathematical & Statistical Frontiers, under grant number CE140100049.\\nDirk Kroese, Zdravko Botev,\\nThomas Taimre, and Radislav Vaisman\\nBrisbane and Sydney'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 11, 'page_label': 'xvi'}, page_content='xvi'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 12, 'page_label': 'xvii'}, page_content='NOTATION\\nWe could, of course, use any notation we want; do not laugh at notations;\\ninvent them, they are powerful. In fact, mathematics is, to a large extent, in-\\nvention of better notations.\\nRichard P. Feynman\\nWe have tried to use a notation system that is, in order of importance, simple, descript-\\nive, consistent, and compatible with historical choices. Achieving all of these goals all of\\nthe time would be impossible, but we hope that our notation helps to quickly recognize\\nthe type or “flavor” of certain mathematical objects (vectors, matrices, random vectors,\\nprobability measures, etc.) and clarify intricate ideas.\\nWe make use of various typographical aids, and it will be beneficial for the reader to\\nbe aware of some of these.\\n• Boldface font is used to indicate composite objects, such as column vectors x =\\n[x1,..., xn]⊤ and matrices X = [xi j]. Note also the di fference between the upright bold\\nfont for matrices and the slanted bold font for vectors.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 12, 'page_label': 'xvii'}, page_content='font for matrices and the slanted bold font for vectors.\\n• Random variables are generally specified with upper case roman letters X,Y,Z and their\\noutcomes with lower case letters x,y,z. Random vectors are thus denoted in upper case\\nslanted bold font: X = [X1,..., Xn]⊤.\\n• Sets of vectors are generally written in calligraphic font, such as X, but the set of real\\nnumbers uses the common blackboard bold font R. Expectation and probability also use\\nthe latter font.\\n• Probability distributions use a sans serif font, such as Bin and Gamma. Exceptions to\\nthis rule are the “standard” notations N and U for the normal and uniform distributions.\\n• We often omit brackets when it is clear what the argument is of a function or operator.\\nFor example, we prefer EX2 to E[X2].\\nxvii'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 13, 'page_label': 'xviii'}, page_content='xviii Notation\\n• We employ color to emphasize that certain words refer to a dataset, function, or\\npackage in Python. All code is written in typewriter font. To be compatible with past\\nnotation choices, we introduced a special blue symbol X for the model (design) matrix of\\na linear model.\\n• Important notation such as T, g, g∗ is often defined in a mnemonic way, such as Tfor\\n“training”, g for “guess”, g∗for the “star” (that is, optimal) guess, and ℓfor “loss”.\\n• We will occasionally use a Bayesian notation convention in which the same symbol is\\nused to denote different (conditional) probability densities. In particular, instead of writing\\nfX(x) and fX |Y (x |y) for the probability density function (pdf) of X and the conditional pdf\\nof X given Y, we simply write f (x) and f (x |y). This particular style of notation can be of\\ngreat descriptive value, despite its apparent ambiguity.\\nGeneral font/notation rules\\nx scalar\\nx vector\\nX random vector\\nX matrix\\nX set\\nbx estimate or approximation'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 13, 'page_label': 'xviii'}, page_content='x scalar\\nx vector\\nX random vector\\nX matrix\\nX set\\nbx estimate or approximation\\nx∗ optimal\\nx average\\nCommon mathematical symbols\\n∀ for all\\n∃ there exists\\n∝ is proportional to\\n⊥ is perpendicular to\\n∼ is distributed as\\niid\\n∼, ∼iid are independent and identically distributed as\\napprox.\\n∼ is approximately distributed as\\n∇f gradient of f\\n∇2 f Hessian of f\\nf ∈Cp f has continuous derivatives of order p\\n≈ is approximately\\n≃ is asymptotically\\n≪ is much smaller than\\n⊕ direct sum'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 14, 'page_label': 'xix'}, page_content='Notation xix\\n⊙ elementwise product\\n∩ intersection\\n∪ union\\n:=, =: is defined as\\na.s.\\n−→ converges almost surely to\\nd\\n−→ converges in distribution to\\nP\\n−→ converges in probability to\\nLp\\n−→ converges in Lp-norm to\\n∥·∥ Euclidean norm\\n⌈x⌉ smallest integer larger than x\\n⌊x⌋ largest integer smaller than x\\nx+ max{x,0}\\nMatrix/vector notation\\nA⊤, x⊤ transpose of matrix A or vector x\\nA−1 inverse of matrix A\\nA+ pseudo-inverse of matrix A\\nA−⊤ inverse of matrix A⊤or transpose of A−1\\nA ≻0 matrix A is positive definite\\nA ⪰0 matrix A is positive semidefinite\\ndim(x) dimension of vector x\\ndet(A) determinant of matrix A\\n|A| absolute value of the determinant of matrix A\\ntr(A) trace of matrix A\\nReserved letters and words\\nC set of complex numbers\\nd di fferential symbol\\nE expectation\\ne the number 2 .71828 ...\\nf probability density (discrete or continuous)\\ng prediction function\\n1{A}or 1A indicator function of set A\\ni the square root of −1\\nℓ risk: expected loss'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 15, 'page_label': 'xx'}, page_content='xx Notation\\nLoss loss function\\nln (natural) logarithm\\nN set of natural numbers {0,1,... }\\nO big-O order symbol: f (x) = O(g(x)) if |f (x)|⩽αg(x) for some constant αas\\nx →a\\no little-o order symbol: f (x) = o(g(x)) if f (x)/g(x) →0 as x →a\\nP probability measure\\nπ the number 3.14159 ...\\nR set of real numbers (one-dimensional Euclidean space)\\nRn n-dimensional Euclidean space\\nR+ positive real line: [0,∞)\\nτ deterministic training set\\nT random training set\\nX model (design) matrix\\nZ set of integers {..., −1,0,1,... }\\nProbability distributions\\nBer Bernoulli\\nBeta beta\\nBin binomial\\nExp exponential\\nGeom geometric\\nGamma gamma\\nF Fisher–Snedecor F\\nN normal or Gaussian\\nPareto Pareto\\nPoi Poisson\\nt Student’s t\\nU uniform\\nAbbreviations and acronyms\\ncdf cumulative distribution function\\nCMC crude Monte Carlo\\nCE cross-entropy\\nEM expectation–maximization\\nGP Gaussian process\\nKDE Kernel density estimate /estimator'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 16, 'page_label': 'xxi'}, page_content='Notation xxi\\nKL Kullback–Leibler\\nKKT Karush–Kuhn–Tucker\\niid independent and identically distributed\\nMAP maximum a posteriori\\nMCMC Markov chain Monte Carlo\\nMLE maximum likelihood estimator /estimate\\nOOB out-of-bag\\nPCA principal component analysis\\npdf probability density function (discrete or continuous)\\nSVD singular value decomposition'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 17, 'page_label': 'xxii'}, page_content='xxii'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 18, 'page_label': '1'}, page_content='CHAPTER 1\\nIMPORTING , SUMMARIZING , AND\\nVISUALIZING DATA\\nThis chapter describes where to find useful data sets, how to load them into Python,\\nand how to (re)structure the data. We also discuss various ways in which the data can\\nbe summarized via tables and figures. Which type of plots and numerical summaries\\nare appropriate depends on the type of the variable(s) in play. Readers unfamiliar with\\nPython are advised to read Appendix D first.\\n1.1 Introduction\\nData comes in many shapes and forms, but can generally be thought of as being the result\\nof some random experiment — an experiment whose outcome cannot be determined in\\nadvance, but whose workings are still subject to analysis. Data from a random experiment\\nare often stored in a table or spreadsheet. A statistical convention is to denote variables —\\noften called features features— as columns and the individual items (or units) as rows. It is useful\\nto think of three types of columns in such a spreadsheet:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 18, 'page_label': '1'}, page_content='to think of three types of columns in such a spreadsheet:\\n1. The first column is usually an identifier or index column, where each unit /row is\\ngiven a unique name or ID.\\n2. Certain columns (features) can correspond to the design of the experiment, specify-\\ning, for example, to which experimental group the unit belongs. Often the entries in\\nthese columns are deterministic; that is, they stay the same if the experiment were to\\nbe repeated.\\n3. Other columns represent the observed measurements of the experiment. Usually,\\nthese measurements exhibit variability; that is, they would change if the experiment\\nwere to be repeated.\\nThere are many data sets available from the Internet and in software packages. A well-\\nknown repository of data sets is the Machine Learning Repository maintained by the Uni-\\nversity of California at Irvine (UCI), found at https://archive.ics.uci.edu/.\\n1'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 19, 'page_label': '2'}, page_content=\"2 Introduction\\nThese data sets are typically stored in a CSV (comma separated values) format, which\\ncan be easily read into Python. For example, to access theabalone data set from this web-\\nsite with Python, download the file to your working directory, import the pandas package\\nvia\\nimport pandas as pd\\nand read in the data as follows:\\nabalone = pd.read_csv( 'abalone.data ',header = None)\\nIt is important to add header = None, as this lets Python know that the first line of the\\nCSV does not contain the names of the features, as it assumes so by default. The data set\\nwas originally used to predict the age of abalone from physical measurements, such as\\nshell weight and diameter.\\nAnother useful repository of over 1000 data sets from various packages in the R pro-\\ngramming language, collected by Vincent Arel-Bundock, can be found at:\\nhttps://vincentarelbundock.github.io/Rdatasets/datasets.html.\\nFor example, to read Fisher’s famous iris data set from R’s datasets package into Py-\\nthon, type:\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 19, 'page_label': '2'}, page_content=\"For example, to read Fisher’s famous iris data set from R’s datasets package into Py-\\nthon, type:\\nurlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/ '\\ndataname = 'datasets/iris.csv '\\niris = pd.read_csv(urlprefix + dataname)\\nThe iris data set contains four physical measurements (sepal /petal length/width) on\\n50 specimens (each) of 3 species of iris: setosa, versicolor, and virginica. Note that in this\\ncase the headers are included. The output of read_csv is a DataFrame object, which is\\npandas’s implementation of a spreadsheet; see Section D.12.1. The DataFrame method☞486\\nhead gives the first few rows of the DataFrame, including the feature names. The number\\nof rows can be passed as an argument and is 5 by default. For the iris DataFrame, we\\nhave:\\niris.head()\\nUnnamed: 0 Sepal.Length ... Petal.Width Species\\n0 1 5.1 ... 0.2 setosa\\n1 2 4.9 ... 0.2 setosa\\n2 3 4.7 ... 0.2 setosa\\n3 4 4.6 ... 0.2 setosa\\n4 5 5.0 ... 0.2 setosa\\n[5 rows x 6 columns]\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 19, 'page_label': '2'}, page_content=\"2 3 4.7 ... 0.2 setosa\\n3 4 4.6 ... 0.2 setosa\\n4 5 5.0 ... 0.2 setosa\\n[5 rows x 6 columns]\\nThe names of the features can be obtained via thecolumnsattribute of the DataFrame\\nobject, as in iris.columns. Note that the first column is a duplicate index column, whose\\nname (assigned by pandas) is 'Unnamed: 0'. We can drop this column and reassign the\\nirisobject as follows:\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 20, 'page_label': '3'}, page_content=\"Importing, Summarizing, and Visualizing Data 3\\niris = iris.drop( 'Unnamed: 0 ',1)\\nThe data for each feature (corresponding to its specific name) can be accessed by using\\nPython’s slicing notation []. For example, the object iris[’Sepal.Length’] contains\\nthe 150 sepal lengths.\\nThe first three rows of the abalone data set from the UCI repository can be found as\\nfollows:\\nabalone.head(3)\\n0 1 2 3 4 5 6 7 8\\n0 M 0.455 0.365 0.095 0.5140 0.2245 0.1010 0.150 15\\n1 M 0.350 0.265 0.090 0.2255 0.0995 0.0485 0.070 7\\n2 F 0.530 0.420 0.135 0.6770 0.2565 0.1415 0.210 9\\nHere, the missing headers have been assigned according to the order of the natural\\nnumbers. The names should correspond to Sex, Length, Diameter, Height, Whole weight,\\nShucked weight, Viscera weight, Shell weight, and Rings, as described in the file with the\\nname abalone.nameson the UCI website. We can manually add the names of the features\\nto the DataFrameby reassigning the columns attribute, as in:\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 20, 'page_label': '3'}, page_content=\"to the DataFrameby reassigning the columns attribute, as in:\\nabalone.columns = [ 'Sex', 'Length ', 'Diameter ', 'Height ',\\n'Whole weight ','Shucked weight ', 'Viscera weight ', 'Shell weight ',\\n'Rings ']\\n1.2 Structuring Features According to Type\\nWe can generally classify features as either quantitative or qualitative.Quantitative Quantitativefeatures\\npossess “numerical quantity”, such as height, age, number of births, etc., and can either be\\ncontinuous or discrete. Continuous quantitative features take values in a continuous range\\nof possible values, such as height, voltage, or crop yield; such features capture the idea\\nthat measurements can always be made more precisely. Discrete quantitative features have\\na countable number of possibilities, such as a count.\\nIn contrast, qualitative qualitativefeatures do not have a numerical meaning, but their possible\\nvalues can be divided into a fixed number of categories, such as {M,F}for gender or {blue,\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 20, 'page_label': '3'}, page_content='values can be divided into a fixed number of categories, such as {M,F}for gender or {blue,\\nblack, brown, green}for eye color. For this reason such features are also calledcategorical categorical.\\nA simple rule of thumb is: if it does not make sense to average the data, it is categorical.\\nFor example, it does not make sense to average eye colors. Of course it is still possible to\\nrepresent categorical data with numbers, such as 1 = blue, 2 = black, 3 = brown, but such\\nnumbers carry no quantitative meaning. Categorical features are often called factors factors.\\nWhen manipulating, summarizing, and displaying data, it is important to correctly spe-\\ncify the type of the variables (features). We illustrate this using the nutrition_elderly\\ndata set from [73], which contains the results of a study involving nutritional measure-\\nments of thirteen features (columns) for 226 elderly individuals (rows). The data set can be\\nobtained from:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 20, 'page_label': '3'}, page_content='obtained from:\\nhttp://www.biostatisticien.eu/springeR/nutrition_elderly.xls.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 21, 'page_label': '4'}, page_content=\"4 Structuring Features According to Type\\nExcel files can be read directly into pandas via the read_excel method:\\nxls = 'http://www.biostatisticien.eu/springeR/nutrition_elderly.xls '\\nnutri = pd.read_excel(xls)\\nThis creates a DataFrameobject nutri. The first three rows are as follows:\\npd.set_option( 'display.max_columns ', 8) # to fit display\\nnutri.head(3)\\ngender situation tea ... cooked_fruit_veg chocol fat\\n0 2 1 0 ... 4 5 6\\n1 2 1 1 ... 5 1 4\\n2 2 1 0 ... 2 5 4\\n[3 rows x 13 columns]\\nYou can check the type (or structure) of the variables via theinfo method of nutri.\\nnutri.info()\\n<class 'pandas.core.frame.DataFrame '>\\nRangeIndex: 226 entries, 0 to 225\\nData columns (total 13 columns):\\ngender 226 non-null int64\\nsituation 226 non-null int64\\ntea 226 non-null int64\\ncoffee 226 non-null int64\\nheight 226 non-null int64\\nweight 226 non-null int64\\nage 226 non-null int64\\nmeat 226 non-null int64\\nfish 226 non-null int64\\nraw_fruit 226 non-null int64\\ncooked_fruit_veg 226 non-null int64\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 21, 'page_label': '4'}, page_content='fish 226 non-null int64\\nraw_fruit 226 non-null int64\\ncooked_fruit_veg 226 non-null int64\\nchocol 226 non-null int64\\nfat 226 non-null int64\\ndtypes: int64(13)\\nmemory usage: 23.0 KB\\nAll 13 features in nutri are (at the moment) interpreted by Python as quantitative\\nvariables, indeed as integers, simply because they have been entered as whole numbers.\\nThe meaning of these numbers becomes clear when we consider the description of the\\nfeatures, given in Table 1.2. Table 1.1 shows how the variable types should be classified.\\nTable 1.1: The feature types for the data frame nutri.\\nQualitative gender, situation, fat\\nmeat, fish, raw_fruit, cooked_fruit_veg, chocol\\nDiscrete quantitative tea, coffee\\nContinuous quantitative height, weight, age'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 22, 'page_label': '5'}, page_content='Importing, Summarizing, and Visualizing Data 5\\nTable 1.2: Description of the variables in the nutritional study [73].\\nFeature Description Unit orCoding\\ngender Gender 1 =Male; 2=Female\\nsituation Family status\\n1=Single\\n2=Living with spouse\\n3=Living with family\\n4=Living with someone else\\ntea Daily consumption of tea Number of cups\\ncoffee Daily consumption of coffee Number of cups\\nheight Height cm\\nweight Weight (actually: mass) kg\\nage Age at date of interview Years\\nmeat Consumption of meat\\n0=Never\\n1=Less than once a week\\n2=Once a week\\n3=2–3 times a week\\n4=4–6 times a week\\n5=Every day\\nfish Consumption of fish As in meat\\nraw_fruit Consumption of raw fruits As inmeat\\ncooked_fruit_veg Consumption of cookedAs inmeatfruits and vegetables\\nchocol Consumption of chocolate As inmeat\\nfat\\n1=Butter\\n2=Margarine\\n3=Peanut oil\\nType of fat used 4 =Sunflower oil\\nfor cooking 5 =Olive oil\\n6=Mix of vegetable oils (e.g., Isio4)\\n7=Colza oil\\n8=Duck or goose fat'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 22, 'page_label': '5'}, page_content=\"for cooking 5 =Olive oil\\n6=Mix of vegetable oils (e.g., Isio4)\\n7=Colza oil\\n8=Duck or goose fat\\nNote that the categories of the qualitative features in the second row of Table 1.1,meat,\\n. . . ,chocolhave a natural order. Such qualitative features are sometimes calledordinal, in\\ncontrast to qualitative features without order, which are called nominal. We will not make\\nsuch a distinction in this book.\\nWe can modify the Python value and type for each categorical feature, using the\\nreplace and astype methods. For categorical features, such as gender, we can replace\\nthe value 1 with 'Male' and 2 with 'Female', and change the type to 'category' as\\nfollows.\\nDICT = {1: 'Male ', 2: 'Female '} # dictionary specifies replacement\\nnutri[ 'gender '] = nutri[ 'gender '].replace(DICT).astype( 'category ')\\nThe structure of the other categorical-type features can be changed in a similar way.\\nContinuous features such as heightshould have type float:\\nnutri[ 'height '] = nutri[ 'height '].astype( float )\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 23, 'page_label': '6'}, page_content=\"6 Summary Tables\\nWe can repeat this for the other variables (see Exercise 2) and save this modified data\\nframe as a CSV file, by using the pandas method to_csv.\\nnutri.to_csv( 'nutri.csv ',index=False)\\n1.3 Summary Tables\\nIt is often useful to summarize a large spreadsheet of data in a more condensed form. A\\ntable of counts or a table of frequencies makes it easier to gain insight into the underlying\\ndistribution of a variable, especially if the data are qualitative. Such tables can be obtained\\nwith the methods describe and value_counts.\\nAs a first example, we load the nutri DataFrame, which we restructured and saved\\n(see previous section) as 'nutri.csv', and then construct a summary for the feature\\n(column) 'fat'.\\nnutri = pd.read_csv( 'nutri.csv ')\\nnutri[ 'fat'].describe()\\ncount 226\\nunique 8\\ntop sunflower\\nfreq 68\\nName: fat, dtype: object\\nWe see that there are 8 di fferent types of fat used and that sunflower has the highest\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 23, 'page_label': '6'}, page_content=\"We see that there are 8 di fferent types of fat used and that sunflower has the highest\\ncount, with 68 out of 226 individuals using this type of cooking fat. The method\\nvalue_counts gives the counts for the different fat types.\\nnutri[ 'fat'].value_counts()\\nsunflower 68\\npeanut 48\\nolive 40\\nmargarine 27\\nIsio4 23\\nbutter 15\\nduck 4\\ncolza 1\\nName: fat, dtype: int64\\nColumn labels are also attributes of a DataFrame, and nutri.fat, for example, is\\nexactly the same object as nutri['fat'].\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 24, 'page_label': '7'}, page_content='Importing, Summarizing, and Visualizing Data 7\\nIt is also possible to use crosstab to cross tabulate between two or more variables, cross tabulate\\ngiving a contingency table:\\npd.crosstab(nutri.gender, nutri.situation)\\nsituation Couple Family Single\\ngender\\nFemale 56 7 78\\nMale 63 2 20\\nWe see, for example, that the proportion of single men is substantially smaller than the\\nproportion of single women in the data set of elderly people. To add row and column totals\\nto a table, use margins=True.\\npd.crosstab(nutri.gender, nutri.situation, margins=True)\\nsituation Couple Family Single All\\ngender\\nFemale 56 7 78 141\\nMale 63 2 20 85\\nAll 119 9 98 226\\n1.4 Summary Statistics\\nIn the following, x = [x1,..., xn]⊤ is a column vector of n numbers. For our nutri data,\\nthe vector x could, for example, correspond to the heights of the n = 226 individuals.\\nThe sample mean sample meanof x, denoted by x, is simply the average of the data values:\\nx = 1\\nn\\nnX\\ni=1\\nxi.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 24, 'page_label': '7'}, page_content=\"x = 1\\nn\\nnX\\ni=1\\nxi.\\nUsing the mean method in Python for the nutri data, we have, for instance:\\nnutri[ 'height '].mean()\\n163.96017699115043\\nThe p-sample quantile sample quantile(0 < p <1) of x is a value x such that at least a fraction p of the\\ndata is less than or equal to x and at least a fraction 1−p of the data is greater than or equal\\nto x. The sample median sample medianis the sample 0 .5-quantile. The p-sample quantile is also called\\nthe 100 ×p percentile. The 25, 50, and 75 sample percentiles are called the first, second,\\nand third quartiles quartilesof the data. For the nutri data they are obtained as follows.\\nnutri[ 'height '].quantile(q=[0.25,0.5,0.75])\\n0.25 157.0\\n0.50 163.0\\n0.75 170.0\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 25, 'page_label': '8'}, page_content=\"8 Visualizing Data\\nThe sample mean and median give information about thelocation of the data, while the\\ndistance between sample quantiles (say the 0.1 and 0.9 quantiles) gives some indication of\\nthe dispersion (spread) of the data. Other measures for dispersion are the sample range,sample range\\nmaxi xi −mini xi, the sample variancesample variance\\ns2 = 1\\nn −1\\nnX\\ni=1\\n(xi −x )2, (1.1)\\nand the sample standard deviation s =\\n√\\ns2. For the nutri data, the range (in cm) is:sample\\nstandard\\ndeviation\\n☞455\\nnutri[ 'height '].max () - nutri[ 'height '].min ()\\n48.0\\nThe variance (in cm2) is:\\nround (nutri[ 'height '].var(), 2) # round to two decimal places\\n81.06\\nAnd the standard deviation can be found via:\\nround (nutri[ 'height '].std(), 2)\\n9.0\\nWe already encountered thedescribe method in the previous section for summarizing\\nqualitative features, via the most frequent count and the number of unique elements. When\\napplied to a quantitative feature, it returns instead the minimum, maximum, mean, and the\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 25, 'page_label': '8'}, page_content=\"applied to a quantitative feature, it returns instead the minimum, maximum, mean, and the\\nthree quartiles. For example, the 'height' feature in the nutri data has the following\\nsummary statistics.\\nnutri[ 'height '].describe()\\ncount 226.000000\\nmean 163.960177\\nstd 9.003368\\nmin 140.000000\\n25\\\\% 157.000000\\n50\\\\% 163.000000\\n75\\\\% 170.000000\\nmax 188.000000\\nName: height, dtype: float64\\n1.5 Visualizing Data\\nIn this section we describe various methods for visualizing data. The main point we would\\nlike to make is that the way in which variables are visualized should always be adapted to\\nthe variable types; for example, qualitative data should be plotted differently from quantit-\\native data.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 26, 'page_label': '9'}, page_content=\"Importing, Summarizing, and Visualizing Data 9\\nFor the rest of this section, it is assumed that matplotlib.pyplot, pandas, and\\nnumpy, have been imported in the Python code as follows.\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\n1.5.1 Plotting Qualitative Variables\\nSuppose we wish to display graphically how many elderly people are living by themselves,\\nas a couple, with family, or other. Recall that the data are given in the situationcolumn\\nof our nutri data. Assuming that we already restructured the data, as in Section 1.2, we ☞ 3\\ncan make a barplot of the number of people in each category via the plt.bar function of barplot\\nthe standard matplotlib plotting library. The inputs are the x-axis positions, heights, and\\nwidths of each bar respectively.\\nwidth = 0.35 # the width of the bars\\nx = [0, 0.8, 1.6] # the bar positions on x-axis\\nsituation_counts=nutri[ 'situation '].value_counts()\\nplt.bar(x, situation_counts, width, edgecolor = 'black ')\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 26, 'page_label': '9'}, page_content=\"plt.bar(x, situation_counts, width, edgecolor = 'black ')\\nplt.xticks(x, situation_counts.index)\\nplt.show()\\nCouple Single Family\\n0\\n25\\n50\\n75\\n100\\n125\\nFigure 1.1: Barplot for the qualitative variable 'situation'.\\n1.5.2 Plotting Quantitative Variables\\nWe now present a few useful methods for visualizing quantitative data, again using the\\nnutri data set. We will first focus on continuous features (e.g.,'age') and then add some\\nspecific graphs related to discrete features (e.g., 'tea'). The aim is to describe the variab-\\nility present in a single feature. This typically involves a central tendency, where observa-\\ntions tend to gather around, with fewer observations further away. The main aspects of the\\ndistribution are the location (or center) of the variability, thespread of the variability (how\\nfar the values extend from the center), and theshape of the variability; e.g., whether or not\\nvalues are spread symmetrically on either side of the center.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 27, 'page_label': '10'}, page_content=\"10 Visualizing Data\\n1.5.2.1 Boxplot\\nA boxplot can be viewed as a graphical representation of the five-number summary ofboxplot\\nthe data consisting of the minimum, maximum, and the first, second, and third quartiles.\\nFigure 1.2 gives a boxplot for the 'age' feature of the nutri data.\\nplt.boxplot(nutri[ 'age'],widths=width,vert=False)\\nplt.xlabel( 'age')\\nplt.show()\\nThe widthsparameter determines the width of the boxplot, which is by default plotted\\nvertically. Setting vert=Falseplots the boxplot horizontally, as in Figure 1.2.\\n65\\n 70\\n 75\\n 80\\n 85\\n 90\\nage\\n1\\nFigure 1.2: Boxplot for 'age'.\\nThe box is drawn from the first quartile (Q1) to the third quartile (Q3). The vertical line\\ninside the box signifies the location of the median. So-called “whiskers” extend to either\\nside of the box. The size of the box is called the interquartile range: IQR = Q3 −Q1. The\\nleft whisker extends to the largest of (a) the minimum of the data and (b) Q1 −1.5 IQR.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 27, 'page_label': '10'}, page_content='left whisker extends to the largest of (a) the minimum of the data and (b) Q1 −1.5 IQR.\\nSimilarly, the right whisker extends to the smallest of (a) the maximum of the data and\\n(b) Q3 + 1.5 IQR. Any data point outside the whiskers is indicated by a small hollow dot,\\nindicating a suspicious or deviant point (outlier). Note that a boxplot may also be used for\\ndiscrete quantitative features.\\n1.5.2.2 Histogram\\nA histogram is a common graphical representation of the distribution of a quantitativehistogram\\nfeature. We start by breaking the range of the values into a number of bins or classes.\\nWe tally the counts of the values falling in each bin and then make the plot by drawing\\nrectangles whose bases are the bin intervals and whose heights are the counts. In Python\\nwe can use the function plt.hist. For example, Figure 1.3 shows a histogram of the 226\\nages in nutri, constructed via the following Python code.\\nweights = np.ones_like(nutri.age)/nutri.age.count()'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 27, 'page_label': '10'}, page_content=\"weights = np.ones_like(nutri.age)/nutri.age.count()\\nplt.hist(nutri.age,bins=9,weights=weights,facecolor= 'cyan ',\\nedgecolor= 'black ', linewidth=1)\\nplt.xlabel( 'age')\\nplt.ylabel( 'Proportion of Total ')\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 28, 'page_label': '11'}, page_content=\"Importing, Summarizing, and Visualizing Data 11\\nHere 9 bins were used. Rather than using raw counts (the default), the vertical axis\\nhere gives the percentage in each class, defined by count\\ntotal . This is achieved by choosing the\\n“weights” parameter to be equal to the vector with entries 1/266, with length 226. Various\\nplotting parameters have also been changed.\\n65\\n 70\\n 75\\n 80\\n 85\\n 90\\nage\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20Proportion of Total\\nFigure 1.3: Histogram of 'age'.\\nHistograms can also be used for discrete features, although it may be necessary to\\nexplicitly specify the bins and placement of the ticks on the axes.\\n1.5.2.3 Empirical Cumulative Distribution Function\\nThe empirical cumulative distribution function , denoted by Fn, is a step function which empirical\\ncumulative\\ndistribution\\nfunction\\njumps an amount k/n at observation values, where k is the number of tied observations\\nat that value. For observations x1,..., xn, Fn(x) is the fraction of observations less than or\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 28, 'page_label': '11'}, page_content=\"at that value. For observations x1,..., xn, Fn(x) is the fraction of observations less than or\\nequal to x, i.e.,\\nFn(x) = number of xi ⩽x\\nn = 1\\nn\\nnX\\ni=1\\n1{xi ⩽x}, (1.2)\\nwhere 1 denotes the indicator indicatorfunction; that is, 1{xi ⩽x}is equal to 1 when xi ⩽x and 0\\notherwise. To produce a plot of the empirical cumulative distribution function we can use\\nthe plt.step function. The result for the age data is shown in Figure 1.4. The empirical\\ncumulative distribution function for a discrete quantitative variable is obtained in the same\\nway.\\nx = np.sort(nutri.age)\\ny = np.linspace(0,1, len (nutri.age))\\nplt.xlabel( 'age')\\nplt.ylabel( 'Fn(x) ')\\nplt.step(x,y)\\nplt.xlim(x. min (),x. max ())\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 29, 'page_label': '12'}, page_content=\"12 Visualizing Data\\n65\\n 70\\n 75\\n 80\\n 85\\n 90\\nage\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Fn(x)\\nFigure 1.4: Plot of the empirical distribution function for the continuous quantitative fea-\\nture 'age'.\\n1.5.3 Data Visualization in a Bivariate Setting\\nIn this section, we present a few useful visual aids to explore relationships between two\\nfeatures. The graphical representation will depend on the type of the two features.\\n1.5.3.1 Two-way Plots for Two Categorical Variables\\nComparing barplots for two categorical variables involves introducing subplots to the fig-\\nure. Figure 1.5 visualizes the contingency table of Section 1.3, which cross-tabulates the\\nfamily status (situation) with the gender of the elderly people. It simply shows two barplots\\nnext to each other in the same figure.\\nCouple\\n Family\\n Single\\n0\\n20\\n40\\n60\\n80Counts\\nMale\\nFemale\\nFigure 1.5: Barplot for two categorical variables.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 30, 'page_label': '13'}, page_content=\"Importing, Summarizing, and Visualizing Data 13\\nThe figure was made using the seaborn package, which was specifically designed to\\nsimplify statistical visualization tasks.\\nimport seaborn as sns\\nsns.countplot(x= 'situation ', hue = 'gender ', data=nutri,\\nhue_order = [ 'Male ', 'Female '], palette = [ 'SkyBlue ','Pink '],\\nsaturation = 1, edgecolor= 'black ')\\nplt.legend(loc= 'upper center ')\\nplt.xlabel( '' )\\nplt.ylabel( 'Counts ')\\nplt.show()\\n1.5.3.2 Plots for Two Quantitative Variables\\nWe can visualize patterns between two quantitative features using ascatterplot scatterplot. This can be\\ndone with plt.scatter. The following code produces a scatterplot of 'weight' against\\n'height' for the nutri data.\\nplt.scatter(nutri.height, nutri.weight, s=12, marker= 'o')\\nplt.xlabel( 'height ')\\nplt.ylabel( 'weight ')\\nplt.show()\\n140\\n 150\\n 160\\n 170\\n 180\\n 190\\nheight\\n40\\n50\\n60\\n70\\n80\\n90weight\\nFigure 1.6: Scatterplot of 'weight' against 'height'.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 30, 'page_label': '13'}, page_content=\"170\\n 180\\n 190\\nheight\\n40\\n50\\n60\\n70\\n80\\n90weight\\nFigure 1.6: Scatterplot of 'weight' against 'height'.\\nThe next Python code illustrates that it is possible to produce highly sophisticated scat-\\nter plots, such as in Figure 1.7. The figure shows the birth weights (mass) of babies whose\\nmothers smoked (blue triangles) or not (red circles). In addition, straight lines were fitted to\\nthe two groups, suggesting that birth weight decreases with age when the mother smokes,\\nbut increases when the mother does not smoke! The question is whether these trends are\\nstatistically significant or due to chance. We will revisit this data set later on in the book. ☞ 200\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 31, 'page_label': '14'}, page_content=\"14 Visualizing Data\\nurlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/ '\\ndataname = 'MASS/birthwt.csv '\\nbwt = pd.read_csv(urlprefix + dataname)\\nbwt = bwt.drop( 'Unnamed: 0 ',1) #drop unnamed column\\nstyles = {0: [ 'o','red'], 1: [ '^','blue ']}\\nfor k in styles:\\ngrp = bwt[bwt.smoke==k]\\nm,b = np.polyfit(grp.age, grp.bwt, 1) # fit a straight line\\nplt.scatter(grp.age, grp.bwt, c=styles[k][1], s=15, linewidth=0,\\nmarker = styles[k][0])\\nplt.plot(grp.age, m*grp.age + b, '-', color=styles[k][1])\\nplt.xlabel( 'age')\\nplt.ylabel( 'birth weight (g) ')\\nplt.legend([ 'non-smokers ','smokers '],prop={ 'size ':8},\\nloc=(0.5,0.8))\\nplt.show()\\n10\\n 15\\n 20\\n 25\\n 30\\n 35\\n 40\\n 45\\n 50\\nage\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000birth weight (g)\\nnon-smokers\\nsmokers\\nFigure 1.7: Birth weight against age for smoking and non-smoking mothers.\\n1.5.3.3 Plots for One Qualitative and One Quantitative Variable\\nIn this setting, it is interesting to draw boxplots of the quantitative feature for each level\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 31, 'page_label': '14'}, page_content=\"In this setting, it is interesting to draw boxplots of the quantitative feature for each level\\nof the categorical feature. Assuming the variables are structured correctly, the function\\nplt.boxplot can be used to produce Figure 1.8, using the following code:\\nmales = nutri[nutri.gender == 'Male ']\\nfemales = nutri[nutri.gender == 'Female ']\\nplt.boxplot([males.coffee,females.coffee],notch=True,widths\\n=(0.5,0.5))\\nplt.xlabel( 'gender ')\\nplt.ylabel( 'coffee ')\\nplt.xticks([1,2],[ 'Male ','Female '])\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 32, 'page_label': '15'}, page_content=\"Importing, Summarizing, and Visualizing Data 15\\nMale\\n Female\\ngender\\n0\\n1\\n2\\n3\\n4\\n5coffee\\nFigure 1.8: Boxplots of a quantitative feature 'coffee' as a function of the levels of a\\ncategorical feature 'gender'. Note that we used a different, “notched”, style boxplot this\\ntime.\\nFurther Reading\\nThe focus in this book is on the mathematical and statistical analysis of data, and for the\\nrest of the book we assume that the data is available in a suitable form for analysis. How-\\never, a large part of practical data science involves the cleaning of data; that is, putting\\nit into a form that is amenable to analysis with standard software packages. Standard Py-\\nthon modules such as numpy and pandas can be used to reformat rows, rename columns,\\nremove faulty outliers, merge rows, and so on. McKinney, the creator of pandas, gives\\nmany practical case studies in [84]. Effective data visualization techniques are beautifully\\nillustrated in [65].\\nExercises\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 32, 'page_label': '15'}, page_content=\"illustrated in [65].\\nExercises\\nBefore you attempt these exercises, make sure you have up-to-date versions of the relevant\\nPython packages, specifically matplotlib, pandas, and seaborn. An easy way to ensure\\nthis is to update packages via the Anaconda Navigator, as explained in Appendix D.\\n1. Visit the UCI Repository https://archive.ics.uci.edu/. Read the description of\\nthe data and download the Mushroom data setagaricus-lepiota.data. Using pandas,\\nread the data into a DataFramecalled mushroom, via read_csv.\\n(a) How many features are in this data set?\\n(b) What are the initial names and types of the features?\\n(c) Rename the first feature (index 0) to 'edibility' and the sixth feature (index 5) to\\n'odor' [Hint: the column names in pandas are immutable; so individual columns\\ncannot be modified directly. However it is possible to assign the entire column names\\nlist via mushroom.columns = newcols. ]\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 33, 'page_label': '16'}, page_content=\"16 Exercises\\n(d) The 6th column lists the various odors of the mushrooms: encoded as 'a', 'c', . . . .\\nReplace these with the names 'almond', 'creosote', etc. (categories correspond-\\ning to each letter can be found on the website). Also replace the 'edibility' cat-\\negories 'e' and 'p' with 'edible' and 'poisonous'.\\n(e) Make a contingency table cross-tabulating 'edibility' and 'odor'.\\n(f) Which mushroom odors should be avoided, when gathering mushrooms for consump-\\ntion?\\n(g) What proportion of odorless mushroom samples were safe to eat?\\n2. Change the type and value of variables in the nutri data set according to Table 1.2 and\\nsave the data as a CSV file. The modified data should have eight categorical features, three\\nfloats, and two integer features.\\n3. It frequently happens that a table with data needs to be restructured before the data can\\nbe analyzed using standard statistical software. As an example, consider the test scores in\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 33, 'page_label': '16'}, page_content=\"be analyzed using standard statistical software. As an example, consider the test scores in\\nTable 1.3 of 5 students before and after specialized tuition.\\nTable 1.3: Student scores.\\nStudent Before After\\n1 75 85\\n2 30 50\\n3 100 100\\n4 50 52\\n5 60 65\\nThis is not in the standard format described in Section 1.1. In particular, the student scores\\nare divided over two columns, whereas the standard format requires that they are collected\\nin one column, e.g., labelled 'Score'. Reformat (by hand) the table in standard format,\\nusing three features:\\n• 'Score', taking continuous values,\\n• 'Time', taking values 'Before' and 'After',\\n• 'Student', taking values from 1 to 5.\\nUseful methods for reshaping tables in pandas are melt, stack, and unstack.\\n4. Create a similar barplot as in Figure 1.5, but now plot the corresponding proportions of\\nmales and females in each of the three situation categories. That is, the heights of the bars\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 33, 'page_label': '16'}, page_content=\"males and females in each of the three situation categories. That is, the heights of the bars\\nshould sum up to 1 for both barplots with the same ’gender’value. [Hint: seaborn does\\nnot have this functionality built in, instead you need to first create a contingency table and\\nuse matplotlib.pyplot to produce the figure.]\\n5. The iris data set, mentioned in Section 1.1, contains various features, including☞2\\n'Petal.Length' and 'Sepal.Length', of three species of iris: setosa, versicolor, and\\nvirginica.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 34, 'page_label': '17'}, page_content=\"Importing, Summarizing, and Visualizing Data 17\\n(a) Load the data set into a pandas DataFrameobject.\\n(b) Using matplotlib.pyplot, produce boxplots of 'Petal.Length' for each the\\nthree species, in one figure.\\n(c) Make a histogram with 20 bins for 'Petal.Length'.\\n(d) Produce a similar scatterplot for 'Sepal.Length' against 'Petal.Length' to that\\nof the left plot in Figure 1.9. Note that the points should be colored according to the\\n’Species’feature as per the legend in the right plot of the figure.\\n(e) Using the kdeplot method of the seaborn package, reproduce the right plot of\\nFigure 1.9, where kernel density plots for 'Petal.Length' are given. ☞ 131\\n1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\nPetal.Length\\n5\\n6\\n7\\n8Sepal.Length\\n2\\n 4\\n 6\\n 8\\nPetal.Length\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5Density\\nsetosa\\nversicolor\\nvirginica\\nFigure 1.9: Left: scatterplot of 'Sepal.Length' against 'Petal.Length'. Right: kernel\\ndensity estimates of 'Petal.Length' for the three species of iris.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 34, 'page_label': '17'}, page_content=\"density estimates of 'Petal.Length' for the three species of iris.\\n6. Import the data set EuStockMarkets from the same website as theiris data set above.\\nThe data set contains the daily closing prices of four European stock indices during the\\n1990s, for 260 working days per year.\\n(a) Create a vector of times (working days) for the stock prices, between 1991.496 and\\n1998.646 with increments of 1/260.\\n(b) Reproduce Figure 1.10. [Hint: Use a dictionary to map column names (stock indices)\\nto colors.]\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 35, 'page_label': '18'}, page_content='18 Exercises\\n1991\\n 1992\\n 1993\\n 1994\\n 1995\\n 1996\\n 1997\\n 1998\\n 1999\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000\\n7000\\n8000\\n9000\\nDAX\\nSMI\\nCAC\\nFTSE\\nFigure 1.10: Closing stock indices for various European stock markets.\\n7. Consider the KASANDRdata set from the UCI Machine Learning Repository, which can\\nbe downloaded from\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/00385/de\\n.tar.bz2.\\nThis archive file has a size of 900Mb, so it may take a while to download. Uncompressing\\nthe file (e.g., via 7-Zip) yields a directorydecontaining two large CSV files:test_de.csv\\nand train_de.csv, with sizes 372Mb and 3Gb, respectively. Such large data files can still\\nbe processed e fficiently in pandas, provided there is enough memory. The files contain\\nrecords of user information from Kelkoo web logs in Germany as well as meta-data on\\nusers, offers, and merchants. The data sets have 7 attributes and 1919561 and 15844717\\nrows, respectively. The data sets are anonymized via hex strings.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 35, 'page_label': '18'}, page_content=\"rows, respectively. The data sets are anonymized via hex strings.\\n(a) Load train_de.csvinto a pandas DataFrameobject de, using\\nread_csv('train_de.csv',␣delimiter␣=␣'\\\\t').\\nIf not enough memory is available, load test_de.csv instead. Note that entries are\\nseparated here by tabs, not commas. Time how long it takes for the file to load, using\\nthe time package. (It took 38 seconds for train_de.csv to load on one of our\\ncomputers.)\\n(b) How many unique users and merchants are in this data set?\\n8. Visualizing data involving more than two features requires careful design, which is often\\nmore of an art than a science.\\n(a) Go to Vincent Arel-Bundocks’s website (URL given in Section 1.1) and read the\\nOrangedata set into a pandas DataFrame objectcalled orange. Remove its first\\n(unnamed) column.\\n(b) The data set contains the circumferences of 5 orange trees at various stages in their\\ndevelopment. Find the names of the features.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 35, 'page_label': '18'}, page_content='development. Find the names of the features.\\n(c) In Python, import seaborn and visualize the growth curves (circumference against\\nage) of the trees, using the regplot and FacetGrid methods.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 36, 'page_label': '19'}, page_content='CHAPTER 2\\nSTATISTICAL LEARNING\\nThe purpose of this chapter is to introduce the reader to some common concepts\\nand themes in statistical learning. We discuss the di fference between supervised and\\nunsupervised learning, and how we can assess the predictive performance of supervised\\nlearning. We also examine the central role that the linear and Gaussian properties play\\nin the modeling of data. We conclude with a section on Bayesian learning. The required\\nprobability and statistics background is given in Appendix C.\\n2.1 Introduction\\nAlthough structuring and visualizing data are important aspects of data science, the main\\nchallenge lies in the mathematical analysis of the data. When the goal is to interpret the\\nmodel and quantify the uncertainty in the data, this analysis is usually referred to as stat-\\nistical learning. In contrast, when the emphasis is on making predictions using large-scale statistical\\nlearningdata, then it is common to speak about machine learning or data mining.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 36, 'page_label': '19'}, page_content='learningdata, then it is common to speak about machine learning or data mining.\\nmachine\\nlearning\\ndata mining\\nThere are two major goals for modeling data: 1) to accurately predict some future\\nquantity of interest, given some observed data, and 2) to discover unusual or interesting\\npatterns in the data. To achieve these goals, one must rely on knowledge from three im-\\nportant pillars of the mathematical sciences.\\nFunction approximation. Building a mathematical model for data usually means under-\\nstanding how one data variable depends on another data variable. The most natural\\nway to represent the relationship between variables is via a mathematical function or\\nmap. We usually assume that this mathematical function is not completely known,\\nbut can be approximated well given enough computing power and data. Thus, data\\nscientists have to understand how best to approximate and represent functions using\\nthe least amount of computer processing and memory.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 36, 'page_label': '19'}, page_content='the least amount of computer processing and memory.\\nOptimization. Given a class of mathematical models, we wish to find the best possible\\nmodel in that class. This requires some kind of e fficient search or optimization pro-\\ncedure. The optimization step can be viewed as a process of fitting or calibrating\\na function to observed data. This step usually requires knowledge of optimization\\nalgorithms and efficient computer coding or programming.\\n19'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 37, 'page_label': '20'}, page_content='20 Supervised and Unsupervised Learning\\nProbability and Statistics. In general, the data used to fit the model is viewed as a realiz-\\nation of a random process or numerical vector, whose probability law determines the\\naccuracy with which we can predict future observations. Thus, in order to quantify\\nthe uncertainty inherent in making predictions about the future, and the sources of er-\\nror in the model, data scientists need a firm grasp of probability theory and statistical\\ninference.\\n2.2 Supervised and Unsupervised Learning\\nGiven an input or featurefeature vector x, one of the main goals of machine learning is to predict\\nan output or responseresponse variable y. For example, x could be a digitized signature and y a\\nbinary variable that indicates whether the signature is genuine or false. Another example is\\nwhere x represents the weight and smoking habits of an expecting mother and y the birth\\nweight of the baby. The data science attempt at this prediction is encoded in a mathematical'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 37, 'page_label': '20'}, page_content='weight of the baby. The data science attempt at this prediction is encoded in a mathematical\\nfunction g, called theprediction functionprediction\\nfunction\\n, which takes as an inputx and outputs aguess g(x)\\nfor y (denoted by by, for example). In a sense, g encompasses all the information about the\\nrelationship between the variables x and y, excluding the effects of chance and randomness\\nin nature.\\nIn regression problems, the response variable y can take any real value. In contrast,regression\\nwhen y can only lie in a finite set, say y ∈{0,..., c −1}, then predicting y is conceptually\\nthe same as classifying the input x into one of c categories, and so prediction becomes a\\nclassificationclassification problem.\\nWe can measure the accuracy of a prediction by with respect to a given response y by\\nusing some loss functionloss function Loss(y,by). In a regression setting the usual choice is the squared-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 37, 'page_label': '20'}, page_content='error loss (y−by)2. In the case of classification, the zero–one (also written 0–1) loss function\\nLoss(y,by) = 1{y , by}is often used, which incurs a loss of 1 whenever the predicted class\\nby is not equal to the class y. Later on in this book, we will encounter various other useful\\nloss functions, such as the cross-entropy and hinge loss functions (see, e.g., Chapter 7).\\nThe word error is often used as a measure of distance between a “true” object y and\\nsome approximation by thereof. If y is real-valued, the absolute error |y −by|and the\\nsquared error (y−by)2 are both well-established error concepts, as are the norm∥y−by∥\\nand squared norm ∥y −by∥2 for vectors. The squared error (y −by)2 is just one example\\nof a loss function.\\nIt is unlikely that any mathematical functiong will be able to make accurate predictions\\nfor all possible pairs ( x,y) one may encounter in Nature. One reason for this is that, even'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 37, 'page_label': '20'}, page_content='for all possible pairs ( x,y) one may encounter in Nature. One reason for this is that, even\\nwith the same input x, the output y may be different, depending on chance circumstances\\nor randomness. For this reason, we adopt a probabilistic approach and assume that each\\npair (x,y) is the outcome of a random pair ( X,Y) that has some joint probability density\\nf (x,y). We then assess the predictive performance via the expected loss, usually called the\\nriskrisk , for g:\\nℓ(g) = ELoss(Y,g(X)). (2.1)\\nFor example, in the classification case with zero–one loss function the risk is equal to the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 38, 'page_label': '21'}, page_content='Statistical Learning 21\\nprobability of incorrect classification: ℓ(g) = P[Y , g(X)]. In this context, the prediction\\nfunction g is called a classifier classifier. Given the distribution of (X,Y) and any loss function, we\\ncan in principle find the best possibleg∗:= argming ELoss(Y,g(X)) that yields the smallest\\nrisk ℓ∗:= ℓ(g∗). We will see in Chapter 7 that in the classification case withy ∈{0,..., c−1} ☞ 251\\nand ℓ(g) = P[Y , g(X)], we have\\ng∗(x) = argmax\\ny∈{0,...,c−1}\\nf (y |x),\\nwhere f (y |x) = P[Y = y |X = x] is the conditional probability of Y = y given X = x.\\nAs already mentioned, for regression the most widely-used loss function is the squared-\\nerror loss. In this setting, the optimal prediction function g∗ is often called the regression\\nfunction. The following theorem specifies its exact form. regression\\nfunction\\nTheorem 2.1: Optimal Prediction Function for Squared-Error Loss\\nFor the squared-error loss Loss(y,by) = (y −by)2, the optimal prediction function g∗is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 38, 'page_label': '21'}, page_content='For the squared-error loss Loss(y,by) = (y −by)2, the optimal prediction function g∗is\\nequal to the conditional expectation of Y given X = x:\\ng∗(x) = E[Y |X = x].\\nProof: Let g∗(x) = E[Y |X = x]. For any function g, the squared-error risk satisfies\\nE(Y −g(X))2 = E[(Y −g∗(X) + g∗(X) −g(X))2]\\n= E(Y −g∗(X))2 + 2E[(Y −g∗(X))(g∗(X) −g(X))] + E(g∗(X) −g(X))2\\n⩾E(Y −g∗(X))2 + 2E[(Y −g∗(X))(g∗(X) −g(X))]\\n= E(Y −g∗(X))2 + 2E{(g∗(X) −g(X))E[Y −g∗(X) |X]}.\\nIn the last equation we used the tower property. By the definition of the conditional expect- ☞ 431\\nation, we have E[Y −g∗(X) |X] = 0. It follows that E(Y −g(X))2 ⩾E(Y −g∗(X))2, showing\\nthat g∗yields the smallest squared-error risk. □\\nOne consequence of Theorem 2.1 is that, conditional on X = x, the (random) response\\nY can be written as\\nY = g∗(x) + ε(x), (2.2)\\nwhere ε(x) can be viewed as the random deviation of the response from its conditional\\nmean at x. This random deviation satisfies Eε(x) = 0. Further, the conditional variance of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 38, 'page_label': '21'}, page_content='mean at x. This random deviation satisfies Eε(x) = 0. Further, the conditional variance of\\nthe response Y at x can be written as Var ε(x) = v2(x) for some unknown positive function\\nv. Note that, in general, the probability distribution of ε(x) is unspecified.\\nSince, the optimal prediction functiong∗depends on the typically unknown joint distri-\\nbution of (X,Y), it is not available in practice. Instead, all that we have available is a finite\\nnumber of (usually) independent realizations from the joint density f (x,y). We denote this\\nsample by T = {(X1,Y1),..., (Xn,Yn)}and call it the training set training set(Tis a mnemonic for\\ntraining) with n examples. It will be important to distinguish between a random training\\nset Tand its (deterministic) outcome {(x1,y1),..., (xn,yn)}. We will use the notation τfor\\nthe latter. We will also add the subscriptn in τn when we wish to emphasize the size of the\\ntraining set.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 39, 'page_label': '22'}, page_content='22 Supervised and Unsupervised Learning\\nOur goal is thus to “learn” the unknown g∗using the n examples in the training set T.\\nLet us denote bygT the best (by some criterion) approximation forg∗that we can construct\\nfrom T. Note that gT is a random function. A particular outcome is denoted by gτ. It is\\noften useful to think of a teacher–learner metaphor, whereby the function gT is a learnerlearner\\nwho learns the unknown functional relationship g∗: x 7→y from the training data T. We\\ncan imagine a “teacher” who provides n examples of the true relationship between the\\noutput Yi and the input Xi for i = 1,..., n, and thus “trains” the learner gT to predict the\\noutput of a new input X, for which the correct output Y is not provided by the teacher (is\\nunknown).\\nThe above setting is calledsupervised learningsupervised\\nlearning\\n, because one tries to learn the functional\\nrelationship between the feature vector x and response y in the presence of a teacher who'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 39, 'page_label': '22'}, page_content='relationship between the feature vector x and response y in the presence of a teacher who\\nprovides n examples. It is common to speak of “explaining” or predicting y on the basis of\\nx, where x is a vector of explanatory variablesexplanatory\\nvariables\\n.\\nAn example of supervised learning is email spam detection. The goal is to train the\\nlearner gT to accurately predict whether any future email, as represented by the feature\\nvector x, is spam or not. The training data consists of the feature vectors of a number\\nof different email examples as well as the corresponding labels (spam or not spam). For\\ninstance, a feature vector could consist of the number of times sales-pitch words like “free”,\\n“sale”, or “miss out” occur within a given email.\\nAs seen from the above discussion, most questions of interest in supervised learning\\ncan be answered if we know the conditional pdf f (y |x), because we can then in principle\\nwork out the function value g∗(x).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 39, 'page_label': '22'}, page_content='work out the function value g∗(x).\\nIn contrast, unsupervised learningunsupervised\\nlearning\\nmakes no distinction between response and explan-\\natory variables, and the objective is simply to learn the structure of the unknown distribu-\\ntion of the data. In other words, we need to learn f (x). In this case the guess g(x) is an\\napproximation of f (x) and the risk is of the form\\nℓ(g) = ELoss( f (X),g(X)).\\nAn example of unsupervised learning is when we wish to analyze the purchasing be-\\nhaviors of the customers of a grocery shop that has a total of, say, a hundred items on sale.\\nA feature vector here could be a binary vector x ∈{0,1}100 representing the items bought\\nby a customer on a visit to the shop (a 1 in the k-th position if a customer bought item\\nk ∈{1,..., 100}and a 0 otherwise). Based on a training set τ = {x1,..., xn}, we wish to\\nfind any interesting or unusual purchasing patterns. In general, it is di fficult to know if an'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 39, 'page_label': '22'}, page_content='find any interesting or unusual purchasing patterns. In general, it is di fficult to know if an\\nunsupervised learner is doing a good job, because there is no teacher to provide examples\\nof accurate predictions.\\nThe main methodologies for unsupervised learning include clustering, principal com-\\nponent analysis, and kernel density estimation, which will be discussed in Chapter 4.☞121\\nIn the next three sections we will focus on supervised learning. The main super-\\nvised learning methodologies are regression and classification, to be discussed in detail in\\nChapters 5 and 7. More advanced supervised learning techniques, including reproducing☞167\\n☞251 kernel Hilbert spaces, tree methods, and deep learning, will be discussed in Chapters 6, 8,\\nand 9.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 40, 'page_label': '23'}, page_content='Statistical Learning 23\\n2.3 Training and Test Loss\\nGiven an arbitrary prediction function g, it is typically not possible to compute its riskℓ(g)\\nin (2.1). However, using the training sample T, we can approximate ℓ(g) via the empirical\\n(sample average) risk\\nℓT(g) = 1\\nn\\nnX\\ni=1\\nLoss(Yi,g(Xi)), (2.3)\\nwhich we call the training loss training loss. The training loss is thus an unbiased estimator of the risk\\n(the expected loss) for a prediction function g, based on the training data.\\nTo approximate the optimal prediction function g∗ (the minimizer of the risk ℓ(g)) we\\nfirst select a suitable collection of approximating functions Gand then take our learner to\\nbe the function in Gthat minimizes the training loss; that is,\\ngG\\nT = argmin\\ng∈G\\nℓT(g). (2.4)\\nFor example, the simplest and most useful Gis the set of linear functions of x; that is, the\\nset of all functions g : x 7→β⊤x for some real-valued vector β.\\nWe suppress the superscript Gwhen it is clear which function class is used. Note that'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 40, 'page_label': '23'}, page_content='We suppress the superscript Gwhen it is clear which function class is used. Note that\\nminimizing the training loss over all possible functions g (rather than over all g ∈G) does\\nnot lead to a meaningful optimization problem, as any function g for which g(Xi) = Yi for\\nall i gives minimal training loss. In particular, for a squared-error loss, the training loss will\\nbe 0. Unfortunately, such functions have a poor ability to predict new (that is, independent\\nfrom T) pairs of data. This poor generalization performance is called overfitting overfitting.\\nBy choosing g a function that predicts the training data exactly (and is, for example,\\n0 otherwise), the squared-error training loss is zero. Minimizing the training loss is\\nnot the ultimate goal!\\nThe prediction accuracy of new pairs of data is measured by the generalization risk generalization\\nrisk\\nof\\nthe learner. For a fixed training set τit is defined as\\nℓ(gG\\nτ) = ELoss(Y,gG\\nτ(X)), (2.5)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 40, 'page_label': '23'}, page_content='risk\\nof\\nthe learner. For a fixed training set τit is defined as\\nℓ(gG\\nτ) = ELoss(Y,gG\\nτ(X)), (2.5)\\nwhere (X,Y) is distributed according to f (x,y). In the discrete case the generalization risk\\nis therefore: ℓ(gG\\nτ) = P\\nx,y Loss(y,gG\\nτ(x)) f (x,y) (replace the sum with an integral for the\\ncontinuous case). The situation is illustrated in Figure 2.1, where the distribution of (X,Y)\\nis indicated by the red dots. The training set (points in the shaded regions) determines a\\nfixed prediction function shown as a straight line. Three possible outcomes of ( X,Y) are\\nshown (black dots). The amount of loss for each point is shown as the length of the dashed\\nlines. The generalization risk is the average loss over all possible pairs (x,y), weighted by\\nthe corresponding f (x,y).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 41, 'page_label': '24'}, page_content='24 Training and Test Loss\\nxx x\\ny\\ny\\ny\\nFigure 2.1: The generalization risk for a fixed training set is the weighted-average loss over\\nall possible pairs (x,y).\\nFor a random training set T, the generalization risk is thus a random variable that\\ndepends on T(and G). If we average the generalization risk over all possible instances of\\nT, we obtain the expected generalization riskexpected\\ngeneralization\\nrisk\\n:\\nEℓ(gG\\nT) = ELoss(Y,gG\\nT(X)), (2.6)\\nwhere (X,Y) in the expectation above is independent of T. In the discrete case, we have\\nEℓ(gG\\nT) = P\\nx,y,x1,y1,...,xn,yn Loss(y,gG\\nτ(x)) f (x,y) f (x1,y1) ··· f (xn,yn). Figure 2.2 gives an il-\\nlustration.\\ny\\nx\\ny\\ny\\nxx\\nFigure 2.2: The expected generalization risk is the weighted-average loss over all possible\\npairs (x,y) and over all training sets.\\nFor any outcome τof the training data, we can estimate the generalization risk without\\nbias by taking the sample average\\nℓT′(gG\\nτ) := 1\\nn′\\nn′\\nX\\ni=1\\nLoss(Y′\\ni ,gG\\nτ(X′\\ni )), (2.7)\\nwhere {(X′\\n1,Y′\\n1),..., (X′'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 41, 'page_label': '24'}, page_content='ℓT′(gG\\nτ) := 1\\nn′\\nn′\\nX\\ni=1\\nLoss(Y′\\ni ,gG\\nτ(X′\\ni )), (2.7)\\nwhere {(X′\\n1,Y′\\n1),..., (X′\\nn′,Y′\\nn′)}=: T′ is a so-called test sampletest sample . The test sample is com-\\npletely separate from T, but is drawn in the same way asT; that is, via independent draws\\nfrom f (x,y), for some sample size n′. We call the estimator (2.7) the test losstest loss . For a ran-\\ndom training set Twe can define ℓT′(gG\\nT) similarly. It is then crucial to assume that Tis\\nindependent of T′. Table 2.1 summarizes the main definitions and notation for supervised\\nlearning.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 42, 'page_label': '25'}, page_content='Statistical Learning 25\\nTable 2.1: Summary of definitions for supervised learning.\\nx Fixed explanatory (feature) vector.\\nX Random explanatory (feature) vector.\\ny Fixed (real-valued) response.\\nY Random response.\\nf (x,y) Joint pdf of X and Y, evaluated at (x,y).\\nf (y |x) Conditional pdf of Y given X = x, evaluated at y.\\nτor τn Fixed training data {(xi,yi),i = 1,..., n}.\\nTor Tn Random training data {(Xi,Yi),i = 1,..., n}.\\nX Matrix of explanatory variables, with n rows x⊤\\ni ,i = 1,..., n\\nand dim( x) feature columns; one of the features may be the\\nconstant 1.\\ny Vector of response variables (y1,..., yn)⊤.\\ng Prediction (guess) function.\\nLoss(y,by) Loss incurred when predicting response y with by.\\nℓ(g) Risk for prediction function g; that is, ELoss(Y,g(X)).\\ng∗ Optimal prediction function; that is, argming ℓ(g).\\ngG Optimal prediction function in function class G; that is,\\nargming∈Gℓ(g).\\nℓτ(g) Training loss for prediction function g; that is, the sample av-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 42, 'page_label': '25'}, page_content='argming∈Gℓ(g).\\nℓτ(g) Training loss for prediction function g; that is, the sample av-\\nerage estimate of ℓ(g) based on a fixed training sample τ.\\nℓT(g) The same as ℓτ(g), but now for a random training sample T.\\ngG\\nτ or gτ The learner: argmin g∈Gℓτ(g). That is, the optimal prediction\\nfunction based on a fixed training set τ and function class G.\\nWe suppress the superscript Gif the function class is implicit.\\ngG\\nT or gT The learner, where we have replaced τwith a random training\\nset T.\\nTo compare the predictive performance of various learners in the function class G, as\\nmeasured by the test loss, we can use the same fixed training set τ and test set τ′ for all\\nlearners. When there is an abundance of data, the “overall” data set is usually (randomly)\\ndivided into a training and test set, as depicted in Figure 2.3. We then use the training data\\nto construct various learners gG1\\nτ ,gG2\\nτ ,... , and use the test data to select the best (with the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 42, 'page_label': '25'}, page_content='τ ,gG2\\nτ ,... , and use the test data to select the best (with the\\nsmallest test loss) among these learners. In this context the test set is called the validation\\nset validation set. Once the best learner has been chosen, a third “test” set can be used to assess the\\npredictive performance of the best learner. The training, validation, and test sets can again\\nbe obtained from the overall data set via a random allocation. When the overall data set\\nis of modest size, it is customary to perform the validation phase (model selection) on the\\ntraining set only, using cross-validation. This is the topic of Section 2.5.2. ☞ 38'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 43, 'page_label': '26'}, page_content='26 Training and Test Loss\\n\\x01\\x02\\x03\\x04\\x05\\x04\\x05\\x06\\n\\x01\\x07\\x08'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 43, 'page_label': '26'}, page_content='\\x03\\x05\\x03\\t\\x0e\\x02\\x0f\\x10\\x07\\x08\\x0c\\x0e\\x05\\x08\\x07 \\n\\x0b\\x0c\\r\\x03\\x05\\x03\\t\\x0e\\x02\\x0f\\x10\\x07\\x08\\x0c\\x0e\\x05\\x08\\x07\\n\\x01\\x02\\x03\\x04\\x05\\x04\\x05\\x06\\n\\x11\\x03\\r\\x04\\x12\\x03\\t\\x04\\x0e\\x05\\n\\x01\\x07\\x08\\t\\nFigure 2.3: Statistical learning algorithms often require the data to be divided into training\\nand test data. If the latter is used for model selection, a third set is needed for testing the\\nperformance of the selected model.\\nWe next consider a concrete example that illustrates the concepts introduced so far.\\nExample 2.1 (Polynomial Regression) In what follows, it will appear that we have ar-\\nbitrarily replaced the symbols x,g,Gwith u,h,H, respectively. The reason for this switch\\nof notation will become clear at the end of the example.\\nThe data (depicted as dots) in Figure 2.4 are n = 100 points (ui,yi),i = 1,..., n drawn\\nfrom iid random points ( Ui,Yi),i = 1,..., n, where the {Ui}are uniformly distributed on\\nthe interval (0,1) and, given Ui = ui, the random variable Yi has a normal distribution with\\nexpectation 10 −140ui + 400u2\\ni −250u3\\ni and variance ℓ∗ = 25. This is an example of a'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 43, 'page_label': '26'}, page_content='expectation 10 −140ui + 400u2\\ni −250u3\\ni and variance ℓ∗ = 25. This is an example of a\\npolynomial regression modelpolynomial\\nregression\\nmodel\\n. Using a squared-error loss, the optimal prediction function\\nh∗(u) = E[Y |U = u] is thus\\nh∗(u) = 10 −140u + 400u2 −250u3,\\nwhich is depicted by the dashed curve in Figure 2.4.\\n0.0\\n 0.2\\n 0.4\\n 0.6\\n 0.8\\n 1.0\\nu\\n10\\n0\\n10\\n20\\n30\\n40\\nh * (u)\\ndata points\\ntrue\\nFigure 2.4: Training data and the optimal polynomial prediction function h∗.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 44, 'page_label': '27'}, page_content='Statistical Learning 27\\nTo obtain a good estimate of h∗(u) based on the training set τ = {(ui,yi),i = 1,..., n},\\nwe minimize the outcome of the training loss (2.3):\\nℓτ(h) = 1\\nn\\nnX\\ni=1\\n(yi −h(ui))2, (2.8)\\nover a suitable setHof candidate functions. Let us take the setHp of polynomial functions\\nin u of order p −1:\\nh(u) := β1 + β2u + β3u2 + ··· + βpup−1 (2.9)\\nfor p = 1,2,... and parameter vector β= [β1,β2,...,β p]⊤. This function class contains the\\nbest possible h∗(u) = E[Y |U = u] for p ⩾4. Note that optimization overHp is a parametric\\noptimization problem, in that we need to find the best β. Optimization of (2.8) over Hp is\\nnot straightforward, unless we notice that (2.9) is alinear function in β. In particular, if we\\nmap each feature u to a feature vector x = [1,u,u2,..., up−1]⊤, then the right-hand side of\\n(2.9) can be written as the function\\ng(x) = x⊤β,\\nwhich is linear in x (as well as β). The optimal h∗(u) in Hp for p ⩾4 then corresponds'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 44, 'page_label': '27'}, page_content='g(x) = x⊤β,\\nwhich is linear in x (as well as β). The optimal h∗(u) in Hp for p ⩾4 then corresponds\\nto the function g∗(x) = x⊤β∗ in the set Gp of linear functions from Rp to R, where β∗ =\\n[10,−140,400,−250,0,..., 0]⊤. Thus, instead of working with the set Hp of polynomial\\nfunctions we may prefer to work with the set Gp of linear functions. This brings us to a\\nvery important idea in statistical learning:\\nExpand the feature space to obtain a linear prediction function.\\nLet us now reformulate the learning problem in terms of the new explanatory (feature)\\nvariables xi = [1,ui,u2\\ni ,..., up−1\\ni ]⊤, i = 1,..., n. It will be convenient to arrange these\\nfeature vectors into a matrix X with rows x⊤\\n1 ,..., x⊤\\nn :\\nX =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 u1 u2\\n1 ··· up−1\\n1\\n1 u2 u2\\n2 ··· up−1\\n2\\n... ... ... ... ...\\n1 un u2\\nn ··· up−1\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (2.10)\\nCollecting the responses {yi}into a column vector y, the training loss (2.3) can now be\\nwritten compactly as\\n1\\nn ∥y −Xβ∥2. (2.11)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 44, 'page_label': '27'}, page_content='written compactly as\\n1\\nn ∥y −Xβ∥2. (2.11)\\nTo find the optimal learner (2.4) in the class Gp we need to find the minimizer of (2.11):\\nbβ= argmin\\nβ\\n∥y −Xβ∥2, (2.12)\\nwhich is called theordinary least-squares ordinary\\nleast-squares\\nsolution. As is illustrated in Figure 2.5, to findbβ,\\nwe choose Xbβto be equal to the orthogonal projection of y onto the linear space spanned\\nby the columns of the matrix X; that is, Xbβ= Py, where P is the projection matrix projection\\nmatrix\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 45, 'page_label': '28'}, page_content='28 Training and Test Loss\\nSpan(X)\\nXβ\\nXˆβ\\ny\\nFigure 2.5: Xbβ is the orthogonal projection of y onto the linear space spanned by the\\ncolumns of the matrix X.\\nAccording to Theorem A.4, the projection matrix is given by☞362\\nP = X X+, (2.13)\\nwhere the p ×n matrix X+ in (2.13) is the pseudo-inverse of X. If X happens to be of full☞360\\npseudo-inverse column rank (so that none of the columns can be expressed as a linear combination of the\\n☞356 other columns), then X+ = (X⊤X)−1X⊤.\\nIn any case, from Xbβ = Py and PX = X, we can see that bβ satisfies the normal\\nequationsnormal\\nequations\\n:\\nX⊤Xβ= X⊤Py = (PX)⊤y = X⊤y. (2.14)\\nThis is a set of linear equations, which can be solved very fast and whose solution can be\\nwritten explicitly as:\\nbβ= X+y. (2.15)\\nFigure 2.6 shows the trained learners for various values of p:\\nh\\nHp\\nτ (u) = g\\nGp\\nτ (x) = x⊤bβ\\n0.0\\n 0.2\\n 0.4\\n 0.6\\n 0.8\\n 1.0\\nu\\n10\\n0\\n10\\n20\\n30\\n40\\nh p(u)\\ndata points\\ntrue\\np = 2, underfit\\np = 4, correct\\np = 16, overfit'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 45, 'page_label': '28'}, page_content='0.8\\n 1.0\\nu\\n10\\n0\\n10\\n20\\n30\\n40\\nh p(u)\\ndata points\\ntrue\\np = 2, underfit\\np = 4, correct\\np = 16, overfit\\nFigure 2.6: Training data with fitted curves forp = 2,4, and 16. The true cubic polynomial\\ncurve for p = 4 is also plotted (dashed line).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 46, 'page_label': '29'}, page_content='Statistical Learning 29\\nWe see that for p = 16 the fitted curve lies closer to the data points, but is further away\\nfrom the dashed true polynomial curve, indicating that we overfit. The choice p = 4 (the\\ntrue cubic polynomial) is much better than p = 16, or indeed p = 2 (straight line).\\nEach function class Gp gives a different learner g\\nGp\\nτ , p = 1,2,... . To assess which is\\nbetter, we should not simply take the one that gives the smallest training loss. We can\\nalways get a zero training loss by taking p = n, because for any set of n points there exists\\na polynomial of degree n −1 that interpolates all points!\\nInstead, we assess the predictive performance of the learners using the test loss (2.7),\\ncomputed from a test data set. If we collect all n′ test feature vectors in a matrix X′ and\\nthe corresponding test responses in a vector y′, then, similar to (2.11), the test loss can be\\nwritten compactly as\\nℓτ′(g\\nGp\\nτ ) = 1\\nn′∥y′−X′bβ∥2,\\nwhere bβis given by (2.15), using the training data.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 46, 'page_label': '29'}, page_content='ℓτ′(g\\nGp\\nτ ) = 1\\nn′∥y′−X′bβ∥2,\\nwhere bβis given by (2.15), using the training data.\\nFigure 2.7 shows a plot of the test loss against the number of parameters in the vector\\nβ; that is, p. The graph has a characteristic “bath-tub” shape and is at its lowest for p = 4,\\ncorrectly identifying the polynomial order 3 for the true model. Note that the test loss, as\\nan estimate for the generalization risk (2.7), becomes numerically unreliable after p = 16\\n(the graph goes down, where it should go up). The reader may check that the graph for\\nthe training loss exhibits a similar numerical instability for large p, and in fact fails to\\nnumerically decrease to 0 for largep, contrary to what it should do in theory. The numerical\\nproblems arise from the fact that for large p the columns of the (Vandermonde) matrix X\\nare of vastly different magnitudes and so floating point errors quickly become very large.\\nFinally, observe that the lower bound for the test loss is here around 21, which corres-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 46, 'page_label': '29'}, page_content='Finally, observe that the lower bound for the test loss is here around 21, which corres-\\nponds to an estimate of the minimal (squared-error) risk ℓ∗= 25.\\n1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\n 9\\n 10\\n 11\\n 12\\n 13\\n 14\\n 15\\n 16\\n 17\\n 18\\nNumber of parameters p\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160Test loss\\nFigure 2.7: Test loss as function of the number of parameters p of the model.\\nThis script shows how the training data were generated and plotted in Python:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 47, 'page_label': '30'}, page_content=\"30 Training and Test Loss\\npolyreg1.py\\nimport numpy as np\\nfrom numpy.random import rand , randn\\nfrom numpy.linalg import norm , solve\\nimport matplotlib.pyplot as plt\\ndef generate_data(beta , sig, n):\\nu = np.random.rand(n, 1)\\ny = (u ** np.arange(0, 4)) @ beta + sig * np.random.randn(n, 1)\\nreturn u, y\\nnp.random.seed(12)\\nbeta = np.array([[10, -140, 400, -250]]).T\\nn = 100\\nsig = 5\\nu, y = generate_data(beta , sig, n)\\nxx = np.arange(np. min (u), np. max (u)+5e-3, 5e-3)\\nyy = np.polyval(np.flip(beta), xx)\\nplt.plot(u, y, '.', markersize=8)\\nplt.plot(xx, yy, '--',linewidth=3)\\nplt.xlabel(r '$u$')\\nplt.ylabel(r '$h^*(u)$ ')\\nplt.legend([ 'data points ','true '])\\nplt.show()\\nThe following code, which imports the code above, fits polynomial models with p =\\n1,..., K = 18 parameters to the training data and plots a selection of fitted curves, as\\nshown in Figure 2.6.\\npolyreg2.py\\nfrom polyreg1 import *\\nmax_p = 18\\np_range = np.arange(1, max_p + 1, 1)\\nX = np.ones((n, 1))\\nbetahat, trainloss = {}, {}\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 47, 'page_label': '30'}, page_content='max_p = 18\\np_range = np.arange(1, max_p + 1, 1)\\nX = np.ones((n, 1))\\nbetahat, trainloss = {}, {}\\nfor p in p_range: # p is the number of parameters\\nif p > 1:\\nX = np.hstack((X, u**(p-1))) # add column to matrix\\nbetahat[p] = solve(X.T @ X, X.T @ y)\\ntrainloss[p] = (norm(y - X @ betahat[p])**2/n)\\np = [2, 4, 16] # select three curves\\n#replot the points and true line and store in the list \"plots\"\\nplots = [plt.plot(u, y, \\'k.\\', markersize=8)[0],\\nplt.plot(xx, yy, \\'k--\\',linewidth=3)[0]]\\n# add the three curves\\nfor i in p:\\nyy = np.polyval(np.flip(betahat[i]), xx)\\nplots.append(plt.plot(xx, yy)[0])'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 48, 'page_label': '31'}, page_content=\"Statistical Learning 31\\nplt.xlabel(r '$u$')\\nplt.ylabel(r '$h^{\\\\mathcal{H}_p}_{\\\\tau}(u)$ ')\\nplt.legend(plots,( 'data points ', 'true ','$p=2$, underfit ',\\n'$p=4$, correct ','$p=16$, overfit '))\\nplt.savefig( 'polyfitpy.pdf ',format ='pdf')\\nplt.show()\\nThe last code snippet which imports the previous code, generates the test data and plots the\\ngraph of the test loss, as shown in Figure 2.7.\\npolyreg3.py\\nfrom polyreg2 import *\\n# generate test data\\nu_test, y_test = generate_data(beta, sig, n)\\nMSE = []\\nX_test = np.ones((n, 1))\\nfor p in p_range:\\nif p > 1:\\nX_test = np.hstack((X_test, u_test**(p-1)))\\ny_hat = X_test @ betahat[p] # predictions\\nMSE.append(np. sum ((y_test - y_hat)**2/n))\\nplt.plot(p_range, MSE, 'b', p_range, MSE, 'bo')\\nplt.xticks(ticks=p_range)\\nplt.xlabel( 'Number of parameters $p$ ')\\nplt.ylabel( 'Test loss ')\\n2.4 Tradeoffs in Statistical Learning\\nThe art of machine learning in the supervised case is to make the generalization risk (2.5)\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 48, 'page_label': '31'}, page_content='The art of machine learning in the supervised case is to make the generalization risk (2.5)\\nor expected generalization risk (2.6) as small as possible, while using as few computational\\nresources as possible. In pursuing this goal, a suitable class Gof prediction functions has\\nto be chosen. This choice is driven by various factors, such as\\n• the complexity of the class (e.g., is it rich enough to adequately approximate, or even\\ncontain, the optimal prediction function g∗?),\\n• the ease of training the learner via the optimization program (2.4),\\n• how accurately the training loss (2.3) estimates the risk (2.1) within class G,\\n• the feature types (categorical, continuous, etc.).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 49, 'page_label': '32'}, page_content='32 Tradeoffs in Statistical Learning\\nAs a result, the choice of a suitable function class Gusually involves a tradeoff between\\nconflicting factors. For example, a learner from a simple class Gcan be trained very\\nquickly, but may not approximate g∗ very well, whereas a learner from a rich class G\\nthat contains g∗may require a lot of computing resources to train.\\nTo better understand the relation between model complexity, computational simplicity,\\nand estimation accuracy, it is useful to decompose the generalization risk into several parts,\\nso that the tradeoffs between these parts can be studied. We will consider two such decom-\\npositions: the approximation–estimation tradeoff and the bias–variance tradeoff.\\nWe can decompose the generalization risk (2.5) into the following three components:\\nℓ(gG\\nτ) = ℓ∗\\n|{z}\\nirreducible risk\\n+ ℓ(gG) −ℓ∗\\n|     {z     }\\napproximation error\\n+ ℓ(gG\\nτ) −ℓ(gG)|          {z          }\\nstatistical error\\n, (2.16)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 49, 'page_label': '32'}, page_content='approximation error\\n+ ℓ(gG\\nτ) −ℓ(gG)|          {z          }\\nstatistical error\\n, (2.16)\\nwhere ℓ∗:= ℓ(g∗) is the irreducible riskirreducible risk and gG:= argming∈Gℓ(g) is the best learner within\\nclass G. No learner can predict a new response with a smaller risk than ℓ∗.\\nThe second component is the approximation errorapproximation\\nerror\\n; it measures the difference between\\nthe irreducible risk and the best possible risk that can be obtained by selecting the best\\nprediction function in the selected class of functionsG. Determining a suitable class Gand\\nminimizing ℓ(g) over this class is purely a problem of numerical and functional analysis,\\nas the training data τare not present. For a fixedGthat does not contain the optimal g∗, the\\napproximation error cannot be made arbitrarily small and may be the dominant component\\nin the generalization risk. The only way to reduce the approximation error is by expanding\\nthe class Gto include a larger set of possible functions.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 49, 'page_label': '32'}, page_content='the class Gto include a larger set of possible functions.\\nThe third component is the statistical (estimation) errorstatistical\\n(estimation)\\nerror\\n. It depends on the training\\nset τand, in particular, on how well the learner gG\\nτ estimates the best possible prediction\\nfunction, gG, within class G. For any sensible estimator this error should decay to zero (in\\nprobability or expectation) as the training size tends to infinity.☞439\\nThe approximation–estimation tradeo ffapproximation–\\nestimation\\ntradeoff\\npits two competing demands against each\\nother. The first is that the class Ghas to be simple enough so that the statistical error is\\nnot too large. The second is that the classGhas to be rich enough to ensure a small approx-\\nimation error. Thus, there is a tradeoff between the approximation and estimation errors.\\nFor the special case of the squared-error loss, the generalization risk is equal toℓ(gG\\nτ) =\\nE(Y −gG\\nτ(X))2; that is, the expected squared error 1 between the predicted value gG'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 49, 'page_label': '32'}, page_content='τ) =\\nE(Y −gG\\nτ(X))2; that is, the expected squared error 1 between the predicted value gG\\nτ(X)\\nand the response Y. Recall that in this case the optimal prediction function is given by\\ng∗(x) = E[Y |X = x]. The decomposition (2.16) can now be interpreted as follows.\\n1. The first component, ℓ∗ = E(Y −g∗(X))2, is the irreducible error, as no prediction\\nfunction will yield a smaller expected squared error.\\n2. The second component, the approximation error ℓ(gG) −ℓ(g∗), is equal to E(gG(X) −\\ng∗(X))2. We leave the proof (which is similar to that of Theorem 2.1) as an exercise;\\nsee Exercise 2. Thus, the approximation error (defined as a risk difference) can here\\nbe interpreted as the expected squared error between the optimal predicted value and\\nthe optimal predicted value within the class G.\\n3. For the third component, the statistical error, ℓ(gG\\nτ) −ℓ(gG) there is no direct inter-\\npretation as an expected squared error unless Gis the class of linear functions; that'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 49, 'page_label': '32'}, page_content='pretation as an expected squared error unless Gis the class of linear functions; that\\n1Colloquially called mean squared error.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 50, 'page_label': '33'}, page_content='Statistical Learning 33\\nis, g(x) = x⊤β for some vector β. In this case we can write (see Exercise 3) the\\nstatistical error as ℓ(gG\\nτ) −ℓ(gG) = E(gG\\nτ(X) −gG(X))2.\\nThus, when using a squared-error loss, the generalization risk for a linear class Gcan\\nbe decomposed as:\\nℓ(gG\\nτ) = E(gG\\nτ(X) −Y)2 = ℓ∗+ E(gG(X) −g∗(X))2\\n|                 {z                 }\\napproximation error\\n+ E(gG\\nτ(X) −gG(X))2\\n|                  {z                  }\\nstatistical error\\n. (2.17)\\nNote that in this decomposition the statistical error is the only term that depends on the\\ntraining set.\\nExample 2.2 (Polynomial Regression (cont.)) We continue Example 2.1. Here G =\\nGp is the class of linear functions of x = [1,u,u2,..., up−1]⊤, and g∗(x) = x⊤β∗. Condi-\\ntional on X = x we have that Y = g∗(x) + ε(x), with ε(x) ∼N(0,ℓ∗), where ℓ∗ = E(Y −\\ng∗(X))2 = 25 is the irreducible error. We wish to understand how the approximation and\\nstatistical errors behave as we change the complexity parameter p.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 50, 'page_label': '33'}, page_content='statistical errors behave as we change the complexity parameter p.\\nFirst, we consider the approximation error. Any function g ∈Gp can be written as\\ng(x) = h(u) = β1 + β2u + ··· + βpup−1 = [1,u,..., up−1] β,\\nand so g(X) is distributed as [1 ,U,..., Up−1]β, where U ∼ U(0,1). Similarly, g∗(X) is\\ndistributed as [1 ,U,U2,U3]β∗. It follows that an expression for the approximation error\\nis:\\nR 1\\n0\\n\\x10\\n[1,u,..., up−1] β−[1,u,u2,u3] β∗\\x112\\ndu.To minimize this error, we set the gradient\\nwith respect to βto zero and obtain the p linear equations ☞ 397\\nR 1\\n0\\n\\x10\\n[1,u,..., up−1] β−[1,u,u2,u3] β∗\\x11\\ndu = 0,\\nR 1\\n0\\n\\x10\\n[1,u,..., up−1] β−[1,u,u2,u3 ]β∗\\x11\\nu du = 0,\\n...\\nR 1\\n0\\n\\x10\\n[1,u,..., up−1] β−[1,u,u2,u3] β∗\\x11\\nup−1du = 0.\\nLet\\nHp =\\nZ 1\\n0\\n[1,u,..., up−1]⊤[1,u,..., up−1] du\\nbe the p ×p Hilbert matrix Hilbert matrix, which has (i, j)-th entry given by\\nR 1\\n0 ui+ j−2 du = 1/(i + j −1).\\nThen, the above system of linear equations can be written as Hpβ = eHβ∗, where eH is the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 50, 'page_label': '33'}, page_content='Then, the above system of linear equations can be written as Hpβ = eHβ∗, where eH is the\\np ×4 upper left sub-block of Hep and ep = max{p,4}. The solution, which we denote by βp,\\nis:\\nβp =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n65\\n6 , p = 1,\\n[−20\\n3 ,35]⊤, p = 2,\\n[−5\\n2 ,10,25]⊤, p = 3,\\n[10,−140,400,−250,0,..., 0]⊤, p ⩾4.\\n(2.18)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 51, 'page_label': '34'}, page_content='34 Tradeoffs in Statistical Learning\\nHence, the approximation error E\\n\\x10\\ngGp (X) −g∗(X)\\n\\x112\\nis given by\\nZ 1\\n0\\n\\x10\\n[1,u,..., up−1] βp −[1,u,u2,u3] β∗\\x112\\ndu =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n32225\\n252 ≈127.9, p = 1,\\n1625\\n63 ≈25.8, p = 2,\\n625\\n28 ≈22.3, p = 3,\\n0, p ⩾4.\\n(2.19)\\nNotice how the approximation error becomes smaller as p increases. In this particular\\nexample the approximation error is in fact zero for p ⩾4. In general, as the class of ap-\\nproximating functions Gbecomes more complex, the approximation error goes down.\\nNext, we illustrate the typical behavior of the statistical error. Since gτ(x) = x⊤bβ, the\\nstatistical error can be written as\\nZ 1\\n0\\n\\x10\\n[1,..., up−1](bβ−βp)\\n\\x112\\ndu = (bβ−βp)⊤Hp(bβ−βp). (2.20)\\nFigure 2.8 illustrates the decomposition (2.17) of the generalization risk for thesame train-\\ning set that was used to compute the test loss in Figure 2.7. Recall that test loss gives an\\nestimate of the generalization risk, using independent test data. Comparing the two figures,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 51, 'page_label': '34'}, page_content='estimate of the generalization risk, using independent test data. Comparing the two figures,\\nwe see that in this case the two match closely. The global minimum of the statistical error is\\napproximately 0.28, with minimizer p = 4. Since the approximation error is monotonically\\ndecreasing to zero, p = 4 is also the global minimizer of the generalization risk.\\n0 2 4 6 8 10 12 14 16 18\\n0\\n50\\n100\\n150 approximation error\\nstatistical error\\nirreducible error\\ngeneralization risk\\nFigure 2.8: The generalization risk for a particular training set is the sum of the irreducible\\nerror, the approximation error, and the statistical error. The approximation error decreases\\nto zero as p increases, whereas the statistical error has a tendency to increase after p = 4.\\nNote that the statistical error depends on the estimate bβ, which in its turn depends on\\nthe training set τ. We can obtain a better understanding of the statistical error by consid-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 51, 'page_label': '34'}, page_content='the training set τ. We can obtain a better understanding of the statistical error by consid-\\nering its expected behavior; that is, averaged over many training sets. This is explored in\\nExercise 11.\\nUsing again a squared-error loss, a second decomposition (for general G) starts from\\nℓ(gG\\nτ) = ℓ∗+ ℓ(gG\\nτ) −ℓ(g∗),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 52, 'page_label': '35'}, page_content='Statistical Learning 35\\nwhere the statistical error and approximation error are combined. Using similar reasoning\\nas in the proof of Theorem 2.1, we have\\nℓ(gG\\nτ) = E(gG\\nτ(X) −Y)2 = ℓ∗+ E\\n\\x10\\ngG\\nτ(X) −g∗(X)\\n\\x112\\n= ℓ∗+ ED2(X,τ),\\nwhere D(x,τ) := gG\\nτ(x) −g∗(x). Now consider the random variable D(x,T) for a random\\ntraining set T. The expectation of its square is:\\nE\\n\\x10\\ngG\\nT(x) −g∗(x)\\n\\x112\\n= ED2(x,T) = (ED(x,T))2 + Var D(x,T)\\n= (EgG\\nT(x) −g∗(x))2\\n|                 {z                 }\\npointwise squared bias\\n+ Var gG\\nT(x)|     {z     }\\npointwise variance\\n. (2.21)\\nIf we view the learner gG\\nT(x) as a function of a random training set, then the pointwise\\nsquared bias pointwise\\nsquared bias\\nterm is a measure for how close gG\\nT(x) is on average to the true g∗(x),\\nwhereas the pointwise variance term measures the deviation of gG\\nT(x) from its expected pointwise\\nvariancevalue EgG\\nT(x). The squared bias can be reduced by making the class of functions Gmore'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 52, 'page_label': '35'}, page_content='variancevalue EgG\\nT(x). The squared bias can be reduced by making the class of functions Gmore\\ncomplex. However, decreasing the bias by increasing the complexity often leads to an in-\\ncrease in the variance term. We are thus seeking learners that provide an optimal balance\\nbetween the bias and variance, as expressed via a minimal generalization risk. This is called\\nthe bias–variance tradeoff bias–variance\\ntradeoff\\n.\\nNote that the expected generalization risk (2.6) can be written asℓ∗+ED2(X,T), where\\nX and Tare independent. It therefore decomposes as\\nEℓ(gG\\nT) = ℓ∗+ E(E[gG\\nT(X) |X] −g∗(X))2\\n|                           {z                           }\\nexpected squared bias\\n+ E[Var[gG\\nT(X) |X]]|                 {z                 }\\nexpected variance\\n. (2.22)\\n2.5 Estimating Risk\\nThe most straightforward way to quantify the generalization risk (2.5) is to estimate it via\\nthe test loss (2.7). However, the generalization risk depends inherently on the training set,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 52, 'page_label': '35'}, page_content='the test loss (2.7). However, the generalization risk depends inherently on the training set,\\nand so different training sets may yield significantly di fferent estimates. Moreover, when\\nthere is a limited amount of data available, reserving a substantial proportion of the data\\nfor testing rather than training may be uneconomical. In this section we consider di fferent\\nmethods for estimating risk measures which aim to circumvent these difficulties.\\n2.5.1 In-Sample Risk\\nWe mentioned that, due to the phenomenon of overfitting, the training loss of the learner,\\nℓτ(gτ) (for simplicity, here we omitGfrom gG\\nτ), is not a good estimate of the generalization\\nrisk ℓ(gτ) of the learner. One reason for this is that we use the same data for both training\\nthe model and assessing its risk. How should we then estimate the generalization risk or\\nexpected generalization risk?\\nTo simplify the analysis, suppose that we wish to estimate the average accuracy of the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 52, 'page_label': '35'}, page_content='To simplify the analysis, suppose that we wish to estimate the average accuracy of the\\npredictions of the learner gτ at the n feature vectors x1,..., xn (these are part of the training'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 53, 'page_label': '36'}, page_content='36 Estimating Risk\\nset τ). In other words, we wish to estimate the in-sample riskin-sample risk of the learner gτ:\\nℓin(gτ) = 1\\nn\\nnX\\ni=1\\nELoss(Y′\\ni ,gτ(xi)), (2.23)\\nwhere each response Y′\\ni is drawn from f (y |xi), independently. Even in this simplified set-\\nting, the training loss of the learner will be a poor estimate of the in-sample risk. Instead, the\\nproper way to assess the prediction accuracy of the learner at the feature vectorsx1,..., xn,\\nis to draw new response values Y′\\ni ∼ f (y |xi), i = 1,..., n, that are independent from the\\nresponses y1,..., yn in the training data, and then estimate the in-sample risk of gτ via\\n1\\nn\\nnX\\ni=1\\nLoss(Y′\\ni ,gτ(xi)).\\nFor a fixed training set τ, we can compare the training loss of the learner with the\\nin-sample risk. Their difference,\\nopτ = ℓin(gτ) −ℓτ(gτ),\\nis called the optimism (of the training loss), because it measures how much the training\\nloss underestimates (is optimistic about) the unknown in-sample risk. Mathematically, it is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 53, 'page_label': '36'}, page_content='loss underestimates (is optimistic about) the unknown in-sample risk. Mathematically, it is\\nsimpler to work with the expected optimismexpected\\noptimism\\n:\\nE[opT|X1 = x1,..., Xn = xn] =: EX opT,\\nwhere the expectation is taken over a random training set T, conditional on Xi = xi,\\ni = 1,..., n. For ease of notation, we have abbreviated the expected optimism to EX opT,\\nwhere EX denotes the expectation operator conditional on Xi = xi,i = 1,..., n. As in Ex-\\nample 2.1, the feature vectors are stored as the rows of ann×p matrix X. It turns out that the\\nexpected optimism for various loss functions can be expressed in terms of the (conditional)\\ncovariance between the observed and predicted response.\\nTheorem 2.2: Expected Optimism\\nFor the squared-error loss and 0–1 loss with 0–1 response, the expected optimism is\\nEX opT = 2\\nn\\nnX\\ni=1\\nCovX(gT(xi),Yi). (2.24)\\nProof: In what follows, all expectations are taken conditional on X1 = x1,..., Xn = xn.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 53, 'page_label': '36'}, page_content='Proof: In what follows, all expectations are taken conditional on X1 = x1,..., Xn = xn.\\nLet Yi be the response for xi and let bYi = gT(xi) be the predicted value. Note that the latter\\ndepends on Y1,..., Yn. Also, let Y′\\ni be an independent copy of Yi for the same xi, as in\\n(2.23). In particular, Y′\\ni has the same distribution as Yi and is statistically independent of\\nall {Yj}, including Yi, and therefore is also independent of bYi. We have\\nEX opT = 1\\nn\\nnX\\ni=1\\nEX\\nh\\n(Y′\\ni −bYi)2 −(Yi −bYi)2i\\n= 2\\nn\\nnX\\ni=1\\nEX\\nh\\n(Yi −Y′\\ni )bYi\\ni\\n= 2\\nn\\nnX\\ni=1\\n\\x10\\nEX[YibYi] −EXYi EXbYi\\n\\x11\\n= 2\\nn\\nnX\\ni=1\\nCovX(bYi,Yi).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 54, 'page_label': '37'}, page_content='Statistical Learning 37\\nThe proof for the 0–1 loss with 0–1 response is left as Exercise 4. □\\nIn summary, the expected optimism indicates how much, on average, the training loss\\ndeviates from the expected in-sample risk. Since the covariance of independent random\\nvariables is zero, the expected optimism is zero if the learnergT is statistically independent\\nfrom the responses Y1,..., Yn.\\nExample 2.3 (Polynomial Regression (cont.)) We continue Example 2.2, where the\\ncomponents of the response vectorY = [Y1,..., Yn]⊤are independent and normally distrib-\\nuted with variance ℓ∗ = 25 (the irreducible error) and expectations EXYi = g∗(xi) = x⊤\\ni β∗,\\ni = 1,..., n. Using the formula (2.15) for the least-squares estimator bβ, the expected op-\\ntimism (2.24) is\\n2\\nn\\nnX\\ni=1\\nCovX\\n\\x10\\nx⊤\\ni bβ,Yi\\n\\x11\\n= 2\\nntr\\n\\x10\\nCovX\\n\\x10\\nXbβ,Y\\n\\x11\\x11\\n= 2\\nntr \\x00CovX\\n\\x00XX+Y,Y\\x01\\x01\\n= 2tr (XX+CovX (Y,Y))\\nn = 2ℓ∗tr (XX+)\\nn = 2ℓ∗p\\nn .\\nIn the last equation we used the cyclic property of the trace (Theorem A.1): tr( XX+) = ☞ 357'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 54, 'page_label': '37'}, page_content='n .\\nIn the last equation we used the cyclic property of the trace (Theorem A.1): tr( XX+) = ☞ 357\\ntr(X+X) = tr(Ip), assuming that rank(X) = p. Therefore, an estimate for the in-sample risk\\n(2.23) is:\\nbℓin(gτ) = ℓτ(gτ) + 2ℓ∗p/n, (2.25)\\nwhere we have assumed that the irreducible risk ℓ∗ is known. Figure 2.9 shows that this\\nestimate is very close to the test loss from Figure 2.7. Hence, instead of computing the test\\nloss to assess the best model complexity p, we could simply have minimized the training\\nloss plus the correction term 2ℓ∗p/n. In practice, ℓ∗also has to be estimated somehow.\\n2 4 6 8 10 12 14 16 18\\n0\\n50\\n100\\n150\\nFigure 2.9: In-sample risk estimate bℓin(gτ) as a function of the number of parameters p of\\nthe model. The test loss is superimposed as a blue dashed curve.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 55, 'page_label': '38'}, page_content='38 Estimating Risk\\n2.5.2 Cross-Validation\\nIn general, for complex function classes G, it is very difficult to derive simple formulas of\\nthe approximation and statistical errors, let alone for the generalization risk or expected\\ngeneralization risk. As we saw, when there is an abundance of data, the easiest way to\\nassess the generalization risk for a given training setτis to obtain a test set τ′and evaluate\\nthe test loss (2.7). When a su fficiently large test set is not available but computational☞24\\nresources are cheap, one can instead gain direct knowledge of the expected generalization\\nrisk via a computationally intensive method called cross-validationcross-validation .\\nThe idea is to make multiple identical copies of the data set, and to partition each copy\\ninto different training and test sets, as illustrated in Figure 2.10. Here, there are four copies\\nof the data set (consisting of response and explanatory variables). Each copy is divided into'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 55, 'page_label': '38'}, page_content='of the data set (consisting of response and explanatory variables). Each copy is divided into\\na test set (colored blue) and training set (colored pink). For each of these sets, we estimate\\nthe model parameters using only training data and then predict the responses for the test\\nset. The average loss between the predicted and observed responses is then a measure for\\nthe predictive power of the model.\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x03\\x05\\n\\t\\n\\x07\\x08\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x03\\x05 \\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x03\\x05\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x03\\x05'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 55, 'page_label': '38'}, page_content='\\x07\\x08\\n\\n\\x07\\x08\\n\\n\\x07\\x08\\nFigure 2.10: An illustration of four-fold cross-validation, representing four copies of the\\nsame data set. The data in each copy is partitioned into a training set (pink) and a test\\nset (blue). The darker columns represent the response variable and the lighter ones the\\nexplanatory variables.\\nIn particular, suppose we partition a data setTof size n into K foldsfolds C1,..., CK of sizes\\nn1,..., nK (hence, n1 + ··· + nK = n). Typically nk ≈n/K, k = 1,..., K.\\nLet ℓCk be the test loss when using Ck as test data and all remaining data, denoted T−k,\\nas training data. Each ℓCk is an unbiased estimator of the generalization risk for training set\\nT−k; that is, for ℓ(gT−k ).\\nThe K-fold cross-validationK-fold\\ncross-validation\\nloss is the weighted average of these risk estimators:\\nCVK =\\nKX\\nk=1\\nnk\\nn ℓCk (gT−k )\\n= 1\\nn\\nKX\\nk=1\\nX\\ni∈Ck\\nLoss(gT−k (xi),yi)\\n= 1\\nn\\nnX\\ni=1\\nLoss(gT−κ(i) (xi),yi),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 56, 'page_label': '39'}, page_content=\"Statistical Learning 39\\nwhere the function κ : {1,..., n} 7→ {1,..., K}indicates to which of the K folds each\\nof the n observations belongs. As the average is taken over varying training sets {T−k}, it\\nestimates the expected generalization risk Eℓ(gT), rather than the generalization risk ℓ(gτ)\\nfor the particular training set τ.\\nExample 2.4 (Polynomial Regression (cont.)) For the polynomial regression ex-\\nample, we can calculate aK-fold cross-validation loss with a nonrandom partitioning of the\\ntraining set using the following code, which imports the previous code for the polynomial\\nregression example. We omit the full plotting code.\\npolyregCV.py\\nfrom polyreg3 import *\\nK_vals = [5, 10, 100] # number of folds\\ncv = np.zeros(( len (K_vals), max_p)) # cv loss\\nX = np.ones((n, 1))\\nfor p in p_range:\\nif p > 1:\\nX = np.hstack((X, u**(p-1)))\\nj = 0\\nfor K in K_vals:\\nloss = []\\nfor k in range (1, K+1):\\n# integer indices of test samples\\ntest_ind = ((n/K)*(k-1) + np.arange(1,n/K+1)-1).astype( 'int')\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 56, 'page_label': '39'}, page_content=\"# integer indices of test samples\\ntest_ind = ((n/K)*(k-1) + np.arange(1,n/K+1)-1).astype( 'int')\\ntrain_ind = np.setdiff1d(np.arange(n), test_ind)\\nX_train, y_train = X[train_ind, :], y[train_ind, :]\\nX_test, y_test = X[test_ind, :], y[test_ind]\\n# fit model and evaluate test loss\\nbetahat = solve(X_train.T @ X_train, X_train.T @ y_train)\\nloss.append(norm(y_test - X_test @ betahat) ** 2)\\ncv[j, p-1] = sum (loss)/n\\nj += 1\\n# basic plotting\\nplt.plot(p_range, cv[0, :], 'k-.')\\nplt.plot(p_range, cv[1, :], 'r')\\nplt.plot(p_range, cv[2, :], 'b--')\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 57, 'page_label': '40'}, page_content='40 Modeling Data\\n2\\n 4\\n 6\\n 8\\n 10\\n 12\\n 14\\n 16\\n 18\\nNumber of parameters p\\n50\\n100\\n150\\n200\\n250\\n300K-fold cross-validation loss\\nK=5\\nK=10\\nK=100\\nFigure 2.11: K-fold cross-validation for the polynomial regression example.\\nFigure 2.11 shows the cross-validation loss forK ∈{5,10,100}. The case K = 100 cor-\\nresponds to the leave-one-out cross-validation, which can be computed more e fficientlyleave-one-out\\ncross-validation using the formula in Theorem 5.1.\\n☞174\\n2.6 Modeling Data\\nThe first step in any data analysis is tomodelmodel the data in one form or another. For example,\\nin an unsupervised learning setting with data represented by a vector x = [x1,..., xp]⊤, a\\nvery general model is to assume thatx is the outcome of a random vectorX = [X1,..., Xp]⊤\\nwith some unknown pdf f . The model can then be refined by assuming a specific form of\\nf .\\nWhen given a sequence of such data vectorsx1,..., xn, one of the simplest models is to'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 57, 'page_label': '40'}, page_content='f .\\nWhen given a sequence of such data vectorsx1,..., xn, one of the simplest models is to\\nassume that the corresponding random vectors X1,..., Xn are independent and identically\\ndistributed (iid). We write☞429\\nX1,..., Xn\\niid\\n∼f or X1,..., Xn\\niid\\n∼Dist,\\nto indicate that the random vectors form an iid sample from a sampling pdf f or sampling\\ndistribution Dist. This model formalizes the notion that the knowledge about one variable\\ndoes not provide extra information about another variable. The main theoretical use of\\nindependent data models is that the joint density of the random vectorsX1,..., Xn is simply\\nthe product of the marginal ones; see Theorem C.1. Specifically,☞429\\nfX1,..., Xn (x1,..., xn) = f (x1) ··· f (xn).\\nIn most models of this kind, our approximation or model for the sampling distribution is\\nspecified up to a small number of parameters. That is, g(x) is of the form g(x |β) which\\nis known up to some parameter vector β. Examples for the one-dimensional case ( p = 1)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 57, 'page_label': '40'}, page_content='is known up to some parameter vector β. Examples for the one-dimensional case ( p = 1)\\ninclude the N(µ,σ2),Bin(n,p), and Exp(λ) distributions. See Tables C.1 and C.2 for other☞425'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 58, 'page_label': '41'}, page_content='Statistical Learning 41\\ncommon sampling distributions.\\nTypically, the parameters are unknown and must be estimated from the data. In a non-\\nparametric setting the whole sampling distribution would be unknown. To visualize the\\nunderlying sampling distribution from outcomes x1,..., xn one can use graphical repres-\\nentations such as histograms, density plots, and empirical cumulative distribution func-\\ntions, as discussed in Chapter 1. ☞ 11\\nIf the order in which the data were collected (or their labeling) is not informative or\\nrelevant, then the joint pdf of X1,..., Xn satisfies the symmetry:\\nfX1,...,Xn (x1,..., xn) = fXπ1 ,...,Xπn (xπ1 ,..., xπn ) (2.26)\\nfor any permutation π1,...,π n of the integers 1 ,..., n. We say that the infinite sequence\\nX1,X2,... is exchangeable exchangeableif this permutational invariance (2.26) holds for any finite subset\\nof the sequence. As we shall see in Section 2.9 on Bayesian learning, it is common to'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 58, 'page_label': '41'}, page_content='of the sequence. As we shall see in Section 2.9 on Bayesian learning, it is common to\\nassume that the random vectors X1,..., Xn are a subset of an exchangeable sequence and\\nthus satisfy (2.26). Note that while iid random variables are exchangeable, the converse is\\nnot necessarily true. Thus, the assumption of an exchangeable sequence of random vectors\\nis weaker than the assumption of iid random vectors.\\nFigure 2.12 illustrates the modeling tradeoffs. The keywords within the triangle repres-\\nent various modeling paradigms. A few keywords have been highlighted, symbolizing their\\nimportance in modeling. The specific meaning of the keywords does not concern us here,\\nbut the point is there are many models to choose from, depending on what assumptions are\\nmade about the data.\\nFigure 2.12: Illustration of the modeling dilemma. Complex models are more generally\\napplicable, but may be di fficult to analyze. Simple models may be highly tractable, but'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 58, 'page_label': '41'}, page_content='applicable, but may be di fficult to analyze. Simple models may be highly tractable, but\\nmay not describe the data accurately. The triangular shape signifies that there are a great\\nmany specific models but not so many generic ones.\\nOn the one hand, models that make few assumptions are more widely applicable, but at\\nthe same time may not be very mathematically tractable or provide insight into the nature\\nof the data. On the other hand, very specific models may be easy to handle and interpret, but'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 59, 'page_label': '42'}, page_content='42 Modeling Data\\nmay not match the data very well. This tradeoff between the tractability and applicability of\\nthe model is very similar to the approximation–estimation tradeoff described in Section 2.4.\\nIn the typical unsupervised setting we have a training setτ= {x1,..., xn}that is viewed\\nas the outcome of n iid random variables X1,..., Xn from some unknown pdf f . The ob-\\njective is then to learn or estimate f from the finite training data. To put the learning in\\na similar framework as for supervised learning discussed in the preceding Sections 2.3–\\n2.5, we begin by specifying a class of probability density functions Gp := {g(·|θ),θ∈Θ},\\nwhere θis a parameter in some subset Θ of Rp. We now seek the best g in Gp to minimize\\nsome risk. Note that Gp may not necessarily contain the true f even for very large p.\\nWe stress that our notation g(x) has a different meaning in the supervised and unsu-\\npervised case. In the supervised case, g is interpreted as a prediction function for a'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 59, 'page_label': '42'}, page_content='pervised case. In the supervised case, g is interpreted as a prediction function for a\\nresponse y; in the unsupervised setting, g is an approximation of a density f .\\nFor each x we measure the discrepancy between the true model f (x) and the hypothes-\\nized model g(x |θ) using the loss function\\nLoss( f (x),g(x |θ)) = ln f (x)\\ng(x |θ) = ln f (x) −ln g(x |θ).\\nThe expected value of this loss (that is, the risk) is thus\\nℓ(g) = Eln f (X)\\ng(X |θ) =\\nZ\\nf (x) ln f (x)\\ng(x |θ) dx. (2.27)\\nThe integral in (2.27) provides a fundamental way to measure the distance between two\\ndensities and is called the Kullback–Leibler (KL) divergence2Kullback–\\nLeibler\\ndivergence\\nbetween f and g(·|θ). Note\\nthat the KL divergence is not symmetric in f and g(·|θ). Moreover, it is always greater\\nthan or equal to 0 (see Exercise 15) and equal to 0 when f = g(·|θ).\\nUsing similar notation as for the supervised learning setting in Table 2.1, define gGp as'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 59, 'page_label': '42'}, page_content='Using similar notation as for the supervised learning setting in Table 2.1, define gGp as\\nthe global minimizer of the risk in the class Gp; that is, gGp = argming∈Gp ℓ(g). If we define\\nθ∗= argmin\\nθ\\nELoss( f (X),g(X |θ)) = argmin\\nθ\\nZ \\x00ln f (x) −ln g(x |θ)\\x01f (x) dx\\n= argmax\\nθ\\nZ\\nf (x) lng(x |θ) dx = argmax\\nθ\\nEln g(X |θ),\\nthen gGp = g(·|θ∗) and learning gGp is equivalent to learning (or estimating) θ∗. To learn θ∗\\nfrom a training set τ= {x1,..., xn}we then minimize the training loss,\\n1\\nn\\nnX\\ni=1\\nLoss( f (xi),g(xi |θ)) = −1\\nn\\nnX\\ni=1\\nln g(xi |θ) + 1\\nn\\nnX\\ni=1\\nln f (xi),\\ngiving:\\nbθn := argmax\\nθ\\n1\\nn\\nnX\\ni=1\\nln g(xi |θ). (2.28)\\n2Sometimes called cross-entropy distance.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 60, 'page_label': '43'}, page_content='Statistical Learning 43\\nAs the logarithm is an increasing function, this is equivalent to\\nbθn := argmax\\nθ\\nnY\\ni=1\\ng(xi |θ),\\nwhere Qn\\ni=1 g(xi |θ) is the likelihood of the data; that is, the joint density of the {Xi}eval-\\nuated at the points {xi}. We therefore have recovered the classical maximum likelihood\\nestimate of θ∗. maximum\\nlikelihood\\nestimate\\n☞ 456\\nWhen the risk ℓ(g(·|θ)) is convex in θover a convex set Θ, we can find the maximum\\nlikelihood estimator by setting the gradient of the training loss to zero; that is, we solve\\n−1\\nn\\nnX\\ni=1\\nS(xi |θ) = 0,\\nwhere S(x |θ) := ∂ln g(x |θ)\\n∂θ is the gradient of ln g(x |θ) with respect to θand is often called\\nthe score score.\\nExample 2.5 (Exponential Model) Suppose we have the training dataτn = {x1,..., xn},\\nwhich is modeled as a realization of n positive iid random variables: X1,..., Xn ∼iid f (x).\\nWe select the class of approximating functions Gto be the parametric class {g : g(x |θ) ='),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 60, 'page_label': '43'}, page_content='We select the class of approximating functions Gto be the parametric class {g : g(x |θ) =\\nθexp(−x θ),x > 0,θ > 0}. In other words, we look for the best gG within the family of\\nexponential distributions with unknown parameter θ> 0. The likelihood of the data is\\nnY\\ni=1\\ng(xi |θ) =\\nnY\\ni=1\\nθexp(−θxi) = exp(−θn xn + n ln θ)\\nand the score isS (x |θ) = −x+θ−1. Thus, maximizing the likelihood with respect toθis the\\nsame as maximizing −θn xn + n ln θor solving −Pn\\ni=1 S (xi |θ)/n = xn −θ−1 = 0. In other\\nwords, the solution to (2.28) is the maximum likelihood estimate bθn = 1/xn.\\nIn a supervised setting, where the data is represented by a vector x of explanatory\\nvariables and a response y, the general model is that ( x,y) is an outcome of ( X,Y) ∼f\\nfor some unknown f . And for a training sequence ( x1,y1),..., (xn,yn) the default model\\nassumption is that ( X1,Y1),..., (Xn,Yn) ∼iid f . As explained in Section 2.2, the analysis'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 60, 'page_label': '43'}, page_content='assumption is that ( X1,Y1),..., (Xn,Yn) ∼iid f . As explained in Section 2.2, the analysis\\nprimarily involves the conditional pdf f (y |x) and in particular (when using the squared-\\nerror loss) the conditional expectation g∗(x) = E[Y |X = x]. The resulting representation\\n(2.2) allows us to then write the response at X = x as a function of the feature x plus an\\nerror term: Y = g∗(x) + ε(x).\\nThis leads to the simplest and most important model for supervised learning, where we\\nchoose a linear class Gof prediction or guess functions and assume that it is rich enough\\nto contain the true g∗. If we further assume that, conditional on X = x, the error term ε\\ndoes not depend on x, that is, Eε= 0 and Var ε= σ2, then we obtain the following model.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 61, 'page_label': '44'}, page_content='44 Modeling Data\\nDefinition 2.1: Linear Model\\nIn a linear modellinear model the response Y depends on a p-dimensional explanatory variable\\nx = [x1,..., xp]⊤via the linear relationship\\nY = x⊤β+ ε, (2.29)\\nwhere Eε= 0 and Var ε= σ2.\\nNote that (2.29) is a model for a single pair ( x,Y). The model for the training set\\n{(xi,Yi)}is simply that each Yi satisfies (2.29) (with x = xi) and that the {Yi}are independ-\\nent. Gathering all responses in the vector Y = [Y1,..., Yn]⊤, we can write\\nY = Xβ+ ε, (2.30)\\nwhere ε = [ε1,...,ε n]⊤is a vector of iid copies of εand X is the so-called model matrixmodel matrix ,\\nwith rows x⊤\\n1 ,..., x⊤\\nn . Linear models are fundamental building blocks of statistical learning\\nalgorithms. For this reason, a large part of Chapter 5 is devoted to linear regression models.☞167\\nExample 2.6 (Polynomial Regression (cont.)) For our running Example 2.1, we see☞26\\nthat the data is described by a linear model of the form (2.30), with model matrix X given\\nin (2.10).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 62, 'page_label': '45'}, page_content='Statistical Learning 45\\nBefore we discuss a few other models in the following sections, we would like to em-\\nphasize a number of points about modeling.\\n• Any model for data is likely to be wrong. For example, real data (as opposed to\\ncomputer-generated data) are often assumed to come from a normal distribution,\\nwhich is never exactly true. However, an important advantage of using a normal\\ndistribution is that it has many nice mathematical properties, as we will see in Sec-\\ntion 2.7.\\n• Most data models depend on a number of unknown parameters, which need to be\\nestimated from the observed data.\\n• Any model for real-life data needs to be checked for suitability. An important cri-\\nterion is that data simulated from the model should resemble the observed data, at\\nleast for a certain choice of model parameters.\\nHere are some guidelines for choosing a model. Think of the data as a spreadsheet or\\ndata frame, as in Chapter 1, where rows represent the data units and the columns the data'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 62, 'page_label': '45'}, page_content='data frame, as in Chapter 1, where rows represent the data units and the columns the data\\nfeatures (variables, groups).\\n• First establish the type of the features (quantitative, qualitative, discrete, continuous,\\netc.).\\n• Assess whether the data can be assumed to be independent across rows or columns.\\n• Decide on the level of generality of the model. For example, should we use a simple\\nmodel with a few unknown parameters or a more generic model that has a large\\nnumber of parameters? Simple specific models are easier to fit to the data (low es-\\ntimation error) than more general models, but the fit itself may not be accurate (high\\napproximation error). The tradeoffs discussed in Section 2.4 play an important role\\nhere.\\n• Decide on using a classical (frequentist) or Bayesian model. Section 2.9 gives a short\\nintroduction to Bayesian learning. ☞ 48\\n2.7 Multivariate Normal Models\\nA standard model for numerical observations x1,..., xn (forming, e.g., a column in a'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 62, 'page_label': '45'}, page_content='A standard model for numerical observations x1,..., xn (forming, e.g., a column in a\\nspreadsheet or data frame) is that they are the outcomes of iid normal random variables\\nX1,..., Xn\\niid\\n∼N(µ,σ2).\\nIt is helpful to view a normally distributed random variable as a simple transformation\\nof a standard normal random variable. To wit, ifZ has a standard normal distribution, then\\nX = µ+ σZ has a N(µ,σ2) distribution. The generalization to n dimensions is discussed\\nin Appendix C.7. We summarize the main points: Let Z1,..., Zn\\niid\\n∼ N(0,1). The pdf of ☞ 434\\nZ = [Z1,..., Zn]⊤(that is, the joint pdf of Z1,..., Zn) is given by\\nfZ(z) =\\nnY\\ni=1\\n1√\\n2π\\ne−1\\n2 z2\\ni = (2π)−n\\n2 e−1\\n2 z⊤z, z ∈Rn. (2.31)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 63, 'page_label': '46'}, page_content='46 Multivariate Normal Models\\nWe write Z ∼N(0,In) and say that Z has a standard normal distribution in Rn. Let\\nX = µ+ B Z (2.32)\\nfor some m ×n matrix B and m-dimensional vector µ. Then X has expectation vector µand\\ncovariance matrix Σ = BB⊤; see (C.20) and (C.21). This leads to the following definition.☞432\\nDefinition 2.2: Multivariate Normal Distribution\\nAn m-dimensional random vector X that can be written in the form (2.32) for some\\nm-dimensional vector µ and m ×n matrix B, with Z ∼N(0,In), is said to have a\\nmultivariate normalmultivariate\\nnormal\\nor multivariate Gaussian distribution with mean vector µand\\ncovariance matrix Σ = BB⊤. We write X ∼N(µ,Σ).\\nThe m-dimensional density of a multivariate normal distribution has a very similar form\\nto the density of the one-dimensional normal distribution and is given in the next theorem.\\nWe leave the proof as an exercise; see Exercise 5.☞60\\nTheorem 2.3: Density of a Multivariate Random Vector'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 63, 'page_label': '46'}, page_content='Theorem 2.3: Density of a Multivariate Random Vector\\nLet X ∼N(µ,Σ), where the m ×m covariance matrix Σ is invertible. Then X has pdf\\nfX(x) = 1√(2π)m |Σ|\\ne−1\\n2 (x−µ)⊤Σ−1(x−µ), x ∈Rm. (2.33)\\nFigure 2.13 shows the pdfs of two bivariate (that is, two-dimensional) normal distribu-\\ntions. In both cases the mean vector isµ= [0,0]⊤and the variances (the diagonal elements\\nof Σ) are 1. The correlation coefficients (or, equivalently here, the covariances) are respect-\\nively ϱ= 0 and ϱ= 0.8.\\n0\\n0.1\\n2\\n0.2\\n0\\n-2 20-2\\n0\\n0.1\\n2\\n0.2\\n0\\n-2 20-2\\nFigure 2.13: Pdfs of bivariate normal distributions with means zero, variances 1, and cor-\\nrelation coefficients 0 (left) and 0.8 (right).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 64, 'page_label': '47'}, page_content='Statistical Learning 47\\nThe main reason why the multivariate normal distribution plays an important role in\\ndata science and machine learning is that it satisfies the following properties, the details\\nand proofs of which can be found in Appendix C.7: ☞ 434\\n1. A ffine combinations are normal.\\n2. Marginal distributions are normal.\\n3. Conditional distributions are normal.\\n2.8 Normal Linear Models\\nNormal linear models combine the simplicity of the linear model with the tractability of\\nthe Gaussian distribution. They are the principal model for traditional statistics, and include\\nthe classic linear regression and analysis of variance models.\\nDefinition 2.3: Normal Linear Model\\nIn a normal linear model normal linear\\nmodel\\nthe response Y depends on a p-dimensional explanatory\\nvariable x = [x1,..., xp]⊤, via the linear relationship\\nY = x⊤β+ ε, (2.34)\\nwhere ε∼N(0,σ2).\\nThus, a normal linear model is a linear model (in the sense of Definition 2.1) with'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 64, 'page_label': '47'}, page_content='Thus, a normal linear model is a linear model (in the sense of Definition 2.1) with\\nnormal error terms. Similar to (2.30), the corresponding normal linear model for the whole\\ntraining set {(xi,Yi)}has the form\\nY = Xβ+ ε, (2.35)\\nwhere X is the model matrix comprised of rows x⊤\\n1 ,..., x⊤\\nn and ε∼N(0,σ2In). Con-\\nsequently, Y can be written asY = Xβ+ σZ, where Z ∼N(0,In), so thatY ∼N(Xβ,σ2In).\\nIt follows from (2.33) that its joint density is given by ☞ 46\\ng(y |β,σ2,X) = (2πσ2)−n\\n2 e− 1\\n2σ2 ||y−Xβ||2\\n. (2.36)\\nEstimation of the parameter βcan be performed via the least-squares method, as discussed\\nin Example 2.1. An estimate can also be obtained via the maximum likelihood method.\\nThis simply means finding the parameters σ2 and β that maximize the likelihood of the\\noutcome y, given by the right-hand side of (2.36). It is clear that for every value of σ2\\nthe likelihood is maximal when ∥y −Xβ∥2 is minimal. As a consequence, the maximum'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 64, 'page_label': '47'}, page_content='the likelihood is maximal when ∥y −Xβ∥2 is minimal. As a consequence, the maximum\\nlikelihood estimate for βis the same as the least-squares estimate (2.15). We leave it as an\\nexercise (see Exercise 18) to show that the maximum likelihood estimate of σ2 is equal to ☞ 64\\ncσ2 = ∥y −Xbβ∥2\\nn , (2.37)\\nwhere bβis the maximum likelihood estimate (least squares estimate in this case) of β.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 65, 'page_label': '48'}, page_content='48 Bayesian Learning\\n2.9 Bayesian Learning\\nIn Bayesian unsupervised learning, we seek to approximate the unknown joint density\\nf (x1,..., xn) of the training data Tn = {X1,..., Xn}via a joint pdf of the form\\nZ \\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nnY\\ni=1\\ng(xi |θ)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8w(θ) dθ, (2.38)\\nwhere g(·|θ) belongs to a family of parametric densities Gp := {g(·|θ), θ ∈Θ}(viewed\\nas a family of pdfs conditional on a parameter θ in some set Θ ⊂Rp) and w(θ) is a pdf\\nthat belongs to a (possibly different) family of densities Wp. Note how the joint pdf (2.38)\\nsatisfies the permutational invariance (2.26) and can thus be useful as a model for training\\ndata which is part of an exchangeable sequence of random variables.\\nFollowing standard practice in a Bayesian context, instead of writing fX(x) and\\nfX |Y (x |y) for the pdf of X and the conditional pdf of X given Y, one simply writes\\nf (x) and f (x |y). If Y is a different random variable, its pdf (at y) is thus denoted by\\nf (y).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 65, 'page_label': '48'}, page_content='f (x) and f (x |y). If Y is a different random variable, its pdf (at y) is thus denoted by\\nf (y).\\nThus, we will use the same symbol g for different (conditional) approximating probab-\\nility densities and f for the different (conditional) true and unknown probability densities.\\nUsing Bayesian notation, we can write g(τ|θ) = Qn\\ni=1 g(xi |θ) and thus the approximating\\njoint pdf (2.38) can then be written as\\nR\\ng(τ|θ) w(θ) dθand the true unknown joint pdf as\\nf (τ) = f (x1,..., xn).\\nOnce Gp and Wp are specified, selecting an approximating function g(x) of the form\\ng(x) =\\nZ\\ng(x |θ) w(θ) dθ\\nis equivalent to selecting a suitablew from Wp. Similar to (2.27), we can use the Kullback–\\nLeibler risk to measure the discrepancy between the proposed approximation (2.38) and the\\ntrue f (τ):\\nℓ(g) = Eln f (T)R\\ng(T| θ) w(θ) dθ\\n=\\nZ\\nf (τ) ln f (τ)R\\ng(τ|θ) w(θ) dθ\\ndτ. (2.39)\\nThe main difference with (2.27) is that since the training data is not necessarily iid (it may'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 65, 'page_label': '48'}, page_content='The main difference with (2.27) is that since the training data is not necessarily iid (it may\\nbe exchangeable, for example), the expectation must be with respect to the joint density of☞41\\nT, not with respect to the marginal f (x) (as in the iid case).\\nMinimizing the training loss is equivalent to maximizing the likelihood of the training\\ndata τ; that is, solving the optimization problem\\nmax\\nw∈Wp\\nZ\\ng(τ|θ) w(θ) dθ,\\nwhere the maximization is over an appropriate class Wp of density functions that is be-\\nlieved to result in the smallest KL risk.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 66, 'page_label': '49'}, page_content='Statistical Learning 49\\nSuppose that we have a rough guess, denoted w0(θ), for the best w ∈Wp that min-\\nimizes the Kullback–Leibler risk. We can always increase the resulting likelihood L0 :=R\\ng(τ|θ) w0(θ) dθ by instead using the density w1(θ) := w0(θ) g(τ|θ)/L0, giving a likeli-\\nhood L1 :=\\nR\\ng(τ|θ) w1(θ) dθ. To see this, write L0 and L1 as expectations with respect to\\nw0. In particular, we can write\\nL0 = Ew0 g(τ|θ) and L1 = Ew1 g(τ|θ) = Ew0 g2(τ|θ)/L0.\\nIt follows that\\nL1 −L0 = 1\\nL0\\nEw0\\nh\\ng2(τ|θ) −L2\\n0\\ni\\n= 1\\nL0\\nVarw0 [g(τ|θ)] ⩾0. (2.40)\\nWe may thus expect to obtain better predictions usingw1 instead of w0, because w1 has\\ntaken into account the observed data τand increased the likelihood of the model. In fact,\\nif we iterate this process (see Exercise 20) and create a sequence of densities w1,w2,...\\nsuch that wt(θ) ∝wt−1(θ) g(τ|θ), then wt(θ) concentrates more and more of its probability\\nmass at the maximum likelihood estimator bθ(see (2.28)) and in the limit equals a (degen-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 66, 'page_label': '49'}, page_content='mass at the maximum likelihood estimator bθ(see (2.28)) and in the limit equals a (degen-\\nerate) point-mass pdf atbθ. In other words, in the limit we recover the maximum likelihood\\nmethod: gτ(x) = g(x |bθ). Thus, unless the class of densities Wp is restricted to be non-\\ndegenerate, maximizing the likelihood as much as possible leads to a degenerate choice\\nfor w(θ).\\nIn many situations, the maximum likelihood estimate g(τ|bθ) is either not an ap-\\npropriate approximation to f (τ) (see Example 2.9), or simply fails to exist (see Exer-\\ncise 10 in Chapter 4). In such cases, given an initial non-degenerate guess w0(θ) = g(θ), ☞ 162\\none can obtain a more appropriate and non-degenerate approximation to f (τ) by taking\\nw(θ) = w1(θ) ∝g(τ|θ) g(θ) in (2.38), giving the following Bayesian learner of f (x):\\ngτ(x) :=\\nZ\\ng(x |θ) g(τ|θ) g(θ)R\\ng(τ|ϑ) g(ϑ) dϑ\\ndθ, (2.41)\\nwhere\\nR\\ng(τ|ϑ) g(ϑ) dϑ= g(τ). Using Bayes’ formula for probability densities, ☞ 428\\ng(θ|τ) = g(τ|θ) g(θ)\\ng(τ) , (2.42)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 66, 'page_label': '49'}, page_content='g(θ|τ) = g(τ|θ) g(θ)\\ng(τ) , (2.42)\\nwe can write w1(θ) = g(θ|τ). With this notation, we have the following definitions.\\nDefinition 2.4: Prior, Likelihood, and Posterior\\nLet τ and Gp := {g(·|θ),θ ∈Θ}be the training set and family of approximating\\nfunctions.\\n• A pdf g(θ) that reflects our a priori beliefs about θis called the prior priorpdf.\\n• The conditional pdf g(τ|θ) is called the likelihood likelihood.\\n• Inference about θis given by the posterior posteriorpdf g(θ|τ), which is proportional\\nto the product of the prior and the likelihood:\\ng(θ|τ) ∝g(τ|θ) g(θ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 67, 'page_label': '50'}, page_content='50 Bayesian Learning\\nRemark 2.1 (Early Stopping) Bayes iteration is an example of an “early stopping”\\nheuristic for maximum likelihood optimization, where we exit after only one step. As ob-\\nserved above, if we keep iterating, we obtain the maximum likelihood estimate (MLE). In\\na sense the Bayes rule provides a regularization of the MLE. Regularization is discussed in\\nmore detail in Chapter 6; see also Example 2.9. The early stopping rule is also of benefit\\nin regularization; see Exercise 20 in Chapter 6.\\nOn the one hand, the initial guess g(θ) conveys the a priori (prior to training the\\nBayesian learner) information about the optimal density inWp that minimizes the KL risk.\\nUsing this prior g(θ), the Bayesian approximation to f (x) is the prior predictive densityprior predictive\\ndensity\\n:\\ng(x) =\\nZ\\ng(x |θ) g(θ) dθ.\\nOn the other hand, the posterior pdf conveys improved knowledge about this optimal dens-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 67, 'page_label': '50'}, page_content='On the other hand, the posterior pdf conveys improved knowledge about this optimal dens-\\nity in Wp after training with τ. Using the posterior g(θ|τ), the Bayesian learner of f (x) is\\nthe posterior predictive densityposterior\\npredictive\\ndensity\\n:\\ngτ(x) = g(x |τ) =\\nZ\\ng(x |θ) g(θ|τ) dθ,\\nwhere we have assumed that g(x |θ,τ) = g(x |θ); that is, the likelihood depends on τonly\\nthrough the parameter θ.\\nThe choice of the prior is typically governed by two considerations:\\n1. the prior should be simple enough to facilitate the computation or simulation of the\\nposterior pdf;\\n2. the prior should be general enough to model ignorance of the parameter of interest.\\nPriors that do not convey much knowledge of the parameter are said to be uninformat-\\nive. The uniform or flat prior in Example 2.9 (to follow) is frequently used.uninformative\\nprior\\nFor the purpose of analytical and numerical computations, we can view θas a ran-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 67, 'page_label': '50'}, page_content='prior\\nFor the purpose of analytical and numerical computations, we can view θas a ran-\\ndom vector with prior density g(θ), which after training is updated to the posterior\\ndensity g(θ|τ).\\nThe above thinking allows us to write g(x |τ) ∝\\nR\\ng(x |θ) g(τ|θ) g(θ) dθ, for example,\\nthus ignoring any constants that do not depend on the argument of the densities.\\nExample 2.7 (Normal Model) Suppose that the training data T= {X1,..., Xn} is\\nmodeled using the likelihood g(x |θ) that is the pdf of\\nX |θ∼N(µ,σ2),\\nwhere θ := [µ,σ2]⊤. Next, we need to specify the prior distribution of θ to complete\\nthe model. We can specify prior distributions for µand σ2 separately and then take their'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 68, 'page_label': '51'}, page_content='Statistical Learning 51\\nproduct to obtain the prior for vector θ(assuming independence). A possible prior distri-\\nbution for µis\\nµ∼N(ν,ϕ2). (2.43)\\nIt is typical to refer to any parameters of the prior density as hyperparameters hyperparamet-\\ners\\nof the\\nBayesian model. Instead of giving directly a prior for σ2 (or σ), it turns out to be con-\\nvenient to give the following prior distribution to 1/σ2:\\n1\\nσ2 ∼Gamma(α,β). (2.44)\\nThe smaller αand βare, the less informative is the prior. Under this prior,σ2 is said to have\\nan inverse gamma inverse gamma3 distribution. If 1/Z ∼Gamma(α,β), then the pdf of Z is proportional\\nto exp (−β/z) /zα+1 (Exercise 19). The Bayesian posterior is then given by: ☞ 64\\ng(µ,σ2 |τ) ∝g(µ) ×g(σ2) ×g(τ|µ,σ2)\\n∝exp\\n(\\n−(µ−ν)2\\n2ϕ2\\n)\\n×\\nexp\\nn\\n−β/σ2\\no\\n(σ2)α+1 ×\\nexp\\nn\\n−P\\ni(xi −µ)2/(2σ2)\\no\\n(σ2)n/2\\n∝(σ2)−n/2−α−1 exp\\n(\\n−(µ−ν)2\\n2ϕ2 − β\\nσ2 −(µ−xn)2 + S 2\\nn\\n2σ2/n\\n)\\n,\\nwhere S 2\\nn := 1\\nn\\nP\\ni x2\\ni −x2\\nn = 1\\nn\\nP\\ni(xi −xn)2 is the (scaled) sample variance. All inference'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 68, 'page_label': '51'}, page_content='where S 2\\nn := 1\\nn\\nP\\ni x2\\ni −x2\\nn = 1\\nn\\nP\\ni(xi −xn)2 is the (scaled) sample variance. All inference\\nabout (µ,σ2) is then represented by the posterior pdf. To facilitate computations it is helpful\\nto find out if the posterior belongs to a recognizable family of distributions. For example,\\nthe conditional pdf of µgiven σ2 and τis\\ng(µ|σ2,τ) ∝exp\\n(\\n−(µ−ν)2\\n2ϕ2 −(µ−xn)2\\n2σ2/n\\n)\\n,\\nwhich after simplification can be recognized as the pdf of\\n(µ|σ2,τ) ∼N\\n\\x10\\nγn xn + (1 −γn)ν, γn σ2/n\\n\\x11\\n, (2.45)\\nwhere we have defined the weight parameter: γn := n\\nσ2\\n.\\x10\\n1\\nϕ2 + n\\nσ2\\n\\x11\\n.We can then see that the\\nposterior mean E[µ|σ2,τ] = γn xn + (1 −γn)νis a weighted linear combination of the prior\\nmean ν and the sample average xn. Further, as n →∞, the weight γn →1 and thus the\\nposterior mean approaches the maximum likelihood estimate xn.\\nIt is sometimes possible to use a priorg(θ) that is not a bona fide probability density, in the\\nsense that\\nR'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 68, 'page_label': '51'}, page_content='sense that\\nR\\ng(θ) dθ= ∞, as long as the resulting posteriorg(θ|τ) ∝g(τ|θ)g(θ) is a proper\\npdf. Such a prior is called an improper prior improper prior.\\nExample 2.8 (Normal Model (cont.)) An example of an improper prior is obtained\\nfrom (2.43) when we let ϕ→∞ (the larger ϕ is, the more uninformative is the prior).\\n3Reciprocal gamma distribution would have been a better name.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 69, 'page_label': '52'}, page_content='52 Bayesian Learning\\nThen, g(µ) ∝1 is a flat prior, but\\nR\\ng(µ) dµ= ∞, making it an improper prior. Neverthe-\\nless, the posterior is a proper density, and in particular the conditional posterior of (µ|σ2,τ)\\nsimplifies to\\n(µ|σ2,τ) ∼N\\n\\x10\\nxn,σ2/n\\n\\x11\\n,\\nbecause the weight parameter γn goes to 1 as ϕ →∞. The improper prior g(µ) ∝1 also\\nallows us to simplify the posterior marginal for σ2:\\ng(σ2 |τ) =\\nZ\\ng(µ,σ2 |τ) dµ∝(σ2)−(n−1)/2−α−1 exp\\n(\\n−β+ nS 2\\nn/2\\nσ2\\n)\\n,\\nwhich we recognize as the density corresponding to\\n1\\nσ2\\n\\x0c\\x0c\\x0c\\x0cτ∼Gamma\\n \\nα+ n −1\\n2 , β+ n\\n2S 2\\nn\\n!\\n.\\nIn addition tog(µ) ∝1, we can also use an improper prior forσ2. If we take the limitα→0\\nand β→0 in (2.44), then we also obtain the improper prior g(σ2) ∝1/σ2 (or equivalently\\ng(1/σ2) ∝1/σ2). In this case, the posterior marginal density for σ2 implies that:\\nnS 2\\nn\\nσ2\\n\\x0c\\x0c\\x0c\\x0cτ∼χ2\\nn−1\\nand the posterior marginal density for µimplies that:\\nµ−xn\\nS n/\\n√\\nn −1\\n\\x0c\\x0c\\x0c\\x0cτ∼tn−1. (2.46)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 69, 'page_label': '52'}, page_content='n−1\\nand the posterior marginal density for µimplies that:\\nµ−xn\\nS n/\\n√\\nn −1\\n\\x0c\\x0c\\x0c\\x0cτ∼tn−1. (2.46)\\nIn general, deriving a simple formula for the posterior density of θ is either impossible\\nor too tedious. Instead, the Monte Carlo methods in Chapter 3 can be used to simulate\\n(approximately) from the posterior for the purposes of inference and prediction.\\nOne way in which a distributional result such as (2.46) can be useful is in the construc-\\ntion of a 95% credible intervalcredible\\ninterval\\nIfor the parameter µ; that is, an interval Isuch that the\\nprobability P[µ∈I| τ] is equal to 0.95. For example, the symmetric 95% credible interval\\nis\\nI=\\n\"\\nxn − S n\\n√\\nn −1\\nγ, xn + S n\\n√\\nn −1\\nγ\\n#\\n,\\nwhere γ is the 0 .975-quantile of the tn−1 distribution. Note that the credible interval is\\nnot a random object and that the parameter µ is interpreted as a random variable with a\\ndistribution. This is unlike the case of classical confidence intervals, where the parameter'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 69, 'page_label': '52'}, page_content='distribution. This is unlike the case of classical confidence intervals, where the parameter\\nis nonrandom, but the interval is (the outcome of) a random object.☞457\\nAs a generalization of the 95% Bayesian credible interval we can define a 1−αcredible\\nregioncredible region , which is any set Rsatisfying\\nP[θ∈R| τ] =\\nZ\\nθ∈R\\ng(θ|τ) dθ⩾1 −α. (2.47)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 70, 'page_label': '53'}, page_content='Statistical Learning 53\\nExample 2.9 (Bayesian Regularization of Maximum Likelihood) Consider model-\\ning the number of deaths during birth in a maternity ward. Suppose that the hospital data\\nconsists of τ = {x1,..., xn}, with xi = 1 if the i-th baby has died during birth and xi = 0\\notherwise, for i = 1,..., n. A possible Bayesian model for the data is θ∼U(0,1) (uniform\\nprior) with (X1,..., Xn |θ)\\niid\\n∼Ber(θ). The likelihood is therefore\\ng(τ|θ) =\\nnY\\ni=1\\nθxi (1 −θ)1−xi = θs (1 −θ)n−s,\\nwhere s = x1 + ··· + xn is the total number of deaths. Since g(θ) = 1, the posterior pdf is\\ng(θ|τ) ∝θs (1 −θ)n−s, θ ∈[0,1],\\nwhich is the pdf of the Beta(s + 1,n −s + 1) distribution. The normalization constant is\\n(n + 1)\\n\\x10n\\ns\\n\\x11\\n. The posterior pdf is shown in Figure 2.14 for ( s,n) = (0,100). It is not difficult\\nFigure 2.14: Posterior pdf for θ, with n = 100 and s = 0.\\nto see that the maximum a posteriori maximum a\\nposteriori\\n(MAP) estimate of θ(the mode or maximizer of the\\nposterior density) is\\nargmax\\nθ'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 70, 'page_label': '53'}, page_content='posteriori\\n(MAP) estimate of θ(the mode or maximizer of the\\nposterior density) is\\nargmax\\nθ\\ng(θ|τ) = s\\nn,\\nwhich agrees with the maximum likelihood estimate. Figure 2.14 also shows that the left\\none-sided 95% credible interval for θ is [0 ,0.0292], where 0 .0292 is the 0.95 quantile\\n(rounded) of the Beta(1,101) distribution.\\nObserve that when (s,n) = (0,100) the maximum likelihood estimate bθ = 0 infers that\\ndeaths at birth are not possible. We know that this inference is wrong — the probability of\\ndeath can never be zero, it is simply (and fortunately) too small to be inferred accurately\\nfrom a sample size of n = 100. In contrast to the maximum likelihood estimate, the pos-\\nterior mean E[θ|τ] = (s + 1)/(n + 2) is not zero for (s,n) = (0,100) and provides the more\\nreasonable point estimate of 0.0098 for the probability of death.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 71, 'page_label': '54'}, page_content='54 Bayesian Learning\\nIn addition, while computing a Bayesian credible interval poses no conceptual di ffi-\\nculties, it is not simple to derive a confidence interval for the maximum likelihood estimate\\nof bθ, because the likelihood as a function of θis not differentiable at θ = 0. As a result of\\nthis lack of smoothness, the usual confidence intervals based on the normal approximation\\ncannot be used.\\nWe now return to the unsupervised learning setting of Section 2.6, but consider this\\nfrom a Bayesian perspective. Recall from (2.39) that the Kullback–Leibler risk for an ap-\\nproximating function g is\\nℓ(g) =\\nZ\\nf (τ′\\nn)[ln f (τ′\\nn) −ln g(τ′\\nn)] dτ′\\nn,\\nwhere τ′\\nn denotes the test data. Since\\nR\\nf (τ′\\nn) ln f (τ′\\nn) dτ′\\nn plays no role in minimizing the\\nrisk, we consider instead the cross-entropy risk, defined as☞122\\nℓ(g) = −\\nZ\\nf (τ′\\nn) lng(τ′\\nn) dτ′\\nn.\\nNote that the smallest possible cross-entropy risk isℓ∗\\nn = −\\nR\\nf (τ′\\nn) ln f (τ′\\nn) dτ′\\nn. The expec-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 71, 'page_label': '54'}, page_content='n = −\\nR\\nf (τ′\\nn) ln f (τ′\\nn) dτ′\\nn. The expec-\\nted generalization risk of the Bayesian learner can then be decomposed as\\nEℓ(gTn ) = ℓ∗\\nn +\\nZ\\nf (τ′\\nn) ln f (τ′\\nn)\\nEg(τ′\\nn |Tn) dτ′\\nn\\n|                            {z                            }\\n“bias” component\\n+ E\\nZ\\nf (τ′\\nn) ln Eg(τ′\\nn |Tn)\\ng(τ′\\nn |Tn) dτ′\\nn\\n|                               {z                               }\\n“variance” component\\n,\\nwhere gTn (τ′\\nn) = g(τ′\\nn |Tn) =\\nR\\ng(τ′\\nn |θ) g(θ|Tn) dθis the posterior predictive density after\\nobserving Tn.\\nAssuming that the setsTn and T′\\nn are comprised of 2n iid random variables with density\\nf , we can show (Exercise 23) that the expected generalization risk simplifies to\\nEℓ(gTn ) = Eln g(Tn) −Eln g(T2n), (2.48)\\nwhere g(τn) and g(τ2n) are the prior predictive densities of τn and τ2n, respectively.\\nLet θn = argmaxθ g(θ|Tn) be the MAP estimator of θ∗ := argmaxθ Eln g(X |θ). As-\\nsuming that θn converges to θ∗(with probability one) and 1\\nn Eln g(Tn |θn) = Eln g(X |θ∗) +'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 71, 'page_label': '54'}, page_content='suming that θn converges to θ∗(with probability one) and 1\\nn Eln g(Tn |θn) = Eln g(X |θ∗) +\\nO(1/n), we can use the following large-sample approximation of the expected generaliza-\\ntion risk.\\nTheorem 2.4: Approximating the Bayesian Cross-Entropy Risk\\nFor n →∞, the expected cross-entropy generalization risk satisfies:\\nEℓ(gTn ) ≃−Eln g(Tn) −p\\n2 ln n, (2.49)\\nwhere (with p the dimension of the parameter vector θand θn the MAP estimator):\\nEln g(Tn) ≃Eln g(Tn |θn) −p\\n2 ln n. (2.50)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 72, 'page_label': '55'}, page_content='Statistical Learning 55\\nProof: To show (2.50), we apply Theorem C.21 to ln\\nR\\ne−nrn(θ)g(θ) dθ, where ☞ 450\\nrn(θ) := −1\\nn ln g(Tn |θ) = −1\\nn\\nnX\\ni=1\\nln g(Xi |θ)\\na.s.\\n−→− Eln g(X |θ) =: r(θ) <∞.\\nThis gives (with probability one)\\nln\\nZ\\ng(Tn |θ) g(θ) dθ≃−nr(θ∗) −p\\n2 ln(n).\\nTaking expectations on both sides and usingnr(θ∗) = nE[rn(θn)] + O(1), we deduce (2.50).\\nTo demonstrate (2.49), we derive the asymptotic approximation ofEln g(T2n) by repeating\\nthe argument for (2.50), but replacing n with 2n, where necessary. Thus, we obtain:\\nEln g(T2n) ≃−2nr(θ∗) −p\\n2 ln(2n).\\nThen, (2.49) follows from the identity (2.48). □\\nThe results of Theorem 2.4 have two major implications for model selection and assess-\\nment. First, (2.49) suggests that −ln g(Tn) can be used as a crude (leading-order) asymp-\\ntotic approximation to the expected generalization risk for large n and fixed p. In this\\ncontext, the prior predictive densityg(Tn) is usually called the model evidence model evidenceor marginal'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 72, 'page_label': '55'}, page_content='likelihood for the class Gp. Since the integral\\nR\\ng(Tn |θ) g(θ) dθis rarely available in closed\\nform, the exact computation of the model evidence is typically not feasible and may require\\nMonte Carlo estimation methods. ☞ 78\\nSecond, when the model evidence is di fficult to compute via Monte Carlo methods or\\notherwise, (2.50) suggests that we can use the following large-sample approximation:\\n−2Eln g(Tn) ≃−2 lng(Tn |θn) + p ln(n). (2.51)\\nThe asymptotic approximation on the right-hand side of (2.51) is called the Bayesian in-\\nformation criterion Bayesian\\ninformation\\ncriterion\\n(BIC). We prefer the classGp with the smallest BIC. The BIC is typic-\\nally used when the model evidence is di fficult to compute and n is sufficiently larger than\\np. For a fixed p, and as n becomes larger and larger, the BIC becomes a more and more\\naccurate estimator of −2Eln g(Tn). Note that the BIC approximation is valid even when the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 72, 'page_label': '55'}, page_content='accurate estimator of −2Eln g(Tn). Note that the BIC approximation is valid even when the\\ntrue density f < Gp. The BIC provides an alternative to the Akaike information criterion\\n(AIC) for model selection. However, while the BIC approximation does not assume that ☞ 126\\nthe true model f belongs to the parametric class under consideration, the AIC assumes\\nthat f ∈Gp. Thus, the AIC is merely a heuristic approximation based on the asymptotic\\napproximations in Theorem 4.1.\\nAlthough the above Bayesian theory has been presented in an unsupervised learn-\\ning setting, it can be readily extended to the supervised case. We only need to relabel\\nthe training set Tn. In particular, when (as is typical for regression models) the train-\\ning responses Y1,..., Yn are considered as random variables but the corresponding fea-\\nture vectors x1,..., xn are viewed as being fixed, then Tn is the collection of random re-\\nsponses {Y1,..., Yn}. Alternatively, we can simply identify Tn with the response vector'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 72, 'page_label': '55'}, page_content='sponses {Y1,..., Yn}. Alternatively, we can simply identify Tn with the response vector\\nY = [Y1,..., Yn]⊤. We will adopt this notation in the next example.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 73, 'page_label': '56'}, page_content='56 Bayesian Learning\\nExample 2.10 (Polynomial Regression (cont.)) Consider Example 2.2 once again, but\\nnow in a Bayesian framework, where the prior knowledge on ( σ2,β) is specified by\\ng(σ2) = 1/σ2 and β|σ2 ∼N(0,σ2D), and D is a (matrix) hyperparameter. Let Σ :=\\n(X⊤X + D−1)−1. Then the posterior can be written as:\\ng(β,σ2 |y) =\\nexp\\n\\x10\\n−∥y−Xβ∥2\\n2σ2\\n\\x11\\n(2πσ2)n/2 ×\\nexp\\n\\x10\\n−β⊤D−1β\\n2σ2\\n\\x11\\n(2πσ2)p/2 |D|1/2 × 1\\nσ2\\n,\\ng(y)\\n= (σ2)−(n+p)/2−1\\n(2π)(n+p)/2 |D|1/2 exp\\n \\n−∥Σ−1/2(β−β)∥2\\n2σ2 −(n + p + 2) σ2\\n2σ2\\n!,\\ng(y),\\nwhere β:= ΣX⊤y and σ2 := y⊤(I −XΣX⊤)y/(n + p + 2) are the MAP estimates of βand\\nσ2, and g(y) is the model evidence for Gp:\\ng(y) =\\n\"\\ng(β,σ2,y) dβdσ2\\n= |Σ|1/2\\n(2π)n/2|D|1/2\\nZ ∞\\n0\\nexp\\n\\x12\\n−(n+p+2) σ\\n2\\n2σ2\\n\\x13\\n(σ2)n/2+1 dσ2\\n= |Σ|1/2Γ(n/2)\\n|D|1/2(π(n + p + 2) σ2)n/2 .\\nTherefore, based on (2.49), we have\\n2Eℓ(gTn ) ≃−2 lng(y) = n ln\\nh\\nπ(n + p + 2) σ2i\\n−2 lnΓ(n/2) + ln |D|−ln |Σ|.\\nOn the other hand, the minus of the log-likelihood of Y can be written as\\n−ln g(y |β,σ2) = ∥y −Xβ∥2\\n2σ2 + n\\n2 ln(2πσ2)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 73, 'page_label': '56'}, page_content='−ln g(y |β,σ2) = ∥y −Xβ∥2\\n2σ2 + n\\n2 ln(2πσ2)\\n= ∥Σ−1/2(β−β)∥2\\n2σ2 + (n + p + 2) σ2\\n2σ2 + n\\n2 ln(2πσ2).\\nTherefore, the BIC approximation (2.51) is\\n−2 lng(y |β,σ2) + (p + 1) ln(n) = n[ln(2πσ2) + 1] + (p + 1) ln(n) + (p + 2), (2.52)\\nwhere the extra ln( n) term in ( p + 1) ln(n) is due to the inclusion of σ2 in θ = (σ2,β).\\nFigure 2.15 shows the model evidence and its BIC approximation, where we used a hyper-\\nparameter D = 104 ×Ip for the prior density of β. We can see that both approximations\\nexhibit a pronounced minimum at p = 4, thus identifying the true polynomial regression\\nmodel. Compare the overall qualitative shape of the cross-entropy risk estimate with the\\nshape of the square-error risk estimate in Figure 2.11.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 74, 'page_label': '57'}, page_content='Statistical Learning 57\\n123456789 1 0\\n600\\n650\\n700\\n750\\n800\\nFigure 2.15: The BIC and marginal likelihood used for model selection.\\nIt is possible to give the model complexity parameter p a Bayesian treatment, in which\\nwe define a prior density on the set of all models under consideration. For example, let\\ng(p), p = 1,..., m be a prior density on m candidate models. Treating the model com-\\nplexity index p as an additional parameter to θ ∈Rp, and applying Bayes’ formula, the\\nposterior for (θ,p) can be written as:\\ng(θ,p |τ) = g(θ|p,τ) ×g(p |τ)\\n= g(τ|θ,p) g(θ|p)\\ng(τ|p)|               {z               }\\nposterior of θgiven model p\\n× g(τ|p) g(p)\\ng(τ)|        {z        }\\nposterior of model p\\n.\\nThe model evidence for a fixed p is now interpreted as the prior predictive density of τ,\\nconditional on the model p:\\ng(τ|p) =\\nZ\\ng(τ|θ,p) g(θ|p) dθ,\\nand the quantity g(τ) = Pm\\np=1 g(τ|p) g(p) is interpreted as the marginal likelihood of all the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 74, 'page_label': '57'}, page_content='and the quantity g(τ) = Pm\\np=1 g(τ|p) g(p) is interpreted as the marginal likelihood of all the\\nm candidate models. Finally, a simple method for model selection is to pick the index bp\\nwith the largest posterior probability:\\nbp = argmax\\np\\ng(p |τ) = argmax\\np\\ng(τ|p) g(p).\\nExample 2.11 (Polynomial Regression (cont.)) Let us revisit Example 2.10 by giving\\nthe parameter p = 1,..., m, with m = 10, a Bayesian treatment. Recall that we used the\\nnotation τ= y in that example. We assume that the prior g(p) = 1/m is flat and uninform-\\native so that the posterior is given by\\ng(p |y) ∝g(y |p) = |Σ|1/2 Γ(n/2)\\n|D|1/2(π(n + p + 2) σ2)n/2 ,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 75, 'page_label': '58'}, page_content='58 Bayesian Learning\\nwhere all quantities in g(y |p) are computed using the first p columns of X. Figure 2.16\\nshows the resulting posterior density g(p |y). The figure also shows the posterior density\\nbg(y |p)\\x0eP10\\np=1 bg(y |p),where\\nbg(y |p) := exp\\n \\n−n[ln(2πσ2) + 1] + (p + 1) ln(n) + (p + 2)\\n2\\n!\\nis derived from the BIC approximation (2.52). In both cases, there is a clear maximum at\\np = 4, suggesting that a third-degree polynomial is the most appropriate model for the\\ndata.\\n1 2 3 4 5 6 7 8 9 10\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure 2.16: Posterior probabilities for each polynomial model of degree p −1.\\nSuppose that we wish to compare two models, say model p = 1 and model p = 2.\\nInstead of computing the posterior g(p |τ) explicitly, we can compare the posterior odds\\nratio:\\ng(p = 1 |τ)\\ng(p = 2 |τ) = g(p = 1)\\ng(p = 2) ×g(τ|p = 1)\\ng(τ|p = 2)|        {z        }\\nBayes factor B1 |2\\n.\\nThis gives rise to the Bayes factorBayes factor Bi |j, whose value signifies the strength of the evidence'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 75, 'page_label': '58'}, page_content='in favor of model i over model j. In particular Bi |j >1 means that the evidence in favor for\\nmodel i is larger.\\nExample 2.12 (Savage–Dickey Ratio) Suppose that we have two models. Model p =\\n2 has a likelihood g(τ|µ,ν, p = 2), depending on two parameters. Model p = 1 has the\\nsame functional form for the likelihood but now ν is fixed to some (known) ν0; that\\nis, g(τ|µ,p = 1) = g(τ|µ,ν = ν0,p = 2). We also assume that the prior information on µ'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 76, 'page_label': '59'}, page_content='Statistical Learning 59\\nfor model 1 is the same as that for model 2, conditioned on ν= ν0. That is, we assume\\ng(µ|p = 1) = g(µ|ν= ν0,p = 2). As model 2 contains model 1 as a special case, the latter\\nis said to be nested inside model 2. We can formally write (see also Exercise 26):\\ng(τ|p = 1) =\\nZ\\ng(τ|µ,p = 1) g(µ|p = 1) dµ\\n=\\nZ\\ng(τ|µ,ν = ν0,p = 2) g(µ|ν= ν0,p = 2) dµ\\n= g(τ|ν= ν0,p = 2) = g(τ,ν = ν0 |p = 2)\\ng(ν= ν0 |p = 2) .\\nHence, the Bayes factor simplifies to\\nB1 |2 = g(τ|p = 1)\\ng(τ|p = 2) = g(τ,ν = ν0 |p = 2)\\ng(ν= ν0 |p = 2)\\n\\x1e\\ng(τ|p = 2) = g(ν= ν0 |τ,p = 2)\\ng(ν= ν0 |p = 2) .\\nIn other words, B1 |2 is the ratio of the posterior density to the prior density ofν, evaluated at\\nν= ν0 and both under the unrestricted modelp = 2. This ratio of posterior to prior densities\\nis called the Savage–Dickey density ratio Savage–Dickey\\ndensity ratio\\n.\\nWhether to use a classical (frequentist) or Bayesian model is largely a question of con-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 76, 'page_label': '59'}, page_content='.\\nWhether to use a classical (frequentist) or Bayesian model is largely a question of con-\\nvenience. Classical inference is useful because it comes with a huge repository of ready-\\nto-use results, and requires no (subjective) prior information on the parameters. Bayesian\\nmodels are useful because the whole theory is based on the elegant Bayes’ formula, and\\nuncertainty in the inference (e.g., confidence intervals) can be quantified much more nat-\\nurally (e.g., credible intervals). A usual practice is to “Bayesify” a classical model, simply\\nby adding some prior information on the parameters.\\nFurther Reading\\nA popular textbook on statistical learning is [55]. Accessible treatments of mathematical\\nstatistics can be found, for example, in [69], [74], and [124]. More advanced treatments\\nare given in [10], [25], and [78]. A good overview of modern-day statistical inference\\nis given in [36]. Classical references on pattern classification and machine learning are'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 76, 'page_label': '59'}, page_content='is given in [36]. Classical references on pattern classification and machine learning are\\n[12] and [35]. For advanced learning theory including information theory and Rademacher\\ncomplexity, we refer to [28] and [109]. An applied reference for Bayesian inference is [46].\\nFor a survey of numerical techniques relevant to computational statistics, see [90].\\nExercises\\n1. Suppose that the loss function is the piecewise linear function\\nLoss(y,by) = α(by −y)+ + β(y −by)+, α,β> 0,\\nwhere c+ is equal to c if c > 0, and zero otherwise. Show that the minimizer of the risk\\nℓ(g) = ELoss(Y,g(X)) satisfies\\nP[Y <g∗(x) |X = x] = β\\nα+ β.\\nIn other words, g∗(x) is the β/(α+ β) quantile of Y, conditional on X = x.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 77, 'page_label': '60'}, page_content='60 Exercises\\n2. Show that, for the squared-error loss, the approximation error ℓ(gG) −ℓ(g∗) in (2.16), is\\nequal to E(gG(X) −g∗(X))2. [Hint: expand ℓ(gG) = E(Y −g∗(X) + g∗(X) −gG(X))2.]\\n3. Suppose Gis the class of linear functions. A linear function evaluated at a featurex can\\nbe described as g(x) = β⊤x for some parameter vector βof appropriate dimension. Denote\\ngG(x) = x⊤βGand gG\\nτ(x) = x⊤bβ. Show that\\nE\\n\\x10\\ngG\\nτ(X) −g∗(X)\\n\\x112\\n= E\\n\\x10\\nX⊤bβ−X⊤βG\\x112\\n+ E\\n\\x10\\nX⊤βG−g∗(X)\\n\\x112\\n.\\nHence, deduce that the statistical error in (2.16) is ℓ(gG\\nτ) −ℓ(gG) = E(gG\\nτ(X) −gG(X))2.\\n4. Show that formula (2.24) holds for the 0–1 loss with 0–1 response.\\n5. Let X be an n-dimensional normal random vector with mean vector µand covariance\\nmatrix Σ, where the determinant ofΣ is non-zero. Show thatX has joint probability density\\nfX(x) = 1√(2π)n |Σ|\\ne−1\\n2 (x−µ)⊤Σ−1(x−µ), x ∈Rn.\\n6. Let bβ = A+y. Using the defining properties of the pseudo-inverse, show that for any☞360\\nβ∈Rp,\\n∥Abβ−y∥⩽∥Aβ−y∥.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 77, 'page_label': '60'}, page_content='β∈Rp,\\n∥Abβ−y∥⩽∥Aβ−y∥.\\n7. Suppose that in the polynomial regression Example 2.1 we select the linear class of\\nfunctions Gp with p ⩾4. Then, g∗∈Gp and the approximation error is zero, because\\ngGp (x) = g∗(x) = x⊤β, where β = [10,−140,400,−250,0,..., 0]⊤ ∈Rp. Use the tower\\nproperty to show that the learner gτ(x) = x⊤bβwith bβ = X+y, assuming rank( X) ⩾4, is☞431\\nunbiasedunbiased :\\nEgT(x) = g∗(x).\\n8. (Exercise 7 continued.) Observe that the learner gT can be written as a linear combina-\\ntion of the response variable:gT(x) = x⊤X+Y. Prove that for any learner of the formx⊤Ay,\\nwhere A ∈Rp×n is some matrix and that satisfies EX[x⊤AY] = g∗(x), we have\\nVarX[x⊤X+Y] ⩽VarX[x⊤AY],\\nwhere the equality is achieved for A = X+. This is called the Gauss–Markov inequalityGauss–Markov\\ninequality\\n.\\nHence, using the Gauss–Markov inequality deduce that for the unconditional variance:\\nVar gT(x) ⩽Var[x⊤AY].\\nDeduce that A = X+ also minimizes the expected generalization risk.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 77, 'page_label': '60'}, page_content='Var gT(x) ⩽Var[x⊤AY].\\nDeduce that A = X+ also minimizes the expected generalization risk.\\n9. Consider again the polynomial regression Example 2.1. Use the fact thatEX bβ= X+h∗(u),\\nwhere h∗(u) = E[Y |U = u] = [h∗(u1),..., h∗(un)]⊤, to show that the expected in-sample\\nrisk is:\\nEX ℓin(gT) = ℓ∗+ ∥h∗(u)∥2 −∥XX+h∗(u)∥2\\nn + ℓ∗p\\nn .\\nAlso, use Theorem C.2 to show that the expected statistical error is:☞430\\nEX (bβ−β)⊤Hp(bβ−β) = ℓ∗tr(X+(X+)⊤Hp) + (X+h∗(u) −β)⊤Hp(X+h∗(u) −β).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 78, 'page_label': '61'}, page_content='Statistical Learning 61\\n10. Consider the setting of the polynomial regression in Example 2.2. Use Theorem C.19\\nto prove that ☞ 449\\n√n (bβn −βp)\\nd\\n−→N\\n\\x10\\n0, ℓ∗H−1\\np + H−1\\np MpH−1\\np\\n\\x11\\n, (2.53)\\nwhere Mp := E[XX⊤(g∗(X) −gGp (X))2] is the matrix with (i, j)-th entry:\\nZ 1\\n0\\nui+ j−2(hHp (u) −h∗(u))2 du,\\nand H−1\\np is the p ×p inverse Hilbert matrix inverse Hilbert\\nmatrix\\nwith (i, j)-th entry:\\n(−1)i+ j(i + j −1)\\n p + i −1\\np −j\\n! p + j −1\\np −i\\n! i + j −2\\ni −1\\n!2\\n.\\nObserve that Mp = 0 for p ⩾4, so that the matrix Mp term is due to choosing a restrictive\\nclass Gp that does not contain the true prediction function.\\n11. In Example 2.2 we saw that the statistical error can be expressed (see (2.20)) as\\nZ 1\\n0\\n\\x10\\n[1,..., up−1](bβ−βp)\\n\\x112\\ndu = (bβ−βp)⊤Hp(bβ−βp).\\nBy Exercise 10 the random vector Zn := √n(bβn −βp) has asymptotically a multivariate\\nnormal distribution with mean vector 0 and covariance matrix V := ℓ∗H−1\\np + H−1\\np MpH−1\\np .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 78, 'page_label': '61'}, page_content='normal distribution with mean vector 0 and covariance matrix V := ℓ∗H−1\\np + H−1\\np MpH−1\\np .\\nUse Theorem C.2 to show that the expected statistical error is asymptotically ☞ 430\\nE(bβ−βp)⊤Hp(bβ−βp) ≃ℓ∗p\\nn +\\ntr(MpH−1\\np )\\nn , n →∞. (2.54)\\nPlot this large-sample approximation of the expected statistical error and compare it with\\nthe outcome of the statistical error.\\nWe note a subtle technical detail: In general, convergence in distribution does not imply\\nconvergence in Lp-norm (see Example C.6), and so here we have implicitly assumed that ☞ 442\\n∥Zn∥\\nd\\n−→Dist.⇒∥Zn∥\\nL2\\n−→constant := limn↑∞E∥Zn∥.\\n12. Consider again Example 2.2. The result in (2.53) suggests that Ebβ →βp as n →∞,\\nwhere βp is the solution in the class Gp given in (2.18). Thus, the large-sample approxim-\\nation of the pointwise bias of the learner g\\nGp\\nT (x) = x⊤bβat x = [1,..., up−1]⊤is\\nEg\\nGp\\nT (x) −g∗(x) ≃[1,..., up−1] βp −[1,u,u2,u3] β∗, n →∞.\\nUse Python to reproduce Figure 2.17, which shows the (large-sample) pointwise squared'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 78, 'page_label': '61'}, page_content='Use Python to reproduce Figure 2.17, which shows the (large-sample) pointwise squared\\nbias of the learner for p ∈{1,2,3}. Note how the bias is larger near the endpoints u = 0\\nand u = 1. Explain why the areas under the curves correspond to the approximation errors.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 79, 'page_label': '62'}, page_content='62 Exercises\\n0 0.2 0.4 0.6 0.8 1\\n0\\n50\\n100\\n150\\n200\\n250\\nFigure 2.17: The large-sample pointwise squared bias of the learner for p = 1,2,3. The\\nbias is zero for p ⩾4.\\n13. For our running Example 2.2 we can use (2.53) to derive a large-sample approximation\\nof the pointwise variance of the learner gT(x) = x⊤bβn. In particular, show that for large n\\nVar gT(x) ≃\\nℓ∗x⊤H−1\\np x\\nn +\\nx⊤H−1\\np MpH−1\\np x\\nn , n →∞. (2.55)\\nFigure 2.18 shows this (large-sample) variance of the learner for di fferent values of the\\npredictor u and model index p. Observe that the variance ultimately increases in p and that\\nit is smaller at u = 1/2 than closer to the endpoints u = 0 or u = 1. Since the bias is also\\n9\\n7\\n5\\n1\\n2\\n0.05\\n3\\n3\\n4\\n0.5 10.95\\nFigure 2.18: The pointwise variance of the learner for various pairs of p and u.\\nlarger near the endpoints, we deduce that the pointwise mean squared error (2.21) is larger\\nnear the endpoints of the interval [0 ,1] than near its middle. In other words, the error is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 79, 'page_label': '62'}, page_content='near the endpoints of the interval [0 ,1] than near its middle. In other words, the error is\\nmuch smaller in the center of the data cloud than near its periphery.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 80, 'page_label': '63'}, page_content='Statistical Learning 63\\n14. Let h : x 7→Rbe a convex function and let X be a random variable. Use the subgradi-\\nent definition of convexity to prove Jensen’s inequality: ☞ 403\\nJensen’s\\ninequalityEh(X) ⩾h(EX). (2.56)\\n15. Using Jensen’s inequality, show that the Kullback–Leibler divergence between prob-\\nability densities f and g is always positive; that is,\\nEln f (X)\\ng(X) ⩾0,\\nwhere X ∼f .\\n16. The purpose of this exercise is to prove the followingVapnik–Chernovenkis bound Vapnik–\\nChernovenkis\\nbound\\n: for\\nany finite class G(containing only a finite number |G|of possible functions) and a general\\nbounded loss function, l ⩽Loss ⩽u, the expected statistical error is bounded from above\\naccording to:\\nEℓ(gG\\nTn\\n) −ℓ(gG) ⩽(u −l) √2 ln(2|G|)√n . (2.57)\\nNote how this bound conveniently does not depend on the distribution of the training set\\nTn (which is typically unknown), but only on the complexity (i.e., cardinality) of the class\\nG. We can break up the proof of (2.57) into the following four parts:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 80, 'page_label': '63'}, page_content='G. We can break up the proof of (2.57) into the following four parts:\\n(a) For a general function class G, training set T, risk function ℓ, and training loss ℓT,\\nwe have, by definition, ℓ(gG) ⩽ℓ(g) and ℓT(gG\\nT) ⩽ℓT(g) for all g ∈G. Show that\\nℓ(gG\\nT) −ℓ(gG) ⩽sup\\ng∈G\\n|ℓT(g) −ℓ(g)|+ ℓT(gG) −ℓ(gG),\\nwhere we used the notation sup (supremum) for the least upper bound. Since\\nEℓT(g) = Eℓ(g), we obtain, after taking expectations on both sides of the inequal-\\nity above:\\nEℓ(gG\\nT) −ℓ(gG) ⩽Esup\\ng∈G\\n|ℓT(g) −ℓ(g)|.\\n(b) If X is a zero-mean random variable taking values in the interval [ l,u], then the fol-\\nlowing Hoeffding’s inequality Hoeffding’s\\ninequality\\nstates that the moment generating function satisfies\\nEetX ⩽exp\\n t2(u −l)2\\n8\\n!\\n, t ∈R. (2.58)\\nProve this result by using the fact that the line segment joining points ( l,exp(tl)) and\\n(u,exp(tu)) bounds the convex function x 7→exp(tx) for x ∈[l,u]; that is:\\netx ⩽etl u −x\\nu −l + etu x −l\\nu −l, x ∈[l,u].'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 80, 'page_label': '63'}, page_content='etx ⩽etl u −x\\nu −l + etu x −l\\nu −l, x ∈[l,u].\\n(c) Let Z1,..., Zn be (possibly dependent and non-identically distributed) zero-mean ran-\\ndom variables with moment generating functions that satisfyEexp(tZk) ⩽exp(t2η2/2)\\nfor all k and some parameter η. Use Jensen’s inequality (2.56) to prove that for any ☞ 427'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 81, 'page_label': '64'}, page_content='64 Exercises\\nt >0,\\nEmax\\nk\\nZk = 1\\nt Eln max\\nk\\netZk ⩽1\\nt ln n + tη2\\n2 .\\nFrom this derive that\\nEmax\\nk\\nZk ⩽η\\n√\\n2 lnn.\\nFinally, show that this last inequality implies that\\nEmax\\nk\\n|Zk|⩽η\\np\\n2 ln(2n). (2.59)\\n(d) Returning to the objective of this exercise, denote the elements of Gby g1,..., g|G|,\\nand let Zk = ℓTn (gk) −ℓ(gk). By part (a) it is sufficient to bound Emaxk |Zk|. Show that\\nthe {Zk}satisfy the conditions of (c) with η = (u −l)/√n. For this you will need to\\napply part (b) to the random variable Loss(g(X),Y) −ℓ(g), where (X,Y) is a generic\\ndata point. Now complete the proof of (2.57).\\n17. Consider the problem in Exercise 16a above. Show that\\n|ℓT(gG\\nT) −ℓ(gG)|⩽2 sup\\ng∈G\\n|ℓT(g) −ℓ(g)|+ ℓT(gG) −ℓ(gG).\\nFrom this, conclude:\\nE|ℓT(gG\\nT) −ℓ(gG)|⩽2Esup\\ng∈G\\n|ℓT(g) −ℓ(g)|.\\nThe last bound allows us to assess how close the training loss ℓT(gG\\nT) is to the optimal risk\\nℓ(gG) within class G.\\n18. Show that for the normal linear model Y ∼N(Xβ,σ2In), the maximum likelihood es-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 81, 'page_label': '64'}, page_content='18. Show that for the normal linear model Y ∼N(Xβ,σ2In), the maximum likelihood es-\\ntimator of σ2 is identical to the method of moments estimator (2.37).\\n19. Let X ∼Gamma(α,λ). Show that the pdf of Z = 1/X is equal to\\nλα(z)−α−1e−λ(z)−1\\nΓ(α) , z >0.\\n20. Consider the sequence w0,w1,... , where w0 = g(θ) is a non-degenerate initial guess\\nand wt(θ) ∝wt−1(θ)g(τ|θ), t >1. We assume thatg(τ|θ) is not the constant function (with\\nrespect to θ) and that the maximum likelihood value\\ng(τ|bθ) = max\\nθ\\ng(τ|θ) <∞\\nexists (is bounded). Let\\nlt :=\\nZ\\ng(τ|θ)wt(θ) dθ.\\nShow that {lt}is a strictly increasing and bounded sequence. Hence, conclude that its limit\\nis g(τ|bθ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 82, 'page_label': '65'}, page_content='Statistical Learning 65\\n21. Consider the Bayesian model for τ= {x1,..., xn}with likelihood g(τ|µ) such that\\n(X1,..., Xn |µ) ∼iid N(µ,1) and prior pdf g(µ) such that µ∼N(ν,1) for some hyperpara-\\nmeter ν. Define a sequence of densities wt(µ),t ⩾2 via wt(µ) ∝wt−1(µ) g(τ|µ), start-\\ning with w1(µ) = g(µ). Let at and bt denote the mean and precision 4 of µ under the\\nposterior gt(µ|τ) ∝g(τ|µ)wt(µ). Show that gt(µ|τ) is a normal density with precision\\nbt = bt−1 + n, b0 = 1 and mean at = (1 −γt)at−1 + γt xn, a0 = ν, where γt := n/(bt−1 + n).\\nHence, deduce that gt(µ|τ) converges to a degenerate density with a point-mass at xn.\\n22. Consider again Example 2.8, where we have a normal model with improper prior\\ng(θ) = g(µ,σ2) ∝1/σ2. Show that the prior predictive pdf is an improper densityg(x) ∝1,\\nbut that the posterior predictive density is\\ng(x |τ) ∝\\n \\n1 + (x −xn)2\\n(n + 1)S 2\\nn\\n!−n/2\\n.\\nDeduce that X−xn\\nS n\\n√(n+1)/(n−1) ∼tn−1.\\n23. Assuming that X1,..., Xn\\niid\\n∼f , show that (2.48) holds and that ℓ∗'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 82, 'page_label': '65'}, page_content='S n\\n√(n+1)/(n−1) ∼tn−1.\\n23. Assuming that X1,..., Xn\\niid\\n∼f , show that (2.48) holds and that ℓ∗\\nn = −n Eln f (X).\\n24. Suppose that τ = {x1,..., xn}are observations of iid continuous and strictly positive\\nrandom variables, and that there are two possible models for their pdf. The first model\\np = 1 is\\ng(x |θ,p = 1) = θexp (−θx)\\nand the second p = 2 is\\ng(x |θ,p = 2) =\\n 2θ\\nπ\\n!1/2\\nexp\\n \\n−θx2\\n2\\n!\\n.\\nFor both models, assume that the prior for θis a gamma density\\ng(θ) = bt\\nΓ(t)θt−1 exp (−bθ) ,\\nwith the same hyperparameters b and t. Find a formula for the Bayes factor, g(τ|p =\\n1)/g(τ|p = 2), for comparing these models.\\n25. Suppose that we have a total of m possible models with prior probabilities g(p),p =\\n1,..., m. Show that the posterior probability of model g(p |τ) can be expressed in terms of\\nall the p(p −1) Bayes factors:\\ng(p = i |τ) =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 +\\nX\\nj,i\\ng(p = j)\\ng(p = i) Bj |i\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n−1\\n.\\n4The precision is the reciprocal of the variance.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 83, 'page_label': '66'}, page_content='66 Exercises\\n26. Given the data τ = {x1,..., xn}, suppose that we use the likelihood ( X |θ) ∼N(µ,σ2)\\nwith parameter θ= (µ,σ2)⊤and wish to compare the following two nested models.\\n(a) Model p = 1, where σ2 = σ2\\n0 is known and this is incorporated via the prior\\ng(θ|p = 1) = g(µ|σ2,p = 1) g(σ2 |p = 1) = 1√\\n2πσ\\ne−(µ−x0)2\\n2σ2 ×δ(σ2 −σ2\\n0).\\n(b) Model p = 2, where both mean and variance are unknown with prior\\ng(θ|p = 2) = g(µ|σ2) g(σ2) = 1√\\n2πσ\\ne−(µ−x0)2\\n2σ2 ×bt(σ2)−t−1e−b/σ2\\nΓ(t) .\\nShow that the prior g(θ|p = 1) can be viewed as the limit of the prior g(θ|p = 2) when\\nt →∞ and b = tσ2\\n0. Hence, conclude that\\ng(τ|p = 1) = lim\\nt→∞\\nb=tσ2\\n0\\ng(τ|p = 2)\\nand use this result to calculateB1 |2. Check that the formula forB1 |2 agrees with the Savage–\\nDickey density ratio:\\ng(τ|p = 1)\\ng(τ|p = 2) = g(σ2 = σ2\\n0 |τ)\\ng(σ2 = σ2\\n0) ,\\nwhere g(σ2 |τ) and g(σ2) are the posterior and prior, respectively, under model p = 2.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 84, 'page_label': '67'}, page_content='CHAPTER 3\\nMONTE CARLO METHODS\\nMany algorithms in machine learning and data science make use of Monte Carlo\\ntechniques. This chapter gives an introduction to the three main uses of Monte Carlo\\nsimulation: to (1) simulate random objects and processes in order to observe their beha-\\nvior, (2) estimate numerical quantities by repeated sampling, and (3) solve complicated\\noptimization problems through randomized algorithms.\\n3.1 Introduction\\nBriefly put, Monte Carlo simulation Monte Carlo\\nsimulation\\nis the generation of random data by means of a com-\\nputer. These data could arise from simple models, such as those described in Chapter 2,\\nor from very complicated models describing real-life systems, such as the positions of\\nvehicles on a complex road network, or the evolution of security prices in the stock mar-\\nket. In many cases, Monte Carlo simulation simply involves random sampling from certain\\nprobability distributions. The idea is to repeat the random experiment that is described by'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 84, 'page_label': '67'}, page_content='probability distributions. The idea is to repeat the random experiment that is described by\\nthe model many times to obtain a large quantity of data that can be used to answer questions\\nabout the model. The three main uses of Monte Carlo simulation are:\\nSampling. Here the objective is to gather information about a random object by observing\\nmany realizations of it. For instance, this could be a random process that mimics the\\nbehavior of some real-life system such as a production line or telecommunications\\nnetwork. Another usage is found in Bayesian statistics, where Markov chains are\\noften used to sample from a posterior distribution. ☞ 49\\nEstimation. In this case the emphasis is on estimating certain numerical quantities related\\nto a simulation model. An example is the evaluation of multidimensional integrals\\nvia Monte Carlo techniques. This is achieved by writing the integral as the expecta-\\ntion of a random variable, which is then approximated by the sample mean. Appeal-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 84, 'page_label': '67'}, page_content='tion of a random variable, which is then approximated by the sample mean. Appeal-\\ning to the Law of Large Numbers guarantees that this approximation will eventually ☞ 446\\nconverge when the sample size becomes large.\\nOptimization. Monte Carlo simulation is a powerful tool for the optimization of complic-\\nated objective functions. In many applications these functions are deterministic and\\n67'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 85, 'page_label': '68'}, page_content='68 Monte Carlo Sampling\\nrandomness is introduced artificially in order to more efficiently search the domain of\\nthe objective function. Monte Carlo techniques are also used to optimize noisy func-\\ntions, where the function itself is random; for example, when the objective function\\nis the output of a Monte Carlo simulation.\\nThe Monte Carlo method dramatically changed the way in which statistics is used in\\ntoday’s analysis of data. The ever-increasing complexity of data requires radically different\\nstatistical models and analysis techniques from those that were used 20 to 100 years ago.\\nBy using Monte Carlo techniques, the data analyst is no longer restricted to using basic\\n(and often inappropriate) models to describe data. Now, any probabilistic model that can\\nbe simulated on a computer can serve as the basis for statistical analysis. This Monte Carlo\\nrevolution has had an impact on both Bayesian and frequentist statistics. In particular, in'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 85, 'page_label': '68'}, page_content='revolution has had an impact on both Bayesian and frequentist statistics. In particular, in\\nfrequentist statistics, Monte Carlo methods are often referred to as resampling techniques.\\nAn important example is the well-known bootstrap method [37], where statistical quantit-\\nies such as confidence intervals and P-values for statistical tests can simply be determined\\nby simulation without the need of a sophisticated analysis of the underlying probability\\ndistributions; see, for example, [69] for basic applications. The impact on Bayesian statist-\\nics has been even more profound, through the use of Markov chain Monte Carlo (MCMC)\\ntechniques [87, 48]. MCMC samplers construct a Markov process which converges in dis-\\ntribution to a desired (often high-dimensional) density. This convergence in distribution\\njustifies using a finite run of the Markov process as an approximate random realization\\nfrom the target density. The MCMC approach has rapidly gained popularity as a versat-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 85, 'page_label': '68'}, page_content='from the target density. The MCMC approach has rapidly gained popularity as a versat-\\nile heuristic approximation, partly due to its simple computer implementation and inbuilt\\nmechanism to tradeoff between computational cost and accuracy; namely, the longer one\\nruns the Markov process, the better the approximation. Nowadays, MCMC methods are\\nindispensable for analyzing posterior distributions for inference and model selection; see\\nalso [50, 99].\\nThe following three sections elaborate on these three uses of Monte Carlo simulation\\nin turn.\\n3.2 Monte Carlo Sampling\\nIn this section we describe a variety of Monte Carlo sampling methods, from the building\\nblock of simulating uniform random numbers to MCMC samplers.\\n3.2.1 Generating Random Numbers\\nAt the heart of any Monte Carlo method is a random number generator: a procedure thatrandom number\\ngenerator produces a stream of uniform random numbers on the interval (0,1). Since such numbers'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 85, 'page_label': '68'}, page_content='generator produces a stream of uniform random numbers on the interval (0,1). Since such numbers\\nare usually produced via deterministic algorithms, they are not truly random. However, for\\nmost applications all that is required is that such pseudo-random numbers are statistically\\nindistinguishable from genuine random numbers U1,U2,... that are uniformly distributed\\non the interval (0,1) and are independent of each other; we write U1,U2,... ∼iid U(0,1).\\nFor example, in Python the rand method of the numpy.random module is widely used for\\nthis purpose.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 86, 'page_label': '69'}, page_content='Monte Carlo Methods 69\\nMost random number generators at present are based on linear recurrence relations.\\nOne of the most important random number generators is the multiple-recursive generator multiple-\\nrecursive\\ngenerator(MRG) of order k, which generates a sequence of integers Xk,Xk+1,... via the linear recur-\\nrence\\nXt = (a1Xt−1 + ··· + akXt−k) mod m, t = k,k + 1,... (3.1)\\nfor some modulus m and multipliers {ai,i = 1,..., k}. Here “mod” refers to the modulo op- modulus\\nmultiplierseration: n mod m is the remainder when n is divided by m. The recurrence is initialized by\\nspecifying k “seeds”, X0,..., Xk−1. To yield fast algorithms, all but a few of the multipliers\\nshould be 0. When m is a large integer, one can obtain a stream of pseudo-random numbers\\nUk,Uk+1,... between 0 and 1 from the sequence Xk,Xk+1,... , simply by setting Ut = Xt/m.\\nIt is also possible to set a small modulus, in particular m = 2. The output function for such'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 86, 'page_label': '69'}, page_content='It is also possible to set a small modulus, in particular m = 2. The output function for such\\nmodulo 2 generators is then typically of the form modulo 2\\ngenerators\\nUt =\\nwX\\ni=1\\nXtw+i−12−i\\nfor some w ⩽k, e.g., w = 32 or 64. Examples of modulo 2 generators are thefeedback shift\\nregister generators, the most popular of which are theMersenne twisters; see, for example, feedback shift\\nregister\\nMersenne\\ntwisters\\n[79] and [83]. MRGs with excellent statistical properties can be implemented e fficiently\\nby combining several simpler MRGs and carefully choosing their respective moduli and\\nmultipliers. One of the most successful is L’Ecuyer’sMRG32k3agenerator; see [77]. From\\nnow on, we assume that the reader has a sound random number generator available.\\n3.2.2 Simulating Random Variables\\nSimulating a random variable X from an arbitrary (that is, not necessarily uniform) distri-\\nbution invariably involves the following two steps:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 86, 'page_label': '69'}, page_content='bution invariably involves the following two steps:\\n1. Simulate uniform random numbers U1,..., Uk on (0,1) for some k = 1,2,... .\\n2. Return X = g(U1,..., Uk), where g is some real-valued function.\\nThe construction of suitable functions g is as much of an art as a science. Many\\nsimulation methods may be found, for example, in [71] and the accompanying website\\nwww.montecarlohandbook.org. Two of the most useful general procedures for gen-\\nerating random variables are the inverse-transform method and the acceptance–rejection\\nmethod. Before we discuss these, we show one possible way to simulate standard normal\\nrandom variables. In Python we can generate standard normal random variables via the\\nrandn method of the numpy.random module.\\nExample 3.1 (Simulating Standard Normal Random Variables) If X and Y are in-\\ndependent standard normally distributed random variables (that is, X,Y ∼iid N(0,1)), then\\ntheir joint pdf is\\nf (x,y) = 1\\n2πe−1\\n2 (x2+y2), (x,y) ∈R2,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 86, 'page_label': '69'}, page_content='their joint pdf is\\nf (x,y) = 1\\n2πe−1\\n2 (x2+y2), (x,y) ∈R2,\\nwhich is a radially symmetric function. In Example C.2 we see that, in polar coordin- ☞ 433\\nates, the angle Θ that the random vector [X,Y]⊤makes with the positive x-axis is U(0,2π)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 87, 'page_label': '70'}, page_content='70 Monte Carlo Sampling\\ndistributed (as would be expected from the radial symmetry) and the radius R has pdf\\nfR(r) = r e−r2/2,r >0. Moreover, R and Θ are independent. We will see shortly, in Ex-\\nample 3.4, that R has the same distribution as\\n√\\n−2 lnU with U ∼U(0,1). So, to sim-☞72\\nulate X,Y ∼iid N(0,1), the idea is to first simulate R and Θ independently and then return\\nX = R cos(Θ) and Y = R sin(Θ) as a pair of independent standard normal random variables.\\nThis leads to the Box–Muller approach for generating standard normal random variables.\\nAlgorithm 3.2.1: Normal Random Variable Simulation: Box–Muller Approach\\noutput: Independent standard normal random variables X and Y.\\n1 Simulate two independent random variables, U1 and U2, from U(0,1).\\n2 X ←(−2 lnU1)1/2 cos(2πU2)\\n3 Y ←(−2 lnU1)1/2 sin(2πU2)\\n4 return X,Y\\nOnce a standard normal number generator is available, simulation from any n-\\ndimensional normal distribution N(µ,Σ) is relatively straightforward. The first step is to'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 87, 'page_label': '70'}, page_content='dimensional normal distribution N(µ,Σ) is relatively straightforward. The first step is to\\nfind an n ×n matrix B that decomposes Σ into the matrix product BB⊤. In fact there exist\\nmany such decompositions. One of the more important ones is theCholesky decomposition,Cholesky\\ndecomposition which is a special case of the LU decomposition; see Section A.6.1 for more information\\n☞368 on such decompositions. In Python, the functioncholesky of numpy.linalg can be used\\nto produce such a matrix B.\\nOnce the Cholesky factorization is determined, it is easy to simulate X ∼N(µ,Σ) as,\\nby definition, it is the a ffine transformation µ+ BZ of an n-dimensional standard normal\\nrandom vector.\\nAlgorithm 3.2.2: Normal Random Vector Simulation\\ninput: µ,Σ\\noutput: X ∼N(µ,Σ)\\n1 Determine the Cholesky factorization Σ = BB⊤.\\n2 Simulate Z = [Z1,..., Zn]⊤by drawing Z1,..., Zn ∼iid N(0,1).\\n3 X ←µ+ BZ\\n4 return X\\nExample 3.2 (Simulating from a Bivariate Normal Distribution) The Python code'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 87, 'page_label': '70'}, page_content='3 X ←µ+ BZ\\n4 return X\\nExample 3.2 (Simulating from a Bivariate Normal Distribution) The Python code\\nbelow draws N = 1000 iid samples from the two bivariate ( n = 2) normal pdfs in Fig-\\nure 2.13. The resulting point clouds are given in Figure 3.1.☞46\\nbvnormal.py\\nimport numpy as np\\nfrom numpy.random import randn\\nimport matplotlib.pyplot as plt\\nN = 1000\\nr = 0.0 #change to 0.8 for other plot\\nSigma = np.array([[1, r], [r, 1]])'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 88, 'page_label': '71'}, page_content='Monte Carlo Methods 71\\nB = np.linalg.cholesky(Sigma)\\nx = B @ randn(2,N)\\nplt.scatter([x[0,:]],[x[1,:]], alpha =0.4, s = 4)\\n2\\n 0\\n 2\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n2\\n 0\\n 2\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\nFigure 3.1: 1000 realizations of bivariate normal distributions with means zero, variances\\n1, and correlation coefficients 0 (left) and 0.8 (right).\\nIn some cases, the covariance matrix Σ has special structure which can be exploited to\\ncreate even faster generation algorithms, as illustrated in the following example.\\nExample 3.3 (Simulating Normal Vectors in O(n2) Time) Suppose that the random\\nvector X = [X1,..., Xn]⊤represents the values at times t0 + kδ, k = 0,..., n −1 of a zero-\\nmean Gaussian process(X(t),t ⩾0) that isweakly stationary, meaning thatCov(X(s),X(t)) ☞ 238\\ndepends only ont−s. Then clearly the covariance matrix ofX, say An, is a symmetric Toep-\\nlitz matrix. Suppose for simplicity that Var X(t) = 1. Then the covariance matrix is in fact ☞ 379\\na correlation matrix, and will have the following structure:\\nAn :='),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 88, 'page_label': '71'}, page_content='a correlation matrix, and will have the following structure:\\nAn :=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 a1 ... an−2 an−1\\na1 1 ... an−2\\n... ... ... ... ...\\nan−2\\n... ... a1\\nan−1 an−2 ··· a1 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nUsing the Levinson–Durbin algorithm we can compute a lower diagonal matrix Ln and\\na diagonal matrix Dn in O(n2) time such that Ln An L⊤\\nn = Dn; see Theorem A.14. If we ☞ 383\\nsimulate Zn ∼N(0,In), then the solution X of the linear system:\\nLn X = D1/2\\nn Zn\\nhas the desired distribution N(0,An). The linear system is solved inO(n2) time via forward\\nsubstitution.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 89, 'page_label': '72'}, page_content='72 Monte Carlo Sampling\\n3.2.2.1 Inverse-Transform Method\\nLet X be a random variable with cumulative distribution function (cdf) F. Let F−1 denote\\nthe inverse1 of F and U ∼U(0,1). Then,\\nP[F−1(U) ⩽x] = P[U ⩽F(x)] = F(x). (3.2)\\nThis leads to the following method to simulate a random variable X with cdf F:\\nAlgorithm 3.2.3: Inverse-Transform Method\\ninput: Cumulative distribution function F.\\noutput: Random variable X distributed according to F.\\n1 Generate U from U(0,1).\\n2 X ←F−1(U)\\n3 return X\\nThe inverse-transform method works both for continuous and discrete distribu-\\ntions. After importing numpy as np, simulating numbers 0 ,..., k −1 according to\\nprobabilities p0,..., pk−1 can be done via np.min(np.where(np.cumsum(p) >\\nnp.random.rand())), where pis the vector of the probabilities.\\nExample 3.4 (Example 3.1 (cont.)) One remaining issue in Example 3.1 was how to\\nsimulate the radius R when we only know its density fR(r) = r e−r2/2,r >0. We can use the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 89, 'page_label': '72'}, page_content='simulate the radius R when we only know its density fR(r) = r e−r2/2,r >0. We can use the\\ninverse-transform method for this, but first we need to determine its cdf. The cdf of R is,\\nby integration of the pdf,\\nFR(r) = 1 −e−1\\n2 r2\\n, r >0,\\nand its inverse is found by solving u = FR(r) in terms of r, giving\\nF−1\\nR (u) =\\np\\n−2 ln(1 −u), u ∈(0,1).\\nThus R has the same distribution as √−2 ln(1 −U), with U ∼U(0,1). Since 1−U also has\\na U(0,1) distribution, R has also the same distribution as\\n√\\n−2 lnU.\\n3.2.2.2 Acceptance–Rejection Method\\nThe acceptance–rejection method is used to sample from a “di fficult” probability density\\nfunction (pdf) f (x) by generating instead from an “easy” pdf g(x) satisfying f (x) ⩽C g(x)\\nfor some constant C ⩾1 (for example, via the inverse-transform method), and then ac-\\ncepting or rejecting the drawn sample with a certain probability. Algorithm 3.2.4 gives the\\npseudo-code.\\nThe idea of the algorithm is to generate uniformly a point (X,Y) under the graph of the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 89, 'page_label': '72'}, page_content='The idea of the algorithm is to generate uniformly a point (X,Y) under the graph of the\\nfunction Cg, by first drawing X ∼g and then Y ∼U(0,Cg(X)). If this point lies under the\\ngraph of f , then we accept X as a sample from f ; otherwise, we try again. The e fficiency\\nof the acceptance–rejection method is usually expressed in terms of the probability of\\nacceptance, which is 1/C.\\n1Every cdf has a unique inverse function defined by F−1(u) = inf{x : F(x) ⩾u}. If, for each u, the\\nequation F(x) = u has a unique solution x, this definition coincides with the usual interpretation of the\\ninverse function.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 90, 'page_label': '73'}, page_content='Monte Carlo Methods 73\\nAlgorithm 3.2.4: Acceptance–Rejection Method\\ninput: Pdf g and constant C such that Cg(x) ⩾f (x) for all x.\\noutput: Random variable X distributed according to pdf f .\\n1 found←false\\n2 while not founddo\\n3 Generate X from g.\\n4 Generate U from U(0,1) independently of X.\\n5 Y ←UCg(X)\\n6 if Y ⩽f (X) then found←true\\n7 return X\\nExample 3.5 (Simulating Gamma Random Variables) Simulating random variables\\nfrom a Gamma(α,λ) distribution is generally done via the acceptance–rejection method.\\nConsider, for example, the Gamma distribution with α= 1.3 and λ= 5.6. Its pdf, ☞ 425\\nf (x) = λαxα−1e−λx\\nΓ(α) , x ⩾0,\\nwhere Γ is the gamma function Γ(α) :=\\nR ∞\\n0 e−x xα−1 dx, α> 0, is depicted by the blue solid\\ncurve in Figure 3.2.\\n0 0.5 1 1.5 2\\n0\\n1\\n2\\n3\\n4\\n5\\nFigure 3.2: The pdf g of the Exp(4) distribution multiplied by C = 1.2 dominates the pdf f\\nof the Gamma(1.3,5.6) distribution.\\nThis pdf happens to lie completely under the graph of Cg(x), where C = 1.2 and'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 90, 'page_label': '73'}, page_content='This pdf happens to lie completely under the graph of Cg(x), where C = 1.2 and\\ng(x) = 4 exp(−4x),x ⩾0 is the pdf of the exponential distribution Exp(4). Hence, we\\ncan simulate from this particular Gamma distribution by accepting or rejecting a sample\\nfrom the Exp(4) distribution according to Step 6 of Algorithm 3.2.4. Simulating from the ☞ 425\\nExp(4) distribution can be done via the inverse-transform method: simulate U ∼U(0,1)\\nand return X = −ln(U)/4. The following Python code implements Algorithm 3.2.4 for this\\nexample.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 91, 'page_label': '74'}, page_content='74 Monte Carlo Sampling\\naccrejgamma.py\\nfrom math import exp, gamma, log\\nfrom numpy.random import rand\\nalpha = 1.3\\nlam = 5.6\\nf = lambda x: lam**alpha * x**(alpha-1) * exp(-lam*x)/gamma(alpha)\\ng = lambda x: 4*exp(-4*x)\\nC = 1.2\\nfound = False\\nwhile not found:\\nx = - log(rand())/4\\nif C*g(x)*rand() <= f(x):\\nfound = True\\nprint (x)\\n3.2.3 Simulating Random Vectors and Processes\\nTechniques for generating random vectors and processes are as diverse as the class of\\nrandom processes themselves; see, for example, [71]. We highlight a few general scenarios.\\nWhen X1,..., Xn are independent random variables with pdfs fi, i = 1,..., n, so that\\ntheir joint pdf is f (x) = f1(x1) ··· fn(xn), the random vector X = [X1,..., Xn]⊤ can be☞429\\nsimply simulated by drawing each component Xi ∼fi individually — for example, via the\\ninverse-transform method or acceptance–rejection.\\nFor dependent components X1,..., Xn, we can, as a consequence of the product rule of\\nprobability, represent the joint pdf f (x) as☞431'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 91, 'page_label': '74'}, page_content='probability, represent the joint pdf f (x) as☞431\\nf (x) = f (x1,..., xn) = f1(x1) f2(x2 |x1) ··· fn(xn |x1,..., xn−1), (3.3)\\nwhere f1(x1) is the marginal pdf of X1 and fk(xk |x1,..., xk−1) is the conditional pdf of Xk\\ngiven X1 = x1,X2 = x2,..., Xk−1 = xk−1. Provided the conditional pdfs are known, one can\\ngenerate X by first generating X1, then, given X1 = x1, generate X2 from f2(x2 |x1), and so\\non, until generating Xn from fn(xn |x1,..., xn−1).\\nThe latter method is particularly applicable for generating Markov chains. Recall from\\nSection C.10 that a Markov chain is a stochastic process {Xt,t = 0,1,2,... }that satisfies☞451\\nMarkov chain the Markov property; meaning that for all t and s the conditional distribution of Xt+s given\\nXu,u ⩽t, is the same as that of Xt+s given only Xt. As a result, each conditional density\\nft(xt |x1,..., xt−1) can be written as a one-step transition density q t(xt |xt−1); that is, the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 91, 'page_label': '74'}, page_content='ft(xt |x1,..., xt−1) can be written as a one-step transition density q t(xt |xt−1); that is, the\\nprobability density to go from state xt−1 to state xt in one step. In many cases of interest\\nthe chain is time-homogeneous, meaning that the transition density qt does not depend on\\nt. Such Markov chains can be generated sequentially, as given in Algorithm 3.2.5.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 92, 'page_label': '75'}, page_content='Monte Carlo Methods 75\\nAlgorithm 3.2.5: Simulate a Markov Chain\\ninput: Number of steps N, initial pdf f0, transition density q.\\n1 Draw X0 from the initial pdf f0.\\n2 for t = 1 to N do\\n3 Draw Xt from the distribution corresponding to the density q(·|Xt−1)\\n4 return X0,..., XN\\nExample 3.6 (Markov Chain Simulation) For time-homogeneous Markov chains\\nwith a discrete state space, we can visualize the one-step transitions by means of a trans-\\nition graph transition\\ngraph\\n, where arrows indicate possible transitions between states and the labels de-\\nscribe the corresponding probabilities. Figure 3.3 shows (on the left) the transition graph\\nof the Markov chain {Xt,t = 0,1,2,... }with state space {1,2,3,4}and one-step transition\\nmatrix\\nP =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 .2 0 .5 0 .3\\n0.5 0 0 .5 0\\n0.3 0 .7 0 0\\n0.1 0 0 0 .9\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\n1 2\\n3 4\\n0.2\\n0.7\\n0.5\\n0.9\\n0.50.3\\n0.50.3\\n0.1\\n0 20 40 60 80 100\\n1\\n2\\n3\\n4\\nFigure 3.3: The transition graph (left) and a typical path (right) of the Markov chain.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 92, 'page_label': '75'}, page_content='1\\n2\\n3\\n4\\nFigure 3.3: The transition graph (left) and a typical path (right) of the Markov chain.\\nIn the same figure (on the right) a typical outcome (path) of the Markov chain is\\nshown. The path was simulated using the Python program below. In this implementation\\nthe Markov chain always starts in state 1. We will revisit Markov chains, and in particular\\nMarkov chains with continuous state spaces, in Section 3.2.5. ☞ 78\\nMCsim.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nn = 101\\nP = np.array([[0, 0.2, 0.5, 0.3],\\n[0.5, 0, 0.5, 0],\\n[0.3, 0.7, 0, 0],\\n[0.1, 0, 0, 0.9]])\\nx = np.array(np.ones(n, dtype= int ))\\nx[0] = 0'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 93, 'page_label': '76'}, page_content=\"76 Monte Carlo Sampling\\nfor t in range (0,n-1):\\nx[t+1] = np. min (np.where(np.cumsum(P[x[t],:]) >\\nnp.random.rand()))\\nx = x + 1 #add 1 to all elements of the vector x\\nplt.plot(np.array( range (0,n)),x, 'o')\\nplt.plot(np.array( range (0,n)),x, '--')\\nplt.show()\\n3.2.4 Resampling\\nThe idea behind resampling is very simple: an iid sample τ := {x1,..., xn}from someresampling\\nunknown cdf F represents our best knowledge of F if we make no further a priori as-\\nsumptions about it. If it is not possible to simulate more samples from F, the best way to\\n“repeat” the experiment is to resample from the original data by drawing from the empir-\\nical cdf Fn; see (1.2). That is, we draw each xi with equal probability and repeat this N☞11\\ntimes, according to Algorithm 3.2.6 below. As we draw here “with replacement”, multiple\\ninstances of the original data points may occur in the resampled data.\\nAlgorithm 3.2.6: Sampling from an Empirical Cdf.\\ninput: Original iid sample x1,..., xn and sample size N.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 93, 'page_label': '76'}, page_content='input: Original iid sample x1,..., xn and sample size N.\\noutput: Iid sample X∗\\n1,..., X∗\\nN from the empirical cdf.\\n1 for t = 1 to N do\\n2 Draw U ∼U(0,1)\\n3 Set I ←⌈nU⌉\\n4 Set X∗\\nt ←xI\\n5 return X∗\\n1,..., X∗\\nN\\nIn Step 3, ⌈nU⌉returns the ceiling of nU; that is, it is the smallest integer larger than\\nor equal to nU. Consequently, I is drawn uniformly at random from the set of indices\\n{1,..., n}.\\nBy sampling from the empirical cdf we can thus (approximately) repeat the experiment\\nthat gave us the original data as many times as we like. This is useful if we want to assess\\nthe properties of certain statistics obtained from the data. For example, suppose that the\\noriginal data τ gave the statistic t(τ). By resampling we can gain information about the\\ndistribution of the corresponding random variable t(T).\\nExample 3.7 (Quotient of Uniforms) Let U1,..., Un,V1,..., Vn be iid U(0,1) random\\nvariables and define Xi = Ui/Vi, i = 1,..., n. Suppose we wish to investigate the distribu-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 93, 'page_label': '76'}, page_content='variables and define Xi = Ui/Vi, i = 1,..., n. Suppose we wish to investigate the distribu-\\ntion of the sample median eX and sample mean X of the (random) data T := {X1,..., Xn}.\\nSince we know the model for Texactly, we can generate a large number, N say, of inde-\\npendent copies of it, and for each of these copies evaluate the sample medians eX1,..., eXN\\nand sample means X1,..., XN. For n = 100 and N = 1000 the empirical cdfs might look\\nlike the left and right curves in Figure 3.4, respectively. Contrary to what you might have\\nexpected, the distributions of the sample median and sample mean do not match at all. The\\nsample median is quite concentrated around 1, whereas the distribution of the sample mean\\nis much more spread out.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 94, 'page_label': '77'}, page_content='Monte Carlo Methods 77\\n0 1 2 3 4 5 6 7\\n0\\n0.5\\n1\\nFigure 3.4: Empirical cdfs of the medians of the resampled data (left curve) and sample\\nmeans (right curve) of the resampled data.\\nInstead of sampling completely new data, we could also reuse the original data by\\nresampling them via Algorithm 3.2.6. This gives independent copies eX∗\\n1,..., eX∗\\nN and\\nX\\n∗\\n1,..., X\\n∗\\nN, for which we can again plot the empirical cdf. The results will be similar\\nto the previous case. In fact, in Figure 3.4 the cdf of the resampled sample medians and\\nsample means are plotted. The corresponding Python code is given below. The essential\\npoint of this example is that resampling of data can greatly add to the understanding of the\\nprobabilistic properties of certain measurements on the data, even if the underlying model\\nis not known. See Exercise 12 for a further investigation of this example. ☞ 117\\nquotunif.py\\nimport numpy as np\\nfrom numpy.random import rand, choice\\nimport matplotlib.pyplot as plt'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 94, 'page_label': '77'}, page_content='import numpy as np\\nfrom numpy.random import rand, choice\\nimport matplotlib.pyplot as plt\\nfrom statsmodels.distributions.empirical_distribution import ECDF\\nn = 100\\nN = 1000\\nx = rand(n)/rand(n) # data\\nmed = np.zeros(N)\\nave = np.zeros(N)\\nfor i in range (0,N):\\ns = choice(x, n, replace=True) # resampled data\\nmed[i] = np.median(s)\\nave[i] = np.mean(s)\\nmed_cdf = ECDF(med)\\nave_cdf = ECDF(ave)\\nplt.plot(med_cdf.x, med_cdf.y)\\nplt.plot(ave_cdf.x, ave_cdf.y)\\nplt.show()'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 95, 'page_label': '78'}, page_content='78 Monte Carlo Sampling\\n3.2.5 Markov Chain Monte Carlo\\nMarkov chain Monte Carlo (MCMC) is a Monte Carlo sampling technique for (approxim-Markov chain\\nMonte Carlo ately) generating samples from an arbitrary distribution — often referred to as the target\\ntarget distribution. The basic idea is to run a Markov chain long enough such that its limiting\\ndistribution is close to the target distribution. Often such a Markov chain is constructed to\\nbe reversible, so that the detailed balance equations (C.43) can be used. Depending on the☞453\\nstarting position of the Markov chain, the initial random variables in the Markov chain may\\nhave a distribution that is significantly different from the target (limiting) distribution. The\\nrandom variables that are generated during this burn-in periodburn-in period are often discarded. The\\nremaining random variables form an approximate and dependent sample from the target\\ndistribution.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 95, 'page_label': '78'}, page_content='remaining random variables form an approximate and dependent sample from the target\\ndistribution.\\nIn the next two sections we discuss two popular MCMC samplers: the Metropolis–\\nHastings sampler and the Gibbs sampler.\\n3.2.5.1 Metropolis–Hastings Sampler\\nThe Metropolis–Hastings sampler [87] is similar to the acceptance–rejection method in☞72\\nthat it simulates a trial state, which is then accepted or rejected according to some random\\nmechanism. Specifically, suppose we wish to sample from a target pdf f (x), where x takes\\nvalues in some d-dimensional set. The aim is to construct a Markov chain{Xt,t = 0,1,... }\\nin such a way that its limiting pdf is f . Suppose the Markov chain is in state x at time t. A\\ntransition of the Markov chain from state x is carried out in two phases. First a proposalproposal\\nstate Y is drawn from a transition density q(·|x). This state is accepted as the new state,\\nwith acceptance probabilityacceptance\\nprobability\\nα(x,y) = min\\n( f (y) q(x |y)\\nf (x) q(y |x),1\\n)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 95, 'page_label': '78'}, page_content='with acceptance probabilityacceptance\\nprobability\\nα(x,y) = min\\n( f (y) q(x |y)\\nf (x) q(y |x),1\\n)\\n, (3.4)\\nor rejected otherwise. In the latter case the chain remains in state x. The algorithm just\\ndescribed can be summarized as follows.\\nAlgorithm 3.2.7: Metropolis–Hastings Sampler\\ninput: Initial state X0, sample size N, target pdf f (x), proposal function q(y |x).\\noutput: X1,..., XN (dependent), approximately distributed according to f (x).\\n1 for t = 0 to N −1 do\\n2 Draw Y ∼q(y |Xt) // draw a proposal\\n3 α←α(Xt,Y) // acceptance probability as in (3.4)\\n4 Draw U ∼U(0,1)\\n5 if U ⩽αthen Xt+1 ←Y\\n6 else Xt+1 ←Xt\\n7 return X1,..., XN\\nThe fact that the limiting distribution of the Metropolis–Hastings Markov chain is equal\\nto the target distribution (under general conditions) is a consequence of the following result.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 96, 'page_label': '79'}, page_content='Monte Carlo Methods 79\\nTheorem 3.1: Local Balance for the Metropolis–Hastings Sampler\\nThe transition density of the Metropolis–Hastings Markov chain satisfies ☞ 453 the de-\\ntailed balance equations.\\nProof: We prove the theorem for the discrete case only. Because a transition of the\\nMetropolis–Hastings Markov chain consists of two steps, the one-step transition probabil-\\nity to go from x to y is not q(y |x) but\\neq(y |x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nq(y |x) α(x,y), if y , x,\\n1 −P\\nz,x q(z |x) α(x,z), if y = x. (3.5)\\nWe thus need to show that\\nf (x)eq(y |x) = f (y)eq(x |y) for all x,y. (3.6)\\nWith the acceptance probability as in (3.4), we need to check (3.6) for three cases:\\n(a) x = y,\\n(b) x , y and f (y)q(x |y) ⩽f (x)q(y |x), and\\n(c) x , y and f (y)q(x |y) > f (x)q(y |x).\\nCase (a) holds trivially. For case (b), α(x,y) = f (y)q(x |y)/( f (x)q(y |x)) and α(y,x) = 1.\\nConsequently,\\neq(y |x) = f (y)q(x |y)/f (x) and eq(x |y) = q(x |y),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 96, 'page_label': '79'}, page_content='Consequently,\\neq(y |x) = f (y)q(x |y)/f (x) and eq(x |y) = q(x |y),\\nso that (3.6) holds. Similarly, for case (c) we have α(x,y) = 1 and α(y,x) = f (x)q(y |x)/\\n( f (y)q(x |y)). It follows that,\\neq(y |x) = q(y |x) and eq(x |y) = f (x)q(y |x)/f (y),\\nso that (3.6) holds again. □\\nThus if the Metropolis–Hastings Markov chain is ergodic, then its limiting pdf is f (x). ☞ 452\\nA fortunate property of the algorithm, which is important in many applications, is that in\\norder to evaluate the acceptance probability α(x,y) in (3.4), one only needs to know the\\ntarget pdf f (x) up to a constant ; that is f (x) = c f (x) for some known function f (x) but\\nunknown constant c.\\nThe efficiency of the algorithm depends of course on the choice of the proposal trans-\\nition density q(y |x). Ideally, we would like q(y |x) to be “close” to the target f (y), irre-\\nspective of x. We discuss two common approaches.\\n1. Choose the proposal transition density q(y |x) independent of x; that is, q(y |x) ='),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 96, 'page_label': '79'}, page_content='1. Choose the proposal transition density q(y |x) independent of x; that is, q(y |x) =\\ng(y) for some pdf g(y). An MCMC sampler of this type is called an independence\\nsampler independence\\nsampler\\n. The acceptance probability is thus\\nα(x,y) = min\\n( f (y) g(x)\\nf (x) g(y), 1\\n)\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 97, 'page_label': '80'}, page_content='80 Monte Carlo Sampling\\n2. If the proposal transition density is symmetric (that is, q(y |x) = q(x |y)), then the\\nacceptance probability has the simple form\\nα(x,y) = min\\n( f (y)\\nf (x), 1\\n)\\n, (3.7)\\nand the MCMC algorithm is called a random walk sampler . A typical example israndom walk\\nsampler when, for a given current state x, the proposal state Y is of the form Y = x + Z,\\nwhere Z is generated from some spherically symmetric distribution, such as N(0,I).\\nWe now give an example illustrating the second approach.\\nExample 3.8 (Random Walk Sampler) Consider the two-dimensional pdf\\nf (x1,x2) = c e−1\\n4\\n√\\nx2\\n1+x2\\n2\\n\\x12\\nsin\\n\\x12\\n2\\nq\\nx2\\n1 + x2\\n2\\n\\x13\\n+ 1\\n\\x13\\n, −2π< x1 <2π, −2π< x2 <2π, (3.8)\\nwhere c is an unknown normalization constant. The graph of this pdf (unnormalized) is\\ndepicted in the left panel of Figure 3.5.\\n-6 -4 -2 0 2 4 6\\n-6\\n-4\\n-2\\n0\\n2\\n4\\n6\\nFigure 3.5: Left panel: the two-dimensional target pdf. Right panel: points from the random'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 97, 'page_label': '80'}, page_content='0\\n2\\n4\\n6\\nFigure 3.5: Left panel: the two-dimensional target pdf. Right panel: points from the random\\nwalk sampler are approximately distributed according to the target pdf.\\nThe following Python program implements a random walk sampler to (approximately)\\ndraw N = 104 dependent samples from the pdf f . At each step, given a current state x,\\na proposal Y is drawn from the N(x,I) distribution. That is, Y = x + Z, with Z bivariate\\nstandard normal. We see in the right panel of Figure 3.5 that the sampler works correctly.\\nThe starting point for the Markov chain is chosen as (0 ,0). Note that the normalization\\nconstant c is never required to be specified in the program.\\nrwsamp.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom numpy import pi, exp, sqrt, sin\\nfrom numpy.random import rand, randn'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 98, 'page_label': '81'}, page_content=\"Monte Carlo Methods 81\\nN = 10000\\na = lambda x: -2*pi < x\\nb = lambda x: x < 2*pi\\nf = lambda x1, x2: exp(-sqrt(x1**2+x2**2)/4)*(\\nsin(2*sqrt(x1**2+x2**2))+1)*a(x1)*b(x1)*a(x2)*b(x2)\\nxx = np.zeros((N,2))\\nx = np.zeros((1,2))\\nfor i in range (1,N):\\ny = x + randn(1,2)\\nalpha = np.amin((f(y[0][0],y[0][1])/f(x[0][0],x[0][1]),1))\\nr = rand() < alpha\\nx = r*y + (1-r)*x\\nxx[i,:] = x\\nplt.scatter(xx[:,0], xx[:,1], alpha =0.4,s =2)\\nplt.axis( 'equal ')\\nplt.show()\\n3.2.5.2 Gibbs Sampler\\nThe Gibbs sampler Gibbs sampler[48] uses a somewhat di fferent methodology from the Metropolis–\\nHastings algorithm and is particularly useful for generatingn-dimensional random vectors.\\nThe key idea of the Gibbs sampler is to update the components of the random vector\\none at a time, by sampling them from conditional pdfs. Thus, Gibbs sampling can be\\nadvantageous if it is easier to sample from the conditional distributions than from the joint\\ndistribution.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 98, 'page_label': '81'}, page_content='distribution.\\nSpecifically, suppose that we wish to sample a random vector X = [X1,..., Xn]⊤ ac-\\ncording to a target pdf f (x). Let f (xi |x1,..., xi−1, xi+1,..., xn) represent the conditional\\npdf2 of the i-th component, Xi, given the other components x1,..., xi−1,xi+1,..., xn. The\\nGibbs sampling algorithm is as follows.\\nAlgorithm 3.2.8: Gibbs Sampler\\ninput: Initial point X0, sample size N, and target pdf f .\\noutput: X1,..., XN approximately distributed according to f .\\n1 for t = 0 to N −1 do\\n2 Draw Y1 from the conditional pdf f (y1 |Xt,2,..., Xt,n).\\n3 for i = 2 to n do\\n4 Draw Yi from the conditional pdf f (yi |Y1,..., Yi−1,Xt,i+1,..., Xt,n).\\n5 Xt+1 ←Y\\n6 return X1,..., XN\\nThere exist many variants of the Gibbs sampler, depending on the steps required to\\nupdate Xt to Xt+1 — called the cycle of the Gibbs algorithm. In the algorithm above, the cycle\\n2In this section we employ a Bayesian notation style, using the same letter f for different (conditional)\\ndensities.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 99, 'page_label': '82'}, page_content='82 Monte Carlo Sampling\\ncycle consists of Steps 2–5, in which the components are updated in a fixed order 1→2 →\\n···→ n. For this reason Algorithm 3.2.8 is also called the systematic Gibbs sampler.systematic\\nGibbs sampler In the random-order Gibbs sampler, the order in which the components are updated\\nrandom-order\\nGibbs sampler in each cycle is a random permutation of {1,..., n}(see Exercise 9). Other modifications\\n☞115 are to update the components in blocks (i.e., several at the same time), or to update only\\na random selection of components. The variant where in each cycle only a single random\\ncomponent is updated is called therandom Gibbs sampler. In the reversible Gibbs samplerrandom Gibbs\\nsampler\\nreversible\\nGibbs sampler\\na cycle consists of the coordinate-wise updating 1 →2 →···→ n −1 →n →n −1 →\\n···→ 2 →1. In all cases, except for the systematic Gibbs sampler, the resulting Markov\\nchain {Xt,t = 1,2,... }is reversible and hence its limiting distribution is precisely f (x).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 99, 'page_label': '82'}, page_content='chain {Xt,t = 1,2,... }is reversible and hence its limiting distribution is precisely f (x).\\n☞452 Unfortunately, the systematic Gibbs Markov chain is not reversible and so the detailed\\nbalance equations are not satisfied. However, a similar result holds, due to Hammersley and\\nClifford, under the so-called positivity condition: if at a point x = (x1,..., xn) all marginal\\ndensities f (xi) >0,i = 1,..., n, then the joint density f (x) >0.\\nTheorem 3.2: Hammersley–Clifford Balance for the Gibbs Sampler\\nLet q1→n(y |x) denote the transition density of the systematic Gibbs sampler, and let\\nqn→1(x |y) be the transition density of the reverse move, in the order n →n −1 →\\n···→ 1. Then, if the positivity condition holds,\\nf (x) q1→n(y |x) = f (y) qn→1(x |y). (3.9)\\nProof: For the forward move we have:\\nq1→n(y |x) = f (y1 |x2,..., xn) f (y2 |y1,x3,..., xn) ··· f (yn |y1,..., yn−1),\\nand for the reverse move:\\nqn→1(x |y) = f (xn |y1,..., yn−1) f (xn−1 |y1,..., yn−2,xn) ··· f (x1 |x2,..., xn).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 99, 'page_label': '82'}, page_content='qn→1(x |y) = f (xn |y1,..., yn−1) f (xn−1 |y1,..., yn−2,xn) ··· f (x1 |x2,..., xn).\\nConsequently,\\nq1→n(y |x)\\nqn→1(x |y) =\\nnY\\ni=1\\nf (yi |y1,..., yi−1,xi+1,..., xn)\\nf (xi |y1,..., yi−1,xi+1,..., xn)\\n=\\nnY\\ni=1\\nf (y1,..., yi,xi+1,..., xn)\\nf (y1,..., yi−1,xi,..., xn)\\n= f (y) Qn−1\\ni=1 f (y1,..., yi,xi+1,..., xn)\\nf (x) Qn\\nj=2 f (y1,..., yj−1,xj,..., xn)\\n= f (y) Qn−1\\ni=1 f (y1,..., yi,xi+1,..., xn)\\nf (x) Qn−1\\nj=1 f (y1,..., yj,xj+1,..., xn)\\n= f (y)\\nf (x).\\nThe result follows by rearranging the last identity. The positivity condition ensures that we\\ndo not divide by 0 along the line. □\\nIntuitively, the long-run proportion of transitions x →y for the “forward move” chain\\nis equal to the long-run proportion of transitions y → x for the “reverse move” chain.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 100, 'page_label': '83'}, page_content='Monte Carlo Methods 83\\nTo verify that the Markov chain X0,X1,... for the systematic Gibbs sampler indeed has\\nlimiting pdf f (x), we need to check that the global balance equations (C.42) hold. By ☞ 452\\nintegrating (in the continuous case) both sides in (3.9) with respect tox, we see that indeed\\nZ\\nf (x) q1→n(y |x) dx = f (y).\\nExample 3.9 (Gibbs Sampler for the Bayesian Normal Model) Gibbs samplers are\\noften applied in Bayesian statistics, to sample from the posterior pdf. Consider for instance\\nthe Bayesian normal model ☞ 51\\nf (µ,σ2) = 1/σ2\\n(x |µ,σ2) ∼N(µ1,σ2I).\\nHere the prior for ( µ,σ2) is improper. improper priorThat is, it is not a pdf in itself, but by obstinately\\napplying Bayes’ formula it does yield a proper posterior pdf. In some sense this prior\\nconveys the least amount of information about µand σ2. Following the same procedure as\\nin Example 2.8, we find the posterior pdf:\\nf (µ,σ2 |x) ∝\\n\\x10\\nσ2\\x11−n/2−1\\nexp\\n(\\n−1\\n2\\nP\\ni(xi −µ)2\\nσ2\\n)\\n. (3.10)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 100, 'page_label': '83'}, page_content='f (µ,σ2 |x) ∝\\n\\x10\\nσ2\\x11−n/2−1\\nexp\\n(\\n−1\\n2\\nP\\ni(xi −µ)2\\nσ2\\n)\\n. (3.10)\\nNote that µand σ2 here are the “variables” andx is a fixed data vector. To simulate samples\\nµand σ2 from (3.10) using the Gibbs sampler, we need the distributions of both (µ|σ2,x)\\nand (σ2 |µ,x). To find f (µ|σ2,x), view the right-hand side of (3.10) as a function of µ\\nonly, regarding σ2 as a constant. This gives\\nf (µ|σ2,x) ∝exp\\n(\\n−nµ2 −2µP\\ni xi\\n2σ2\\n)\\n= exp\\n(\\n−µ2 −2µx\\n2(σ2/n)\\n)\\n∝exp\\n(\\n−1\\n2\\n(µ−x)2\\nσ2/n\\n)\\n. (3.11)\\nThis shows that (µ|σ2,x) has a normal distribution with mean x and variance σ2/n.\\nSimilarly, to find f (σ2 |µ,x), view the right-hand side of (3.10) as a function of σ2,\\nregarding µas a constant. This gives\\nf (σ2 |µ,x) ∝(σ2)−n/2−1 exp\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3−1\\n2\\nnX\\ni=1\\n(xi −µ)2/σ2\\n\\uf8fc\\uf8f4\\uf8f4\\uf8fd\\uf8f4\\uf8f4\\uf8fe, (3.12)\\nshowing that ( σ2 |µ,x) has an inverse-gamma distribution with parameters n/2 and ☞ 425Pn\\ni=1(xi −µ)2/2. The Gibbs sampler thus involves the repeated simulation of\\n(µ|σ2,x) ∼N\\n\\x10\\nx, σ2/n\\n\\x11\\nand ( σ2 |µ,x) ∼InvGamma\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edn/2,\\nnX\\ni=1'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 100, 'page_label': '83'}, page_content='(µ|σ2,x) ∼N\\n\\x10\\nx, σ2/n\\n\\x11\\nand ( σ2 |µ,x) ∼InvGamma\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edn/2,\\nnX\\ni=1\\n(xi −µ)2/2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.\\nSimulating X ∼InvGamma(α,λ) is achieved by first generatingZ ∼Gamma(α,λ) and\\nthen returning X = 1/Z.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 101, 'page_label': '84'}, page_content='84 Monte Carlo Sampling\\nIn our parameterization of the Gamma(α,λ) distribution, λ is the rate parameter.\\nMany software packages instead use the scale parameter c = 1/λ. Be aware of this\\nwhen simulating Gamma random variables.\\nThe Python script below defines a small data set of size n = 10 (which was randomly\\nsimulated from a standard normal distribution), and implements the systematic Gibbs\\nsampler to simulate from the posterior distribution, using N = 105 samples.\\ngibbsamp.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nx = np.array([[-0.9472, 0.5401, -0.2166, 1.1890, 1.3170,\\n-0.4056, -0.4449, 1.3284, 0.8338, 0.6044]])\\nn=x.size\\nsample_mean = np.mean(x)\\nsample_var = np.var(x)\\nsig2 = np.var(x)\\nmu=sample_mean\\nN=10**5\\ngibbs_sample = np.array(np.zeros((N, 2)))\\nfor k in range (N):\\nmu=sample_mean + np.sqrt(sig2/n)*np.random.randn()\\nV=np. sum ((x-mu)**2)/2\\nsig2 = 1/np.random.gamma(n/2, 1/V)\\ngibbs_sample[k,:]= np.array([mu, sig2])'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 101, 'page_label': '84'}, page_content=\"V=np. sum ((x-mu)**2)/2\\nsig2 = 1/np.random.gamma(n/2, 1/V)\\ngibbs_sample[k,:]= np.array([mu, sig2])\\nplt.scatter(gibbs_sample[:,0], gibbs_sample[:,1],alpha =0.1,s =1)\\nplt.plot(np.mean(x), np.var(x), 'wo')\\nplt.show()\\n-1 -0.5 0 0.5 1 1.5 2\\n7\\n0\\n0.5\\n1\\n1.5\\n^f(7 j x)\\nFigure 3.6: Left: approximate draws from the posterior pdf f (µ,σ2 |x) obtained via the\\nGibbs sampler. Right: estimate of the posterior pdf f (µ|x).\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 102, 'page_label': '85'}, page_content='Monte Carlo Methods 85\\nThe left panel of Figure 3.6 shows the ( µ,σ2) points generated by the Gibbs sampler.\\nAlso shown, via the white circle, is the point (x,s2), where x = 0.3798 is the sample mean\\nand s2 = 0.6810 the sample variance. This posterior point cloud visualizes the considerable\\nuncertainty in the estimates. By projecting the ( µ,σ2) points onto the µ-axis — that is,\\nby ignoring the σ2 values — one obtains (approximate) samples from the posterior pdf\\nof µ; that is, f (µ|x). The right panel of Figure 3.6 shows a kernel density estimate (see\\nSection 4.4) of this pdf. The corresponding 0 .025 and 0.975 sample quantiles were found ☞ 134\\nto be −0.2054 and 0.9662, respectively, giving the 95% credible interval (−0.2054,0.9662)\\nfor µ, which contains the true expectation 0. Similarly, an estimated 95% credible interval\\nfor σ2 is (0.3218,2.2485), which contains the true variance 1.\\n3.3 Monte Carlo Estimation'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 102, 'page_label': '85'}, page_content='for σ2 is (0.3218,2.2485), which contains the true variance 1.\\n3.3 Monte Carlo Estimation\\nIn this section we describe how Monte Carlo simulation can be used to estimate complic-\\nated integrals, probabilities, and expectations. A number of variance reduction techniques\\nare introduced as well, including the recent cross-entropy method.\\n3.3.1 Crude Monte Carlo\\nThe most common setting for Monte Carlo estimation is the following: Suppose we wish to\\ncompute the expectation µ= EY of some (say continuous) random variable Y with pdf f ,\\nbut the integral EY =\\nR\\ny f(y) dy is difficult to evaluate. For example, if Y is a complicated\\nfunction of other random variables, it would be di fficult to obtain an exact expression for\\nf (y). The idea of crude Monte Carlo — sometimes abbreviated as CMC — is to approx- crude Monte\\nCarloimate µby simulating many independent copies Y1,..., YN of Y and then take their sample\\nmean Y as an estimator of µ. All that is needed is an algorithm to simulate such copies.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 102, 'page_label': '85'}, page_content='mean Y as an estimator of µ. All that is needed is an algorithm to simulate such copies.\\nBy the Law of Large Numbers, Y converges to µas N →∞, provided the expectation ☞ 446\\nof Y exists. Moreover, by the Central Limit Theorem, Y approximately has a N(µ,σ2/N) ☞ 447\\ndistribution for large N, provided that the variance σ2 = VarY <∞. This enables the con-\\nstruction of an approximate (1 −α) confidence interval for µ: confidence\\ninterval \\nY −z1−α/2\\nS√\\nN\\n, Y + z1−α/2\\nS√\\nN\\n!\\n, (3.13)\\nwhere S is the sample standard deviation of the {Yi}and zγ denotes the γ-quantile of the\\nN(0,1) distribution; see also Section C.13. Instead of specifying the confidence interval, ☞ 457\\none often reports only the sample mean and the estimated standard error: S/\\n√\\nN, or the estimated\\nstandard errorestimated relative error: S/(Y\\n√\\nN). The basic estimation procedure for independent data\\nestimated\\nrelative erroris summarized in Algorithm 3.3.1 below.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 102, 'page_label': '85'}, page_content='estimated\\nrelative erroris summarized in Algorithm 3.3.1 below.\\nIt is often the case that the output Y is a function of some underlying random vector or\\nstochastic process; that is, Y = H(X), where H is a real-valued function and X is a random\\nvector or process. The beauty of Monte Carlo for estimation is that (3.13) holds regardless\\nof the dimension of X.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 103, 'page_label': '86'}, page_content='86 Monte Carlo Estimation\\nAlgorithm 3.3.1: Crude Monte Carlo for Independent Data\\ninput: Simulation algorithm for Y ∼f , sample size N, confidence level 1 −α.\\noutput: Point estimate and approximate (1 −α) confidence interval for µ= EY.\\n1 Simulate Y1,..., YN\\niid\\n∼f .\\n2 Y ←1\\nN\\nPN\\ni=1 Yi\\n3 S 2 ← 1\\nN−1\\nPN\\ni=1(Yi −Y)2\\n4 return Y and the interval (3.13).\\nExample 3.10 (Monte Carlo Integration) In Monte Carlo integration , simulation isMonte Carlo\\nintegration used to evaluate complicated integrals. Consider, for example, the integral\\nµ=\\nZ ∞\\n−∞\\nZ ∞\\n−∞\\nZ ∞\\n−∞\\np\\n|x1 + x2 + x3|e−(x2\\n1+x2\\n2+x2\\n3)/2 dx1 dx2 dx3.\\nDefining Y = |X1 + X2 + X3|1/2(2π)3/2, with X1,X2,X3\\niid\\n∼N(0,1), we can write µ = EY.\\nUsing the following Python program, with a sample size of N = 106, we obtained an\\nestimate Y = 17.031 with an approximate 95% confidence interval (17.017,17.046).\\nmcint.py\\nimport numpy as np\\nfrom numpy import pi\\nc = (2*pi)**(3/2)\\nH = lambda x: c*np.sqrt(np. abs (np. sum (x,axis=1)))\\nN = 10**6\\nz = 1.96'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 103, 'page_label': '86'}, page_content=\"c = (2*pi)**(3/2)\\nH = lambda x: c*np.sqrt(np. abs (np. sum (x,axis=1)))\\nN = 10**6\\nz = 1.96\\nx = np.random.randn(N,3)\\ny = H(x)\\nmY = np.mean(y)\\nsY = np.std(y)\\nRE = sY/mY/np.sqrt(N)\\nprint ('Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f}) '.format (\\nmY, mY*(1-z*RE), mY*(1+z*RE)))\\nEstimate = 17.031, CI = (17.017,17.046)\\nExample 3.11 (Example 2.1 (cont.)) We return to the bias–variance tradeo ff in Ex-\\nample 2.1. Figure 2.7 gives estimates of the (squared-error) generalization risk (2.5) as☞26\\n☞29\\n☞23\\na function of the number of parameters in the model. But how accurate are these estim-\\nates? Because we know in this case the exact model for the data, we can use Monte Carlo\\nsimulation to estimate the generalization risk (for a fixed training set) and the expected\\ngeneralization risk (averaged over all training sets) precisely. All we need to do is repeat\\nthe data generation, fitting, and validation steps many times and then take averages of the\\nresults. The following Python code repeats 100 times:\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 103, 'page_label': '86'}, page_content='results. The following Python code repeats 100 times:\\n1. Simulate the training set of size n = 100.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 104, 'page_label': '87'}, page_content='Monte Carlo Methods 87\\n2. Fit models up to size k = 8.\\n3. Estimate the test loss using a test set with the same sample size n = 100.\\nFigure 3.7 shows that there is some variation in the test losses, due to the randomness in\\nboth the training and test sets. To obtain an accurate estimate of the expected generalization\\nrisk (2.6), take the average of the test losses. We see that fork ⩽8 the estimate in Figure 2.7\\nis close to the true expected generalization risk.\\n1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\nNumber of parameters p\\n25\\n50\\n75\\n100\\n125\\n150\\n175\\n200Test loss\\nFigure 3.7: Independent estimates of the test loss show some variability.\\nCMCtestloss.py\\nimport numpy as np, matplotlib.pyplot as plt\\nfrom numpy.random import rand, randn\\nfrom numpy.linalg import solve\\ndef generate_data(beta, sig, n):\\nu = rand(n, 1)\\ny = (u ** np.arange(0, 4)) @ beta + sig * randn(n, 1)\\nreturn u, y\\nbeta = np.array([[10, -140, 400, -250]]).T\\nn = 100\\nsig = 5\\nbetahat = {}\\nplt.figure(figsize=[6,5])\\ntotMSE = np.zeros(8)\\nmax_p = 8'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 104, 'page_label': '87'}, page_content='n = 100\\nsig = 5\\nbetahat = {}\\nplt.figure(figsize=[6,5])\\ntotMSE = np.zeros(8)\\nmax_p = 8\\np_range = np.arange(1, max_p + 1, 1)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 105, 'page_label': '88'}, page_content=\"88 Monte Carlo Estimation\\nfor N in range (0,100):\\nu, y = generate_data(beta, sig, n) #training data\\nX = np.ones((n, 1))\\nfor p in p_range:\\nif p > 1:\\nX = np.hstack((X, u**(p-1)))\\nbetahat[p] = solve(X.T @ X, X.T @ y)\\nu_test, y_test = generate_data(beta, sig, n) #test data\\nMSE = []\\nX_test = np.ones((n, 1))\\nfor p in p_range:\\nif p > 1:\\nX_test = np.hstack((X_test, u_test**(p-1)))\\ny_hat = X_test @ betahat[p] # predictions\\nMSE.append(np. sum ((y_test - y_hat)**2/n))\\ntotMSE = totMSE + np.array(MSE)\\nplt.plot(p_range, MSE, 'C0',alpha=0.1)\\nplt.plot(p_range,totMSE/N, 'r-o')\\nplt.xticks(ticks=p_range)\\nplt.xlabel( 'Number of parameters $p$ ')\\nplt.ylabel( 'Test loss ')\\nplt.tight_layout()\\nplt.savefig( 'MSErepeatpy.pdf ',format ='pdf')\\nplt.show()\\n3.3.2 Bootstrap Method\\nThe bootstrap method [37] combines CMC estimation with the resampling procedure of\\nSection 3.2.4. The idea is as follows: Suppose we wish to estimate a number µvia some☞76\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 105, 'page_label': '88'}, page_content='Section 3.2.4. The idea is as follows: Suppose we wish to estimate a number µvia some☞76\\nestimator Y = H(T), where T := {X1,..., Xn}is an iid sample from some unknown cdf\\nF. It is assumed that Y does not depend on the order of the {Xi}. To assess the quality (for\\nexample, accuracy) of the estimatorY, one could draw independent replicationsT1,..., TN\\nof Tand find sample estimates for quantities such as the variance VarY, the bias EY −µ,\\nand the mean squared error E(Y −µ)2. However, it may be too time-consuming or simply\\nnot feasible to obtain such replications. An alternative is to resample the original data.\\nTo reiterate, given an outcome τ = {x1,..., xn}of T, we simulate an iid sample T∗ :=\\n{X∗\\n1,..., X∗\\nn}from the empirical cdf Fn, via Algorithm 3.2.6 (hence the resampling size is☞76\\nN = n here).\\nThe rationale is that the empirical cdf Fn is close to the actual cdf F and gets closer as\\nn gets larger. Hence, any quantities depending onF, such as EFg(Y), where g is a function,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 105, 'page_label': '88'}, page_content='n gets larger. Hence, any quantities depending onF, such as EFg(Y), where g is a function,\\ncan be approximated by EFn g(Y). The latter is usually still di fficult to evaluate, but it can\\nbe simply estimated via CMC as\\n1\\nK\\nKX\\ni=1\\ng(Y∗\\ni ),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 106, 'page_label': '89'}, page_content='Monte Carlo Methods 89\\nwhere Y∗\\n1 ,..., Y∗\\nK are independent random variables, each distributed as Y∗ = H(T∗). This\\nseemingly self-referent procedure is called bootstrapping — alluding to Baron von Mün- bootstrapping\\nchausen, who pulled himself out of a swamp by his own bootstraps. As an example, the\\nbootstrap estimate of the expectation of Y is\\ncEY = Y\\n∗\\n= 1\\nK\\nKX\\ni=1\\nY∗\\ni ,\\nwhich is simply the sample mean of {Y∗\\ni }. Similarly, the bootstrap estimate for VarY is the\\nsample variance\\n[VarY = 1\\nK −1\\nKX\\ni=1\\n(Y∗\\ni −Y\\n∗\\n)2. (3.14)\\nBootstrap estimators for the bias and MSE are Y\\n∗\\n−Y and 1\\nK\\nPK\\ni=1(Y∗\\ni −Y)2, respectively.\\nNote that for these estimators the unknown quantityµis replaced with its original estimator\\nY. Confidence intervals can be constructed in the same fashion. We mention two variants:\\nthe normal method and the percentile method. In the normal method, a 1 −αconfidence normal method\\npercentile\\nmethod\\ninterval for µis given by\\n(Y ±z1−α/2S ∗),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 106, 'page_label': '89'}, page_content='percentile\\nmethod\\ninterval for µis given by\\n(Y ±z1−α/2S ∗),\\nwhere S ∗ is the bootstrap estimate of the standard deviation of Y; that is, the square root\\nof (3.14). In the percentile method, the upper and lower bounds of the 1 −α confidence\\ninterval for µare given by the 1 −α/2 and α/2 quantiles of Y, which in turn are estimated\\nvia the corresponding sample quantiles of the bootstrap sample {Y∗\\ni }.\\nThe following example illustrates the usefulness of the bootstrap method for ratio es-\\ntimation and also introduces the renewal reward processmodel for data.\\nExample 3.12 (Bootstrapping the Ratio Estimator) A common scenario in stochastic\\nsimulation is that the output of the simulation consists of independent pairs of data\\n(C1,R1),(C2,R2),... , where each C is interpreted as the length of a period of time — a so-\\ncalled cycle — and R is the reward obtained during that cycle. Such a collection of random\\nvariables {(Ci,Ri)}is called a renewal reward process renewal\\nreward process'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 106, 'page_label': '89'}, page_content='variables {(Ci,Ri)}is called a renewal reward process renewal\\nreward process\\n. Typically, the rewardRi depends on\\nthe cycle length Ci. Let At be the average reward earned by time t; that is, At = PNt\\ni=1 Ri/t,\\nwhere Nt = max{n : C1 + ··· + Cn ⩽t}counts the number of complete cycles at time t. It\\ncan be shown, see Exercise 20, that if the expectations of the cycle length and reward are ☞ 119\\nfinite, then At converges to the constant ER/EC. This ratio can thus be interpreted as the\\nlong-run average reward long-run\\naverage reward\\n.\\nEstimation of the ratio ER/EC from data ( C1,R1),..., (Cn,Rn) is easy: take the ratio\\nestimator ratio estimator\\nA = R\\nC\\n.\\nHowever, this estimator A is not unbiased and it is not obvious how to derive confidence\\nintervals. Fortunately, the bootstrap method can come to the rescue: simply resample the\\npairs {(Ci,Ri)}, obtain ratio estimators A∗\\n1,..., A∗\\nK, and from these compute quantities of\\ninterest such as confidence intervals.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 106, 'page_label': '89'}, page_content='1,..., A∗\\nK, and from these compute quantities of\\ninterest such as confidence intervals.\\nAs a concrete example, let us return to the Markov chain in Example 3.6. Recall that ☞ 75\\nthe chain starts at state 1 at time 0. After a certain amount of time T1, the process returns'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 107, 'page_label': '90'}, page_content='90 Monte Carlo Estimation\\nto state 1. The time steps 0 ,..., T1 −1 form a natural “cycle” for this process, as from\\ntime T1 onwards the process behaves probabilistically exactly the same as when it started,\\nindependently of X0,..., XT1−1. Thus, if we define T0 = 0, and let Ti be the i-th time that\\nthe chain returns to state 1, then we can break up the time interval into independent cycles\\nof lengths Ci = Ti −Ti−1, i = 1,2,... . Now suppose that during the i-th cycle a reward\\nRi =\\nTi−1X\\nt=Ti−1\\nϱt−Ti−1 r(Xt)\\nis received, where r(i) is some fixed reward for visiting state i ∈{1,2,3,4}and ϱ ∈(0,1)\\nis a discounting factor. Clearly,{(Ci,Ri)}is a renewal reward process. Figure 3.8 shows the\\noutcomes of 1000 pairs (C,R), using r(1) = 4,r(2) = 3,r(3) = 10,r(4) = 1, and ϱ= 0.9.\\n0 10 20 30 40 50 60 70\\nCycle length\\n0\\n10\\n20\\n30\\n40\\n50\\n60Reward\\nFigure 3.8: Each circle represents a (cycle length, reward) pair. The varying circle sizes'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 107, 'page_label': '90'}, page_content='60Reward\\nFigure 3.8: Each circle represents a (cycle length, reward) pair. The varying circle sizes\\nindicate the number of occurrences for a given pair. For example, (2,15.43) is the most\\nlikely pair here, occurring 186 out of a 1000 times. It corresponds to the cycle path 1 →\\n3 →2 →1.\\nThe long-run average reward is estimated as 2.50 for our data. But how accurate is this\\nestimate? Figure 3.9 shows a density plot of the bootstrapped ratio estimates, where we\\nindependently resampled the data pairs 1000 times.\\n2.2\\n 2.4\\n 2.6\\n 2.8\\nlong-run average reward\\n0\\n2\\n4density\\nFigure 3.9: Density plot of the bootstrapped ratio estimates for the Markov chain renewal\\nreward process.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 108, 'page_label': '91'}, page_content='Monte Carlo Methods 91\\nFigure 3.9 indicates that the true long-run average reward lies between 2.2 and 2.8\\nwith high confidence. More precisely, the 99% bootstrap confidence interval (percentile\\nmethod) is here (2.27, 2.77). The following Python script spells out the procedure.\\nratioest.py\\nimport numpy as np, matplotlib.pyplot as plt, seaborn as sns\\nfrom numba import jit\\nnp.random.seed(123)\\nn = 1000\\nP = np.array([[0, 0.2, 0.5, 0.3],\\n[0.5 ,0, 0.5, 0],\\n[0.3, 0.7, 0, 0],\\n[0.1, 0, 0, 0.9]])\\nr = np.array([4,3,10,1])\\nCorg = np.array(np.zeros((n,1)))\\nRorg = np.array(np.zeros((n,1)))\\nrho=0.9\\n@jit() #for speed -up; see Appendix\\ndef generate_cyclereward(n):\\nfor i in range (n):\\nt=1\\nxreg = 1 #regenerative state (out of 1,2,3,4)\\nreward = r[0]\\nx= np.amin(np.argwhere(np.cumsum(P[xreg-1,:]) > np.random.\\nrand())) + 1\\nwhile x != xreg:\\nt += 1\\nreward += rho**(t-1)*r[x-1]\\nx = np.amin(np.where(np.cumsum(P[x-1,:]) > np.random.rand\\n())) + 1\\nCorg[i] = t\\nRorg[i] = reward\\nreturn Corg, Rorg'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 108, 'page_label': '91'}, page_content=\"())) + 1\\nCorg[i] = t\\nRorg[i] = reward\\nreturn Corg, Rorg\\nCorg, Rorg = generate_cyclereward(n)\\nAorg = np.mean(Rorg)/np.mean(Corg)\\nK = 5000\\nA = np.array(np.zeros((K,1)))\\nC = np.array(np.zeros((n,1)))\\nR = np.array(np.zeros((n,1)))\\nfor i in range (K):\\nind = np.ceil(n*np.random.rand(1,n)).astype( int )[0]-1\\nC = Corg[ind]\\nR = Rorg[ind]\\nA[i] = np.mean(R)/np.mean(C)\\nplt.xlabel( 'long-run average reward ')\\nplt.ylabel( 'density ')\\nsns.kdeplot(A.flatten(),shade=True)\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 109, 'page_label': '92'}, page_content='92 Monte Carlo Estimation\\n3.3.3 Variance Reduction\\nThe estimation of performance measures in Monte Carlo simulation can be made more\\nefficient by utilizing known information about the simulation model. Variance reduction\\ntechniques include antithetic variables, control variables, importance sampling, conditional\\nMonte Carlo, and stratified sampling; see, for example, [71, Chapter 9]. We shall only deal\\nwith control variables and importance sampling here.\\nSuppose Y is the output of a simulation experiment. A random variable eY, obtained\\nfrom the same simulation run, is called a control variablecontrol\\nvariable\\nfor Y if Y and eY are correlated\\n(negatively or positively) and the expectation of eY is known. The use of control variables\\nfor variance reduction is based on the following theorem. We leave its proof to Exercise 21.\\n☞119\\nTheorem 3.3: Control Variable Estimation\\nLet Y1,..., YN be the output of N independent simulation runs and let eY1,..., eYN be'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 109, 'page_label': '92'}, page_content='Let Y1,..., YN be the output of N independent simulation runs and let eY1,..., eYN be\\nthe corresponding control variables, with EeYk = eµknown. Let ϱY,eY be the correlation\\ncoefficient between each Yk and eYk. For each α∈Rthe estimator\\nbµ(c) = 1\\nN\\nNX\\nk=1\\nh\\nYk −α\\n\\x10eYk −eµ\\n\\x11i\\n(3.15)\\nis an unbiased estimator for µ= EY. The minimal variance of bµ(c) is\\nVar bµ(c) = 1\\nN (1 −ϱ2\\nY,eY ) Var Y, (3.16)\\nwhich is obtained for α= ϱY,eY\\nq\\nVarY/VareY.\\nFrom (3.16) we see that, by using the optimal αin (3.15), the variance of the control\\nvariate estimator is a factor 1 −ϱ2\\nY,eY smaller than the variance of the crude Monte Carlo\\nestimator. Thus, if eY is highly correlated with Y, a significant variance reduction can be\\nachieved. The optimalαis usually unknown, but it can be easily estimated from the sample\\ncovariance matrix of {(Yk,eYk)}.☞456\\nIn the next example, we estimate the multiple integral in Example 3.10 using control\\nvariables.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 109, 'page_label': '92'}, page_content='In the next example, we estimate the multiple integral in Example 3.10 using control\\nvariables.\\nExample 3.13 (Monte Carlo Integration (cont.))☞86 The random variable Y = |X1 + X2 +\\nX3|1/2(2π)3/2 is positively correlated with the random variable eY = X2\\n1 + X2\\n2 + X2\\n3 , for the\\nsame choice of X1,X2,X3\\niid\\n∼N(0,1). As EeY = Var(X1 + X2 + X3) = 3, we can use it as a\\ncontrol variable to estimate the expectation of Y. The following Python program is based\\non Theorem 3.3. It imports the crude Monte Carlo sampling code from Example 3.10.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 110, 'page_label': '93'}, page_content=\"Monte Carlo Methods 93\\nmcintCV.py\\nfrom mcint import *\\nYc = np. sum (x**2, axis=1) # control variable data\\nyc = 3 # true expectation of control variable\\nC = np.cov(y,Yc) # sample covariance matrix\\ncor = C[0][1]/np.sqrt(C[0][0]*C[1][1])\\nalpha = C[0][1]/C[1][1]\\nest = np.mean(y-alpha*(Yc-yc))\\nRECV = np.sqrt((1-cor**2)*C[0][0]/N)/est #relative error\\nprint ('Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f}), Corr = {:3.3f} '.\\nformat (est, est*(1-z*RECV), est*(1+z*RECV),cor))\\nEstimate = 17.045, CI = (17.032,17.057), Corr = 0.480\\nA typical estimate of the correlation coefficient ϱY,eY is 0.48, which gives a reduction of\\nthe variance with a factor 1−0.482 ≈0.77 — a simulation speed-up of 23% compared with\\ncrude Monte Carlo. Although the gain is small in this case, due to the modest correlation\\nbetween Y and eY, little extra work was required to achieve this variance reduction.\\nOne of the most important variance reduction techniques is importance sampling importance\\nsampling\\n. This\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 110, 'page_label': '93'}, page_content='sampling\\n. This\\ntechnique is especially useful for the estimation of very small probabilities. The standard\\nsetting is the estimation of a quantity\\nµ= Ef H(X) =\\nZ\\nH(x) f (x) dx, (3.17)\\nwhere H is a real-valued function and f the probability density of a random vector X,\\ncalled the nominal pdf. The subscript f is added to the expectation operator to indicate that nominal pdf\\nit is taken with respect to the density f .\\nLet g be another probability density such that g(x) = 0 implies that H(x) f (x) = 0.\\nUsing the density g we can represent µas\\nµ=\\nZ\\nH(x) f (x)\\ng(x) g(x) dx = Eg\\n\"\\nH(X) f (X)\\ng(X)\\n#\\n. (3.18)\\nConsequently, if X1,..., XN ∼iid g, then\\nbµ= 1\\nN\\nNX\\nk=1\\nH(Xk) f (Xk)\\ng(Xk) (3.19)\\nis an unbiased estimator of µ. This estimator is called the importance sampling estimator importance\\nsampling\\nestimator\\nand g is called the importance sampling density. The ratio of densities, f (x)/g(x), is called'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 110, 'page_label': '93'}, page_content='and g is called the importance sampling density. The ratio of densities, f (x)/g(x), is called\\nthe likelihood ratio. The importance sampling pseudo-code is given in Algorithm 3.3.2. likelihood ratio'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 111, 'page_label': '94'}, page_content='94 Monte Carlo Estimation\\nAlgorithm 3.3.2: Importance Sampling Estimation\\ninput: Function H, importance sampling density g such that g(x) = 0 for all x for\\nwhich H(x) f (x) = 0, sample size N, confidence level 1 −α.\\noutput: Point estimate and approximate (1 −α) confidence interval for\\nµ= EH(X), where X ∼f .\\n1 Simulate X1,..., XN\\niid\\n∼g and let Yi = H(Xi) f (Xi)/g(Xi), i = 1,..., N.\\n2 Estimate µvia bµ= Y and determine an approximate (1 −α) confidence interval as\\nI:=\\n \\nbµ−z1−α/2\\nS√\\nN\\n, bµ+ z1−α/2\\nS√\\nN\\n!\\n,\\nwhere zγ denotes the γ-quantile of the N(0,1) distribution and S is the sample\\nstandard deviation of Y1,..., YN.\\n3 return bµand the interval I.\\nExample 3.14 (Importance Sampling) Let us examine the workings of importance\\nsampling by estimating the area, µsay, under the graph of the function\\nM(x1,x2) = e−1\\n4\\n√\\nx2\\n1+x2\\n2\\n\\x12\\nsin\\n\\x12\\n2\\nq\\nx2\\n1 + x2\\n2\\n\\x13\\n+ 1\\n\\x13\\n, (x1,x2) ∈R2. (3.20)\\nWe saw a similar function in Example 3.8 (but note the di fferent domain). A natural ap-☞80'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 111, 'page_label': '94'}, page_content=\"We saw a similar function in Example 3.8 (but note the di fferent domain). A natural ap-☞80\\nproach to estimate the area is to truncate the domain to the square [−b,b]2, for large enough\\nb, and to estimate the integral\\nµb =\\nZ b\\n−b\\nZ b\\n−b\\n(2b)2 M(x)|      {z      }\\nH(x)\\nf (x) dx = Ef H(X)\\nvia crude Monte Carlo, where f (x) = 1/(2b)2,x ∈[−b,b]2, is the pdf of the uniform distri-\\nbution on [−b,b]2. Here is the Python code which does just that.\\nimpsamp1.py\\nimport numpy as np\\nfrom numpy import exp, sqrt, sin, pi, log, cos\\nfrom numpy.random import rand\\nb = 1000\\nH = lambda x1, x2: (2*b)**2 * exp(-sqrt(x1**2+x2**2)/4)*(sin(2*sqrt(\\nx1**2+x2**2))+1)*(x1**2 + x2**2 < b**2)\\nf = 1/((2*b)**2)\\nN = 10**6\\nX1 = -b + 2*b*rand(N,1)\\nX2 = -b + 2*b*rand(N,1)\\nZ = H(X1,X2)\\nestCMC = np.mean(Z).item() # to obtain scalar\\nRECMC = np.std(Z)/estCMC/sqrt(N).item()\\nprint ('CI = ({:3.3f},{:3.3f}), RE = {: 3.3f} '.format (estCMC*(1-1.96*\\nRECMC), estCMC*(1+1.96*RECMC),RECMC))\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 112, 'page_label': '95'}, page_content='Monte Carlo Methods 95\\nCI = (82.663,135.036), RE = 0.123\\nFor a truncation level of b = 1000 and a sample size of N = 106, a typical estimate is\\n108.8, with an estimated relative error of 0.123. We have two sources of error here. The\\nfirst is the error in approximatingµby µb. However, as the functionH decays exponentially\\nfast, b = 1000 is more than enough to ensure this error is negligible. The second type of\\nerror is the statistical error, due to the estimation process itself. This can be quantified by\\nthe estimated relative error, and can be reduced by increasing the sample size.\\nLet us now consider an importance sampling approach in which the importance\\nsampling pdf g is radially symmetric and decays exponentially in the radius, similar to the\\nfunction H. In particular, we simulate (X1,X2) in a way akin to Example 3.1, by first gen- ☞ 69\\nerating a radius R ∼Exp(λ) and an angle Θ ∼U(0,2π), and then returning X1 = R cos(Θ)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 112, 'page_label': '95'}, page_content=\"erating a radius R ∼Exp(λ) and an angle Θ ∼U(0,2π), and then returning X1 = R cos(Θ)\\nand X2 = R sin(Θ). By the Transformation Rule (Theorem C.4) we then have ☞ 433\\ng(x) = fR,Θ(r,θ)1\\nr = λe−λr 1\\n2π\\n1\\nr = λe−λ\\n√\\nx2\\n1+x2\\n2\\n2π\\nq\\nx2\\n1 + x2\\n2\\n, x ∈R2 \\\\{0}.\\nThe following code, which imports the one given above, implements the importance\\nsampling steps, using the parameter λ= 0.1.\\nimpsamp2.py\\nfrom impsamp1 import *\\nlam = 0.1;\\ng = lambda x1, x2: lam*exp(-sqrt(x1**2 + x2**2)*lam)/sqrt(x1**2 + x2\\n**2)/(2*pi);\\nU = rand(N,1); V = rand(N,1)\\nR = -log(U)/lam\\nX1 = R*cos(2*pi*V)\\nX2 = R*sin(2*pi*V)\\nZ = H(X1,X2)*f/g(X1,X2)\\nestIS = np.mean(Z).item() # obtain scalar\\nREIS = np.std(Z)/estIS/sqrt(N).item()\\nprint ('CI = ({:3.3f},{:3.3f}), RE = {: 3.3f} '.format (estIS*(1-1.96*\\nREIS), estIS*(1+1.96*REIS),REIS))\\nCI = (100.723,101.077), RE = 0.001\\nA typical estimate is 100.90 with an estimated relative error of 1 ·10−4, which gives\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 112, 'page_label': '95'}, page_content='A typical estimate is 100.90 with an estimated relative error of 1 ·10−4, which gives\\na substantial variance reduction. In terms of approximate 95% confidence intervals, we\\nhave (82.7,135.0) in the CMC case versus (100.7,101.1) in the importance sampling case.\\nOf course, we could have reduced the truncation level b to improve the performance of\\nCMC, but then the approximation error might become more significant. For the importance\\nsampling case, the relative error is hardly affected by the threshold level, but does depend\\non the choice of λ. We chose λsuch that the decay rate is slower than the decay rate of the\\nfunction H, which is 0.25.\\nAs illustrated in the above example, a main difficulty in importance sampling is how to\\nchoose the importance sampling distribution. A poor choice of g may seriously affect the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 113, 'page_label': '96'}, page_content='96 Monte Carlo for Optimization\\naccuracy of both the estimate and the confidence interval. The theoretically optimal choice\\ng∗ for the importance sampling density minimizes the variance of bµ and is therefore the\\nsolution to the functional minimization program\\nmin\\ng\\nVarg\\n \\nH(X) f (X)\\ng(X)\\n!\\n. (3.21)\\nIt is not difficult to show, see also Exercise 22, that if either H(x) ⩾0 or H(x) ⩽0 for all☞119\\nx, then the optimal importance sampling pdfoptimal\\nimportance\\nsampling pdf\\nis\\ng∗(x) = H(x) f (x)\\nµ . (3.22)\\nNamely, in this caseVarg∗bµ= Varg∗(H(X) f (X)/g(X)) = Varg∗µ= 0, so that the estimatorbµ\\nis constant under g∗. An obvious difficulty is that the evaluation of the optimal importance\\nsampling density g∗is usually not possible, since g∗(x) in (3.22) depends on the unknown\\nquantity µ. Nevertheless, one can typically choose a good importance sampling density g\\n“close” to the minimum variance density g∗.\\nOne of the main considerations for choosing a good importance sampling pdf is that'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 113, 'page_label': '96'}, page_content='One of the main considerations for choosing a good importance sampling pdf is that\\nthe estimator (3.19) should have finite variance. This is equivalent to the requirement\\nthat\\nEg\\n\"\\nH2(X) f 2(X)\\ng2(X)\\n#\\n= Ef\\n\"\\nH2(X) f (X)\\ng(X)\\n#\\n<∞. (3.23)\\nThis suggests that g should not have lighter tails than f and that, preferably, the\\nlikelihood ratio, f /g, should be bounded.\\n3.4 Monte Carlo for Optimization\\nIn this section we describe several Monte Carlo methods for optimization. Such random-\\nized algorithms can be useful for solving optimization problems with many local optima\\nand complicated constraints, possibly involving a mix of continuous and discrete variables.\\nRandomized algorithms are also used to solve noisy optimization problems, in which the\\nobjective function is unknown and has to be obtained via Monte Carlo simulation.\\n3.4.1 Simulated Annealing\\nSimulated annealing is a Monte Carlo technique for minimization that emulates the phys-Simulated'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 113, 'page_label': '96'}, page_content='Simulated annealing is a Monte Carlo technique for minimization that emulates the phys-Simulated\\nannealing ical state of atoms in a metal when the metal is heated up and then slowly cooled down.\\nWhen the cooling is performed very slowly, the atoms settle down to a minimum-energy\\nstate. Denoting the state as x and the energy of a state as S (x), the probability distribution\\nof the (random) states is described by the Boltzmann pdf\\nf (x) ∝e−S (x)\\nkT , x ∈X,\\nwhere k is Boltzmann’s constant and T is the temperature.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 114, 'page_label': '97'}, page_content='Monte Carlo Methods 97\\nGoing beyond the physical interpretation, suppose that S (x) is an arbitrary function to\\nbe minimized, with x taking values in some discrete or continuous set X. The Gibbs pdf Gibbs pdfcorresponding to S (x) is defined as\\nfT (x) = e−S (x)\\nT\\nzT\\n, x ∈X,\\nprovided that the normalization constant zT := P\\nx exp(−S (x)/T) is finite. Note that this\\nis simply the Boltzmann pdf with the Boltzmann constant k removed. As T →0, the pdf\\nbecomes more and more peaked around the set of global minimizers of S .\\nThe idea of simulated annealing is to create a sequence of pointsX1,X2,... that are ap-\\nproximately distributed according to pdfs fT1 (x), fT2 (x),... , where T1,T2,... is a sequence\\nof “temperatures” that decreases (is “cooled”) to 0 — known as the annealing schedule. If annealing\\nscheduleeach Xt were sampled exactly from fTt , then Xt would converge to a global minimum of\\nS (x) as Tt →0. However, in practice sampling isapproximate and convergence to a global'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 114, 'page_label': '97'}, page_content='S (x) as Tt →0. However, in practice sampling isapproximate and convergence to a global\\nminimum is not assured. A generic simulated annealing algorithm is as follows.\\nAlgorithm 3.4.1: Simulated Annealing\\ninput: Annealing schedule T0,T1,..., , function S , initial value x0.\\noutput: Approximations to the global minimizer x∗and minimum value S (x∗).\\n1 Set X0 ←x0 and t ←1.\\n2 while not stopping do\\n3 Approximately simulate Xt from fTt (x).\\n4 t ←t + 1\\n5 return Xt,S (Xt)\\nA popular annealing schedule is geometric cooling, where Tt = βTt−1, t = 1,2,... , for geometric\\ncoolinga given initial temperature T0 and a cooling factor β ∈(0,1). Appropriate values for T0\\ncooling factorand βare problem-dependent and this has traditionally required tuning on the part of the\\nuser. A possible stopping criterion is to stop after a fixed number of iterations, or when the\\ntemperature is “small enough”.\\nApproximate sampling from a Gibbs distribution is most often carried out via Markov'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 114, 'page_label': '97'}, page_content='Approximate sampling from a Gibbs distribution is most often carried out via Markov\\nchain Monte Carlo. For each iteration t, the Markov chain should theoretically run for a\\nlarge number of steps to accurately sample from the Gibbs pdf fTt . However, in practice,\\none often only runs a single step of the Markov chain, before updating the temperature, as\\nin Algorithm 3.4.2 below.\\nTo sample from a Gibbs distributionfT , this algorithm uses a random walk Metropolis–\\nHastings sampler. From (3.7), the acceptance probability of a proposal y is thus ☞ 80\\nα(x,y) = min\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\ne−1\\nT S (y)\\ne−1\\nT S (x)\\n, 1\\n\\uf8fc\\uf8f4\\uf8f4\\uf8fd\\uf8f4\\uf8f4\\uf8fe = min\\nn\\ne−1\\nT (S (y)−S (x)), 1\\no\\n.\\nHence, if S (y) < S (x), then the proposal is aways accepted. Otherwise, the proposal is\\naccepted with probability exp(−1\\nT (S (y) −S (x))).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 115, 'page_label': '98'}, page_content='98 Monte Carlo for Optimization\\nAlgorithm 3.4.2: Simulated Annealing with a Random Walk Sampler\\ninput: Objective function S , starting state X0, initial temperature T0, number of\\niterations N, symmetric proposal density q(y |x), constant β.\\noutput: Approximate minimizer and minimum value of S .\\n1 for t = 0 to N −1 do\\n2 Simulate a new state Y from the symmetric proposal q(y |Xt).\\n3 if S (Y) <S (Xt) then\\n4 Xt+1 ←Y\\n5 else\\n6 Draw U ∼U(0,1).\\n7 if U ⩽e−(S (Y)−S (Xt))/Tt then\\n8 Xt+1 ←Y\\n9 else\\n10 Xt+1 ←Xt\\n11 Tt+1 ←βTt\\n12 return XN and S (XN)\\nExample 3.15 (Simulated Annealing for Minimization) Let us minimize the “wig-\\ngly” function depicted in the bottom panel of Figure 3.10 and given by:\\nS (x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n−e−x2/100 sin(13x −x4)5 sin(1 −3x2)2, if −2 ⩽x ⩽2,\\n∞, otherwise.\\n0\\n1\\n2\\n3\\n4\\n5f (x) = e!S(x)=T\\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2\\nx\\n-1\\n0\\n1\\nS(x)\\nT = 1\\nT = 0:4\\nT = 0:2\\nFigure 3.10: Lower panel: the “wiggly” function S (x). Upper panel: three (normalized)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 115, 'page_label': '98'}, page_content='T = 0:2\\nFigure 3.10: Lower panel: the “wiggly” function S (x). Upper panel: three (normalized)\\nGibbs pdfs for temperatures T = 1,0.4,0.2. As the temperature decreases, the Gibbs pdf\\nconverges to the pdf that has all its mass concentrated at the minimizer of S .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 116, 'page_label': '99'}, page_content='Monte Carlo Methods 99\\nThe function has many local minima and maxima, with a global minimum around 1.4.\\nThe figure also illustrates the relationship betweenS and the (unnormalized) Gibbs pdf fT .\\nThe following Python code implements a slight variant of Algorithm 3.4.2 where, in-\\nstead of stopping after a fixed number of iterations, the algorithm stops when the temper-\\nature is lower than some threshold (here 10−3).\\nInstead of stopping after a fixed number N of iterations or when the temperature\\nis low enough, it is useful to stop when consecutive function values are closer than\\nsome distance εto each other, or when the best found function value has not changed\\nover a fixed number d of iterations.\\nFor a “current” state x, the proposal state Y is here drawn from the N(x,0.52) distri-\\nbution. We use geometric cooling with decay parameter β= 0.999 and initial temperature\\nT0 = 1. We set the initial state to x0 = 0. Figure 3.11 depicts a realization of the sequence'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 116, 'page_label': '99'}, page_content=\"T0 = 1. We set the initial state to x0 = 0. Figure 3.11 depicts a realization of the sequence\\nof states xt for t = 0,1,... . After initially fluctuating wildly, the sequence settles down\\nto a value around 1.37, with S (1.37) = −0.92, corresponding to the global optimizer and\\nminimum, respectively.\\nsimann.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\ndef wiggly(x):\\ny = -np.exp(x**2/100)*np.sin(13*x-x**4)**5*np.sin(1-3*x**2)**2\\nind = np.vstack((np.argwhere(x<-2),np.argwhere(x>2)))\\ny[ind]= float ('inf')\\nreturn y\\nS = wiggly\\nbeta = 0.999\\nsig = 0.5\\nT=1\\nx= np.array([0])\\nxx=[]\\nSx=S(x)\\nwhile T>10**(-3):\\nT=beta*T\\ny = x+sig*np.random.randn()\\nSy = S(y)\\nalpha = np.amin((np.exp(-(Sy-Sx)/T),1))\\nif np.random.uniform()<alpha:\\nx=y\\nSx=Sy\\nxx=np.hstack((xx,x))\\nprint ('minimizer = {:3.3f}, minimum ={:3.3f} '.format (x[0],Sx[0]))\\nplt.plot(xx)\\nplt.show()\\nminimizer = 1.365, minimum = -0.958\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 117, 'page_label': '100'}, page_content='100 Monte Carlo for Optimization\\n0\\n 1000\\n 2000\\n 3000\\n 4000\\n 5000\\n 6000\\n 7000\\nnumber of iterations\\n2.0\\n1.5\\n1.0\\n0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nstate\\nFigure 3.11: Typical states generated by the simulated annealing algorithm.\\n3.4.2 Cross-Entropy Method\\nThe cross-entropy (CE) method [103] is a simple Monte Carlo algorithm that can be usedcross-entropy\\nfor both optimization and estimation.\\nThe basic idea of the CE method for minimizing a function S on a set Xis to define\\na parametric family of probability densities {f (·|v),v ∈V} on Xand to iteratively update\\nthe parameter v so that f (·|v) places more mass on states x that have smaller S values than\\non the previous iteration. In particular, the CE algorithm has two basic phases:\\n• Sampling: Samples X1,..., XN are drawn independently according to f (·|v). The\\nobjective function S is evaluated at these points.\\n• Updating: A new parameter v′is selected on the basis of those Xi for which S (Xi) ⩽'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 117, 'page_label': '100'}, page_content='• Updating: A new parameter v′is selected on the basis of those Xi for which S (Xi) ⩽\\nγfor some level γ. These {Xi}form the elite sample set, E.elite sample\\nAt each iteration the level parameter γ is chosen as the worst of the Nelite := ⌈ϱN⌉\\nbest performing samples, where ϱ∈(0,1) is the rarity parameterrarity\\nparameter\\n— typically, ϱ= 0.1 or\\nϱ= 0.01. The parameter v is updated as a smoothed averageαv′+(1−α)v, where α∈(0,1)\\nis the smoothing parametersmoothing\\nparameter\\nand\\nv′:= argmax\\nv∈V\\nX\\nX∈E\\nln f (X |v). (3.24)\\nThe updating rule (3.24) is the result of minimizing the Kullback–Leibler divergence\\nbetween the conditional density of X ∼f (x |v) given S (X) ⩽γ, and f (x; v); see [103].\\nNote that (3.24) yields the maximum likelihood estimator (MLE) of v based on the elite☞456\\nsamples. Hence, for many specific families of distributions, explicit solutions can be found.\\nAn important example is where X ∼N(µ,diag(σ2)); that is, X has independent Gaussian'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 118, 'page_label': '101'}, page_content='Monte Carlo Methods 101\\ncomponents. In this case, the mean vector µ and the vector of variances σ2 are simply\\nupdated via the sample mean and sample variance of the elite samples. This is known as\\nnormal updating. A generic CE procedure for minimization is given in Algorithm 3.4.3. normal\\nupdating\\nAlgorithm 3.4.3: Cross-Entropy Method for Minimization\\ninput: Function S,initial sampling parameter v0, sample size N, rarity parameter\\nϱ, smoothing parameter α.\\noutput: Approximate minimum of S and optimal sampling parameter v.\\n1 Initialize v0, set Nelite ←⌈ϱN⌉and t ←0.\\n2 while a stopping criterion is not met do\\n3 t ←t + 1\\n4 Simulate an iid sample X1,..., XN from the density f (·|vt−1).\\n5 Evaluate the performances S (X1),..., S (XN) and sort them from smallest to\\nlargest: S (1),..., S (N).\\n6 Let γt be the sample ϱ-quantile of the performances:\\nγt ←S (Nelite). (3.25)\\n7 Determine the set of elite samples Et = {Xi : S (Xi) ⩽γt}.\\n8 Let v′\\nt be the MLE of the elite samples:\\nv′\\nt ←argmax\\nv\\nX\\nX∈Et'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 118, 'page_label': '101'}, page_content='8 Let v′\\nt be the MLE of the elite samples:\\nv′\\nt ←argmax\\nv\\nX\\nX∈Et\\nln f (X |v). (3.26)\\n9 Update the sampling parameter as\\nvt ←αv′\\nt + (1 −α)vt−1. (3.27)\\n10 return γt, vt\\nThe CE algorithm produces a sequence of pairs ( γ1,v1),(γ2,v2),..., such that γt con-\\nverges (approximately) to the minimal function value, and f (·|vt) to a degenerate pdf that\\n(approximately) concentrates all its mass at a minimizer of S , as t →∞. A possible stop-\\nping condition is to stop when the sampling distribution f (·|vt) is su fficiently close to a\\ndegenerate distribution. For normal updating this means that the standard deviation is suf-\\nficiently small.\\nThe output of the CE algorithm could also include the overall best function value\\nand corresponding solution.\\nIn the following example, we minimize the same function as in Example 3.15, but ☞ 97\\ninstead use the CE algorithm.\\nExample 3.16 (Cross-Entropy Method for Minimization) In this case we take the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 118, 'page_label': '101'}, page_content='Example 3.16 (Cross-Entropy Method for Minimization) In this case we take the\\nfamily of normal distributions{N(µ,σ2)}for the sampling step (Step 4 of Algorithm 3.4.3),\\nstarting with µ= 0 and σ= 3. The choice of the initial parameter is quite arbitrary, as long\\nas σis large enough to sample a wide range of points. We takeN = 100 samples at each it-\\neration, set ϱ= 0.1, and keep the Nelite = 10 = ⌈Nϱ⌉smallest ones as the elite samples. The\\nparameters µand σare then updated via the sample mean and sample standard deviation'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 119, 'page_label': '102'}, page_content='102 Monte Carlo for Optimization\\nof the elite samples. In this case we do not use any smoothing ( α = 1). In the following\\nPython code the 100 ×2 matrix Sx stores the x-values in the first column and the func-\\ntion values in the second column. The rows of this matrix are sorted in ascending order\\naccording to the function values, giving the matrix sortSx. The first Nelite = 10 rows of\\nthis sorted matrix correspond to the elite samples and their function values. The updating\\nof µ and σ is done in Lines 14 and 15. Figure 3.12 shows how the pdfs of the N(µt,σ2\\nt )\\nsampling distributions degenerate to the point mass at the global minimizer 1.366.\\nCEmethod.py\\nfrom simann import wiggly\\nimport numpy as np\\nnp.set_printoptions(precision=3)\\nmu, sigma = 0, 3\\nN, Nel = 100, 10\\neps = 10**-5\\nS = wiggly\\nwhile sigma > eps:\\nX = np.random.randn(N,1)*sigma + np.array(np.ones((N,1)))*mu\\nSx = np.hstack((X, S(X)))\\nsortSx = Sx[Sx[:,1].argsort(),]\\nElite = sortSx[0:Nel,:-1]\\nmu = np.mean(Elite, axis=0)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 119, 'page_label': '102'}, page_content=\"sortSx = Sx[Sx[:,1].argsort(),]\\nElite = sortSx[0:Nel,:-1]\\nmu = np.mean(Elite, axis=0)\\nsigma = np.std(Elite, axis=0)\\nprint ('S(mu)= {}, mu: {}, sigma: {}\\\\n '.format (S(mu), mu, sigma))\\nS(mu)= [0.071], mu: [0.414], sigma: [0.922]\\nS(mu)= [0.063], mu: [0.81], sigma: [0.831]\\nS(mu)= [-0.033], mu: [1.212], sigma: [0.69]\\nS(mu)= [-0.588], mu: [1.447], sigma: [0.117]\\nS(mu)= [-0.958], mu: [1.366], sigma: [0.007]\\nS(mu)= [-0.958], mu: [1.366], sigma: [0.]\\nS(mu)= [-0.958], mu: [1.366], sigma: [3.535e-05]\\nS(mu)= [-0.958], mu: [1.366], sigma: [2.023e-06]\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 120, 'page_label': '103'}, page_content='Monte Carlo Methods 103\\n-2 -1 0 1 2 3 x\\n0\\n0.5\\n1\\n1.5\\n2\\nf (x; 7; <)\\n5 \\n4 \\n2 \\n 1 iteration 0\\n3 \\nFigure 3.12: The normal pdfs of the first six sampling distributions, truncated to the interval\\n[−2,3]. The initial sampling distribution is N(0,32).\\n3.4.3 Splitting for Optimization\\nMinimizing a function S (x), x ∈X is closely related to drawing a random sample from a\\nlevel set of the form {x ∈X : S (x) ⩽γ}. Suppose S has minimum value γ∗attained at x∗. level set\\nAs long as γ ⩾γ∗, this level set contains the minimizer. Moreover, if γis close to γ∗, the\\nvolume of this level set will be small. So, a randomly selected point from this set is expected\\nto be close to x∗. Thus, by gradually decreasing the level parameter γ, the level sets will\\ngradually shrink towards the set {x∗}. Indeed, the CE method was developed with exactly\\nthis connection in mind; see, e.g., [102]. Note that the CE method employs a parametric'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 120, 'page_label': '103'}, page_content='this connection in mind; see, e.g., [102]. Note that the CE method employs a parametric\\nsampling distribution to obtain samples from the level sets (the elite samples). In [34]\\na non-parametric sampling mechanism is introduced that uses an evolving collection of\\nparticles. The resulting optimization algorithm, calledsplitting for continuous optimization splitting for\\ncontinuous\\noptimization(SCO), provides a fast and accurate way to optimize complicated continuous functions. The\\ndetails of SCO are given in Algorithm 3.4.4.\\nAt iteration t = 0, the algorithm starts with a population of particles Y0 = {Y1,..., YN}\\nthat are uniformly generated on some bounded region B, which is large enough to contain\\na global minimizer. The function values of all particles in Y0 are sorted, and the best\\nNelite = ⌈Nϱ⌉form the elite particle set X1, exactly as in the CE method. Next, the elite\\nparticles are “split” into ⌊N/Nelite⌋children particles, adding one extra child to some of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 120, 'page_label': '103'}, page_content='particles are “split” into ⌊N/Nelite⌋children particles, adding one extra child to some of\\nthe elite particles to ensure that the total number of children is again N. The purpose of\\nLine 4 is to randomize which elite particles receive an extra child. Lines 8–15 describe\\nhow the children of the i-th elite particle are generated. First, in Line 9, we select one\\nof the other elite particles uniformly at random. The same line defines an n-dimensional\\nvector σi whose components are the absolute differences between the vectors X(i) and X(I),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 121, 'page_label': '104'}, page_content='104 Monte Carlo for Optimization\\nAlgorithm 3.4.4: Splitting for Continuous Optimization (SCO)\\ninput: Objective function S , sample size N, rarity parameter ϱ, scale factor w,\\nbounded region B⊂X that is known to contain a global minimizer, and\\nmaximum number of attempts MaxTry.\\noutput: Final iteration number t and sequence (Xbest,1,b1),..., (Xbest,t,bt) of best\\nsolutions and function values at each iteration.\\n1 Simulate Y0 = {Y1,..., YN}uniformly on B. Set t ←0 and Nelite ←⌈Nϱ⌉.\\n2 while stopping condition is not satisfied do\\n3 Determine the Nelite smallest values, S (1) ⩽··· ⩽S (Nelite), of {S (X),X ∈Yt},\\nand store the corresponding vectors, X(1),..., X(Nelite), in Xt+1. Set bt+1 ←S (1)\\nand Xbest,t+1 ←X(1).\\n4 Draw Bi ∼Bernoulli(1\\n2 ), i = 1,..., Nelite, with PNelite\\ni=1 Bi = N mod Nelite.\\n5 for i = 1 to Nelite do\\n6 Ri ←\\nj\\nN\\nNelite\\nk\\n+ Bi // random splitting factor\\n7 Y ←X(i); Y′←Y\\n8 for j = 1 to Ri do\\n9 Draw I ∈{1,..., Nelite}\\\\{i}uniformly and let σi ←w|X(i) −X(I)|.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 121, 'page_label': '104'}, page_content='8 for j = 1 to Ri do\\n9 Draw I ∈{1,..., Nelite}\\\\{i}uniformly and let σi ←w|X(i) −X(I)|.\\n10 Simulate a uniform permutation π= (π1,...,π n) of (1,..., n).\\n11 for k = 1 to n do\\n12 for Try = 1 to MaxTrydo\\n13 Y′(πk) ←Y(πk) + σi(πk)Z, Z ∼N(0,1)\\n14 if S (Y′) <S (Y) then Y ←Y′and break.\\n15 Add Y to Yt+1\\n16 t ←t + 1\\n17 return {(Xbest,k,bk),k = 1,..., t}\\nmultiplied by a constant w. That is,\\nσi = w |X(i) −X(I)|:= w\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n|X(i),1 −X(I),1|\\n|X(i),2 −X(I),2|\\n...\\n|X(i),n −X(I),n|\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nNext, a uniform random permutation π of (1,..., n) is simulated (see Exercise 9). Lines☞115\\n11–14 describe how, starting from a candidate child point Y, each coordinate of Y is re-\\nsampled, in the order determined by π, by adding a standard normal random variable to\\nthat component, multiplied by the corresponding component of σi (Line 13). If the result-\\ning Y′ has a function value that is less than that of Y, then the new candidate is accepted.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 121, 'page_label': '104'}, page_content='ing Y′ has a function value that is less than that of Y, then the new candidate is accepted.\\nOtherwise, the same coordinate is tried again. If no improvement is found in MaxTry at-\\ntempts, the original component is retained. This process is performed for all elite samples,\\nto produce the first-generation population Y1. The procedure is then repeated for iterations\\nt = 1,2,... , until some stopping criterion is met, e.g., when the best found function value\\ndoes not change for a number of consecutive iterations, or when the total number of func-\\ntion evaluations exceeds some threshold. The best found function value and corresponding'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 122, 'page_label': '105'}, page_content='Monte Carlo Methods 105\\nargument (particle) are returned at the conclusion of the algorithm.\\nThe input variable MaxTry governs how much computational time is dedicated to up-\\ndating a component. In most cases we have encountered, the choices w = 0.5 and MaxTry\\n= 5 work well. Empirically, relatively high value for ϱwork well, such as ϱ = 0.4,0.8, or\\neven ϱ = 1. The latter case means that at each stage t all samples from Yt−1 carry over to\\nthe elite set Xt.\\nExample 3.17 (Test Problem 112) Hock and Schittkowski [58] provide a rich source\\nof test problems for multiextremal optimization. A challenging one is Problem 112, where\\nthe goal is to find x so as to minimize the function\\nS (x) =\\n10X\\nj=1\\nxj\\n \\ncj + ln xj\\nx1 + ··· + x10\\n!\\n,\\nsubject to the following set of constraints:\\nx1 + 2x2 + 2x3 + x6 + x10 −2 = 0,\\nx4 + 2x5 + x6 + x7 −1 = 0,\\nx3 + x7 + x8 + 2x9 + x10 −1 = 0,\\nxj ⩾ 0.000001, j = 1,..., 10,\\nwhere the constants {ci}are given in Table 3.1.\\nTable 3.1: Constants for Test Problem 112.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 122, 'page_label': '105'}, page_content='where the constants {ci}are given in Table 3.1.\\nTable 3.1: Constants for Test Problem 112.\\nc1 = −6.089 c2 = −17.164 c3 = −34.054 c4 = −5.914 c5 = −24.721\\nc6 = −14.986 c7 = −24.100 c8 = −10.708 c9 = −26.662 c10 = −22.179\\nThe best known minimal value in [58] was −47.707579. In [89] a better solution was\\nfound, −47.760765, using a genetic algorithm. The corresponding solution vector was\\ncompletely di fferent from the one in [58]. A further improvement, −47.76109081, was\\nfound in [70], using the CE method, giving a similar solution vector to that in [89]:\\n0.04067247 0.14765159 0.78323637 0.00141368 0.48526222\\n0.00069291 0.02736897 0.01794290 0.03729653 0.09685870\\nTo obtain a solution with SCO, we first converted this 10-dimensional problem into a\\n7-dimensional one by defining the objective function\\nS 7(y) = S (x),\\nwhere x2 = y1,x3 = y2,x5 = y3,x6 = y4,x7 = y5,x9 = y6,x10 = y7, and\\nx1 = 2 −(2y1 + 2y2 + y4 + x7),\\nx4 = 1 −(2y3 + y4 + y5),\\nx8 = 1 −(y2 + y5 + 2y6 + y7),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 122, 'page_label': '105'}, page_content='x1 = 2 −(2y1 + 2y2 + y4 + x7),\\nx4 = 1 −(2y3 + y4 + y5),\\nx8 = 1 −(y2 + y5 + 2y6 + y7),\\nsubject to x1,..., x10 ⩾0.000001, where the {xi}are taken as functions of the {yi}. We then\\nadopted a penalty approach (see Section B.4) by adding a penalty function to the original ☞ 415'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 123, 'page_label': '106'}, page_content='106 Monte Carlo for Optimization\\nobjective function:\\neS 7(y) = S (x) + 1000\\n10X\\ni=1\\nmax{−(xi −0.000001),0},\\nwhere, again, the {xi}are defined in terms of the {yi}as above.\\nOptimizing this last function with SCO, we found, in less time than the other al-\\ngorithms, a slightly smaller function value: −47.761090859365858, with solution vector\\n0.040668102417464 0.147730393049955 0.783153291185250 0.001414221643059\\n0.485246633088859 0.000693172682617 0.027399339496606 0.017947274343948\\n0.037314369272343 0.096871356429511\\nin line with the earlier solutions.\\n3.4.4 Noisy Optimization\\nIn noisy optimizationnoisy\\noptimization\\n, the objective function is unknown, but estimates of function val-\\nues are available, e.g., via simulation. For example, to find an optimal prediction function\\ng in supervised learning, the exact risk ℓ(g) = ELoss(Y,g(x)) is usually unknown and\\nonly estimates of the risk are available. Optimizing the risk is thus typically a noisy op-☞20'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 123, 'page_label': '106'}, page_content='only estimates of the risk are available. Optimizing the risk is thus typically a noisy op-☞20\\ntimization problem. Noisy optimization features prominently in simulation studies where\\nthe behavior of some system (e.g., vehicles on a road network) is simulated under certain\\nparameters (e.g., the lengths of the tra ffic light intervals) and the aim is to choose those\\nparameters optimally (e.g., to maximize the traffic throughput). For each parameter setting\\nthe exact value for the objective function is unknown but estimates can be obtained via the\\nsimulation.\\nIn general, suppose the goal is to minimize a function S , where S is unknown, but\\nan estimate of S (x) can be obtained for any choice of x ∈X. Because the gradient ∇S is\\nunknown, one cannot directly apply classical optimization methods. Thestochastic approx-\\nimation method mimics the classical gradient descent method by replacing a deterministicstochastic\\napproximation gradient with an estimate c∇S (x).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 123, 'page_label': '106'}, page_content='approximation gradient with an estimate c∇S (x).\\nA simple estimator for the i-th component of ∇S (x) (that is, ∂S (u)/∂xi), is the central\\ndifference estimatorcentral\\ndifference\\nestimator bS (x + ei δ/2) −bS (x −ei δ/2)\\nδ , (3.28)\\nwhere ei denotes the i-th unit vector, andbS (x+ei δ/2) and bS (x−ei δ/2) can be any estimators\\nof S (x + ei δ/2) and S (x −ei δ/2), respectively. The difference parameter δ >0 should be\\nsmall enough to reduce the bias of the estimator, but large enough to keep the variance of\\nthe estimator small.\\nTo reduce the variance in the estimator (3.28) it is important to have bS (x + ei δ/2)\\nand bS (x −ei δ/2) positively correlated. This can for example be achieved by using\\ncommon random numberscommon random\\nnumbers\\nin the simulation.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 124, 'page_label': '107'}, page_content='Monte Carlo Methods 107\\nIn direct analogy to gradient descent methods, the stochastic approximation method ☞ 412\\nproduces a sequence of iterates, starting with some x1 ∈X, via\\nxt+1 = xt −βt c∇S (xt), (3.29)\\nwhere β1,β2,... is a sequence of strictly positive step sizes. A generic stochastic approx-\\nimation algorithm for minimizing a function S is thus as follows.\\nAlgorithm 3.4.5: Stochastic Approximation\\ninput: A mechanism to estimate any gradient ∇S (x) and step sizes β1,β2,... .\\noutput: Approximate optimizer of S .\\n1 Initialize x1 ∈X. Set t ←1.\\n2 while a stopping criterion is not met do\\n3 Obtain an estimated gradient c∇S (xt) of S at xt.\\n4 Determine a step size βt.\\n5 Set xt+1 ←xt −βt c∇S (xt).\\n6 t ←t + 1\\n7 return xt\\nWhen c∇S (xt) is an unbiased estimator of ∇S (xt) in (3.29) the stochastic approxima-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 124, 'page_label': '107'}, page_content='7 return xt\\nWhen c∇S (xt) is an unbiased estimator of ∇S (xt) in (3.29) the stochastic approxima-\\ntion Algorithm 3.4.5 is referred to as the Robbins–Monro algorithm. When finite di ffer- Robbins–Monroences are used to estimate c∇S (xt), as in (3.28), the resulting algorithm is known as the\\nKiefer–Wolfowitz algorithm. In Section 9.4.1 we will see how stochastic gradient descent Kiefer–\\nWolfowitzis employed in deep learning to minimize the training loss, based on a “minibatch” of\\ntraining data. ☞ 335\\nIt can be shown [72] that, under certain regularity conditions on S , the sequence\\nx1,x2,... converges to the true minimizer x∗ when the step sizes decrease slowly enough\\nto 0; in particular, when\\n∞X\\nt=1\\nβt = ∞ and\\n∞X\\nt=1\\nβ2\\nt <∞. (3.30)\\nIn practice, one rarely uses step sizes that satisfy (3.30), as the convergence of the\\nsequence will be too slow to be of practical use.\\nAn alternative approach to stochastic approximation is the stochastic counterpart stochastic'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 124, 'page_label': '107'}, page_content='An alternative approach to stochastic approximation is the stochastic counterpart stochastic\\ncounterpartmethod, also called sample average approximation. It can be applied in situations where\\nthe noisy objective function is of the form\\nS (x) = EeS (x,ξ), x ∈X, (3.31)\\nwhere ξis a random vector that can be simulated andeS (x,ξ) can be evaluated exactly. The\\nidea is to replace the optimization of (3.31) with that of the sample average\\nbS (x) = 1\\nN\\nNX\\ni=1\\neS (x,ξi), x ∈X, (3.32)\\nwhere ξ1,..., ξN are iid copies of ξ. Note that bS is a deterministic function of x and so can\\nbe optimized using any optimization algorithm. A solution to this sample average version\\nis taken to be an estimator of a solution x∗to the original problem (3.31).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 125, 'page_label': '108'}, page_content='108 Monte Carlo for Optimization\\nExample 3.18 (Determining Good Importance Sampling Parameters) The selection\\nof good importance sampling parameters can be viewed as a stochastic optimization prob-\\nlem. Consider, for instance, the importance sampling estimator in Example 3.14. Recall☞94\\nthat the nominal distribution is the uniform distribution on the square [−b,b]2, with pdf\\nfb(x) = 1\\n(2b)2 , x ∈[−b,b]2,\\nwhere b is large enough to ensure thatµb is close to µ; in that example, we choseb = 1000.\\nThe importance sampling pdf is\\ngλ(x) = fR,Θ(r,θ)1\\nr = λe−λr 1\\n2π\\n1\\nr = λe−λ\\n√\\nx2\\n1+x2\\n2\\n2π\\nq\\nx2\\n1 + x2\\n2\\n, x = (x1,x2) ∈R2 \\\\{0},\\nwhich depends on a free parameter λ. In the example we chose λ = 0.1. Is this the best\\nchoice? Maybe λ= 0.05 or 0.2 would have resulted in a more accurate estimate. The im-\\nportant thing to realize is that the “e ffectiveness” of λ can be measured in terms of the\\nvariance of the estimator bµin (3.19), which is given by☞93\\n1\\nN Vargλ\\n \\nH(X) f (X)\\ngλ(X)\\n!\\n= 1\\nN Egλ\\n\"'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 125, 'page_label': '108'}, page_content='1\\nN Vargλ\\n \\nH(X) f (X)\\ngλ(X)\\n!\\n= 1\\nN Egλ\\n\"\\nH2(X) f 2(X)\\ng2\\nλ(X)\\n#\\n−µ2\\nN = 1\\nN Ef\\n\"\\nH2(X) f (X)\\ngλ(X)\\n#\\n−µ2\\nN .\\nHence, the optimal parameter λ∗ minimizes the function S (λ) = Ef [H2(X) f (X)/gλ(X)],\\nwhich is unknown, but can be estimated from simulation. To solve this stochastic minim-\\nization problem, we first use stochastic approximation. Thus, at each step of the algorithm,\\nthe gradient of S (λ) is estimated from realizations of bS (λ) = H2(X) f (X)/gλ(X), where\\nX ∼ fb. As in the original problem (that is, the estimation of µ), the parameter b should\\nbe large enough to avoid any bias in the estimator of λ∗, but also small enough to en-\\nsure a small variance. The following Python code implements a particular instance of Al-\\ngorithm 3.4.5. For sampling from fb here, we used b = 100 instead of b = 1000, as this will\\nimprove the crude Monte Carlo estimation ofλ∗, without noticeably affecting the bias. The'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 125, 'page_label': '108'}, page_content='improve the crude Monte Carlo estimation ofλ∗, without noticeably affecting the bias. The\\ngradient of S (λ) is estimated in Lines 11–17, using the central difference estimator (3.28).\\nNotice how for theS (λ−δ/2) and S (λ+δ/2) the same random vector X = [X1,X2]⊤is used.\\nThis significantly reduces the variance of the gradient estimator; see also Exercise 23. The☞119\\nstep size βt should be such that βt c∇S (xt) ≈λt. Given the large gradient here, we choose\\nβ0 = 10−7 and decrease it each step by a factor of 0 .99. Figure 3.13 shows how the se-\\nquence λ0,λ1,... decreases towards approximately 0 .125, which we take as an estimator\\nfor the optimal importance sampling parameter λ∗.\\nstochapprox.py\\nimport numpy as np\\nfrom numpy import pi\\nimport matplotlib.pyplot as plt\\nb=100 # choose b large enough , but not too large\\ndelta = 0.01\\nH = lambda x1, x2: (2*b)**2*np.exp(-np.sqrt(x1**2 + x2**2)/4)*(np.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 126, 'page_label': '109'}, page_content='Monte Carlo Methods 109\\nsin(2*np.sqrt(x1**2+x2**2)+1))*(x1**2+x2**2<b**2)\\nf = 1/(2*b)**2\\ng = lambda x1, x2, lam: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.\\nsqrt(x1**2+x2**2)/(2*pi)\\nbeta = 10**-7 #step size very small , as the gradient is large\\nlam=0.25\\nlams = np.array([lam])\\nN=10**4\\nfor i in range (200):\\nx1 = -b + 2*b*np.random.rand(N,1)\\nx2 = -b + 2*b*np.random.rand(N,1)\\nlamL = lam - delta/2\\nlamR = lam + delta/2\\nestL = np.mean(H(x1,x2)**2*f/g(x1, x2, lamL))\\nestR = np.mean(H(x1,x2)**2*f/g(x1, x2, lamR)) #use SAME x1,x2\\ngr = (estR-estL)/delta #gradient\\nlam = lam - gr*beta #gradient descend\\nlams = np.hstack((lams, lam))\\nbeta = beta*0.99\\nlamsize= range (0, (lams.size))\\nplt.plot(lamsize, lams)\\nplt.show()\\n0\\n 25\\n 50\\n 75\\n 100\\n 125\\n 150\\n 175\\n 200\\nsteps\\n0.12\\n0.14\\n0.16\\n0.18\\n0.20\\n0.22\\n0.24\\nFigure 3.13: The stochastic optimization algorithm produces a sequence λt,t = 0,1,2,...\\nthat tends to an approximate estimate of the optimal importance sampling parameter λ∗ ≈\\n0.125.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 126, 'page_label': '109'}, page_content='that tends to an approximate estimate of the optimal importance sampling parameter λ∗ ≈\\n0.125.\\nNext, we estimate λ∗using a stochastic counterpart approach. As the objective function\\nS (λ) is of the form (3.31) (with λtaking the role of x and X the role of ξ), we obtain the\\nsample average\\nbS (λ) = 1\\nN\\nNX\\ni=1\\nH2(Xi) f (Xi)\\ngλ(Xi), (3.33)\\nwhere X1,..., XN ∼iid fb. Once the X1,..., XN ∼iid fb have been simulated, bS (λ) is a de-\\nterministic function of λ, which can be optimized by any means. We take the most basic'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 127, 'page_label': '110'}, page_content='110 Monte Carlo for Optimization\\napproach and simply evaluate the function for λ = 0.01,0.02,..., 0.3 and select the min-\\nimizing λon this grid. The code is given below and Figure 3.14 shows bS (λ) as a function\\nof λ. The minimum value found was 0.60·104 for minimizerbλ∗= 0.12, which is in accord-\\nance with the value obtained via stochastic approximation. The sensitivity of this estimate\\ncan be assessed from the graph: for a wide range of values (say from 0.04 to 0.15) bS stays\\nrather flat. So any of these values could be used in an importance sampling procedure to\\nestimate µ. However, very small values (less than 0.02) and large values (greater than 0.25)\\nshould be avoided. Our original choice of λ= 0.1 was therefore justified and we could not\\nhave done much better.\\nstochcounterpart.py\\nfrom stochapprox import *\\nlams = np.linspace(0.01, 0.31, 1000)\\nres=[]\\nres = np.array(res)\\nfor i in range (lams.size):\\nlam = lams[i]\\nnp.random.seed(1)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 127, 'page_label': '110'}, page_content=\"res=[]\\nres = np.array(res)\\nfor i in range (lams.size):\\nlam = lams[i]\\nnp.random.seed(1)\\ng = lambda x1, x2: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.sqrt\\n(x1**2+x2**2)/(2*pi)\\nX=-b+2*b*np.random.rand(N,1)\\nY=-b+2*b*np.random.rand(N,1)\\nZ=H(X,Y)**2*f/g(X,Y)\\nestCMC = np.mean(Z)\\nres = np.hstack((res, estCMC))\\nplt.plot(lams, res)\\nplt.xlabel(r '$\\\\lambda$ ')\\nplt.ylabel(r '$\\\\hat{S}(\\\\lambda)$ ')\\nplt.ticklabel_format(style= 'sci', axis= 'y', scilimits=(0,0))\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 128, 'page_label': '111'}, page_content='Monte Carlo Methods 111\\n0.00\\n 0.05\\n 0.10\\n 0.15\\n 0.20\\n 0.25\\n 0.30\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0S( )\\n1e4\\nFigure 3.14: The stochastic counterpart method replaces the unknown S (λ) (that is, the\\nscaled variance of the importance sampling estimator) with its sample average, bS (λ). The\\nminimum value of bS is attained around λ= 0.12.\\nA third method for stochastic optimization is the cross-entropy method. In particular,\\nAlgorithm 3.4.3 can easily be modified to minimize noisy functions S (x) = EeS (x,ξ), as ☞ 101\\ndefined in (3.31). The only change required in the algorithm is that every function value\\nS (x) be replaced by its estimate bS (x). Depending on the level of noise in the function, the\\nsample size N might have to be increased considerably.\\nExample 3.19 (Cross-Entropy Method for Noisy Optimization) To explore the use\\nof the CE method for noisy optimization, take the following noisy discrete optimization\\nproblem. Suppose there is a “black box” that contains an unknown binary sequence of n'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 128, 'page_label': '111'}, page_content='problem. Suppose there is a “black box” that contains an unknown binary sequence of n\\nbits. If one feeds the black box any input vector, it will first scramble the input by inde-\\npendently flipping the bits (changing 0 to 1 and 1 to 0) with a probabilityθand then return\\nthe number of bits that do not match the true (unknown) binary sequence. This is illustrated\\nin Figure 3.15 for n = 10.\\nFigure 3.15: A noisy optimization function as a black box. The input to the black box is a\\nbinary vector. Inside the black box the digits of the input vector are scrambled by flipping\\nbits with probability θ. The output is the number of bits of the scrambled vector that do not\\nmatch the true (unknown) binary vector.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 129, 'page_label': '112'}, page_content='112 Monte Carlo for Optimization\\nDenoting by S (x) the true number of matching digits for a binary input vector x, the\\nblack box thus returns a noisy estimate bS (x). The objective is to estimate the binary se-\\nquence inside the black box, by feeding it with many input vectors and observing their\\noutput. Or, to put it in a different way, to minimize S (x) using bS (x) as a proxy. Since there\\nare 2n possible input vectors, it is infeasible to try all possible vectors x even for moderate\\nn.\\nThe following Python program implements the noisy function bS (x) for n = 100. Each\\ninput bit is flipped with a rather high probability θ= 0.4, so that the output is a poor indic-\\nator of how many bits actually match the true vector. This true vector has 1s at positions\\n1,..., 50 and 0s at 51,..., 100.\\nSnoisy.py\\nimport numpy as np\\ndef Snoisy(X): #takes a matrix\\nn = X.shape[1]\\nN = X.shape[0]\\n# true binary vector\\nxorg = np.hstack((np.ones((1,n//2)), np.zeros((1,n//2))))'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 129, 'page_label': '112'}, page_content='N = X.shape[0]\\n# true binary vector\\nxorg = np.hstack((np.ones((1,n//2)), np.zeros((1,n//2))))\\ntheta = 0.4 # probability to flip the input\\n# storing the number of bits unequal to the true vector\\ns = np.zeros(N)\\nfor i in range (0,N):\\n# determine which bits to flip\\nflip = (np.random.uniform(size=(n)) < theta).astype( int )\\nind = flip>0\\nX[i][ind] = 1-X[i][ind]\\ns[i] = (X[i] != xorg). sum ()\\nreturn s\\nThe CE code below to optimize S (x) is quite similar to the continuous optimization\\ncode in Example 3.16. However, instead of sampling iid random variablesX1,..., XN from☞101\\na normal distribution, we now sample iid binary vectorsX1,..., XN from a Ber(p) distribu-\\ntion. More precisely, given a row vector of probabilitiesp = [p1,..., pn], we independently\\nsimulate the components X1,..., Xn of each binary vector X according to Xi ∼Ber(pi),\\ni = 1,..., n. After each iteration, the vector p is updated as the (vector) mean of the elite'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 129, 'page_label': '112'}, page_content='i = 1,..., n. After each iteration, the vector p is updated as the (vector) mean of the elite\\nsamples. The sample size isN = 1000 and the number of elite samples is 100. The compon-\\nents of the initial sampling vectorp are all equal to 1/2; that is, theX are initially uniformly\\nsampled from the set of all binary vectors of length n = 100. At each subsequent iteration\\nthe parameter vector is updated via the mean of the elite samples and evolves towards a\\ndegenerate vector p∗with only 1s and 0s. Sampling from such a Ber(p∗) distribution gives\\nan outcome x∗= p∗, which can be taken as an estimate for the minimizer of S ; that is, the\\ntrue binary vector hidden in the black box. The algorithm stops when p has degenerated\\nsufficiently.\\nFigure 3.16 shows the evolution of the vector of probabilities p. This figure may be\\nseen as the discrete analogue of Figure 3.12. We see that, despite the high noise, the CE'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 129, 'page_label': '112'}, page_content='seen as the discrete analogue of Figure 3.12. We see that, despite the high noise, the CE\\nmethod is able to find the true state of the black box, and hence the minimum value of S .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 130, 'page_label': '113'}, page_content='Monte Carlo Methods 113\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0 10 20 30 40 50 60 70 80 90 100\\n0\\n0.5\\n1\\nFigure 3.16: Evolution of the vector of probabilities p = [p1,..., pn] towards the degener-\\nate solution.\\nCEnoisy.py\\nfrom Snoisy import Snoisy\\nimport numpy as np\\nn = 100\\nrho = 0.1\\nN = 1000; Nel = int (N*rho); eps = 0.01\\np = 0.5*np.ones(n)\\ni = 0\\npstart = p\\nps = np.zeros((1000,n))\\nps[0] = pstart\\npdist = np.zeros((1,1000))\\nwhile np.max (np.minimum(p,1-p)) > eps:\\ni += 1\\nX = (np.random.uniform(size=(N,n)) < p).astype( int )\\nX_tmp = np.array(X, copy=True)\\nSX = Snoisy(X_tmp)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 131, 'page_label': '114'}, page_content='114 Exercises\\nids = np.argsort(SX,axis=0)\\nElite = X[ids[0:Nel],:]\\np = np.mean(Elite,axis=0)\\nps[i] = p\\nprint (p)\\nFurther Reading\\nThe article [68] explores why the Monte Carlo method is so important in today’s quantitat-\\nive investigations. The Handbook of Monte Carlo Methods [71] provides a comprehensive\\noverview of Monte Carlo simulation that explores the latest topics, techniques, and real-\\nworld applications. Popular books on simulation and the Monte Carlo method include [42],\\n[75], and [104]. A classic reference on random variable generation is [32]. Easy introduc-\\ntions to stochastic simulation are given in [49], [98], and [100]. More advanced theory\\ncan be found in [5]. Markov chain Monte Carlo is detailed in [50] and [99]. The research\\nmonograph on the cross-entropy method is [103] and a tutorial is provided in [30]. A range\\nof optimization applications of the CE method is given in [16]. Theoretical results on ad-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 131, 'page_label': '114'}, page_content='of optimization applications of the CE method is given in [16]. Theoretical results on ad-\\naptive tuning schemes for simulated annealing may be found, for example, in [111]. There\\nare several established ways for gradient estimation. These include the finite di fference\\nmethod, infinitesimal perturbation analysis, the score function method, and the method of\\nweak derivatives; see, for example, [51, Chapter 7].\\nExercises\\n1. We can modify the Box–Muller method in Example 3.1 to draw X and Y uniformly☞69\\non the unit disc, {(x,y) ∈R2 : x2 +y2 ⩽1}, in the following way: Independently draw\\na radius R and an angle Θ ∼U(0,2π), and return X = R cos(Θ),Y = R sin(Θ). The\\nquestion is how to draw R.\\n(a) Show that the cdf of R is given by FR(r) = r2 for 0 ⩽r ⩽1 (with FR(r) = 0 and\\nFR(r) = 1 for r <0 and r >1, respectively).\\n(b) Explain how to simulate R using the inverse-transform method.\\n(c) Simulate 100 independent draws of [ X,Y]⊤according to the method described\\nabove.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 131, 'page_label': '114'}, page_content='(c) Simulate 100 independent draws of [ X,Y]⊤according to the method described\\nabove.\\n2. A simple acceptance–rejection method to simulate a vector X in the unit d-ball {x ∈\\nRd : ∥x∥⩽1}is to first generate X uniformly in the hyper cube [−1,1]d and then to\\naccept the point only if∥X∥⩽1. Determine an analytic expression for the probability\\nof acceptance as a function of d and plot this for d = 1,..., 50.\\n3. Let the random variable X have pdf\\nf (x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n1\\n2 x , 0 ⩽x <1,\\n1\\n2 , 1 ⩽x ⩽5\\n2 .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 132, 'page_label': '115'}, page_content='Monte Carlo Methods 115\\nSimulate a random variable from f (x), using\\n(a) the inverse-transform method;\\n(b) the acceptance–rejection method, using the proposal density\\ng(x) = 8\\n25 x , 0 ⩽x ⩽5\\n2 .\\n4. Construct simulation algorithms for the following distributions:\\n(a) The Weib(α,λ) distribution, with cdf F(x) = 1 −e−(λx)α\\n,x ⩾0, where λ> 0 and\\nα> 0.\\n(b) The Pareto(α,λ) distribution, with pdf f (x) = αλ(1 + λx)−(α+1),x ⩾0, where\\nλ> 0 and α> 0.\\n5. We wish to sample from the pdf\\nf (x) = x e−x, x ⩾0,\\nusing acceptance–rejection with the proposal pdf g(x) = e−x/2/2, x ⩾0.\\n(a) Find the smallest C for which Cg(x) ⩾f (x) for all x.\\n(b) What is the e fficiency of this acceptance–rejection method?\\n6. Let [ X,Y]⊤ be uniformly distributed on the triangle with corners (0 ,0),(1,2), and\\n(−1,1). Give the distribution of [U,V]⊤defined by the linear transformation\\n\"U\\nV\\n#\\n=\\n\"1 2\\n3 4\\n#\" X\\nY\\n#\\n.\\n7. Explain how to generate a random variable from the extreme value distribution ,\\nwhich has cdf'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 132, 'page_label': '115'}, page_content='.\\n7. Explain how to generate a random variable from the extreme value distribution ,\\nwhich has cdf\\nF(x) = 1 −e−exp( x−µ\\nσ ) , −∞< x <∞, (σ> 0),\\nvia the inverse-transform method.\\n8. Write a program that generates and displays 100 random vectors that are uniformly\\ndistributed within the ellipse\\n5 x2 + 21 x y + 25 y2 = 9.\\n[Hint: Consider generating uniformly distributed samples within the circle of radius\\n3 and use the fact that linear transformations preserve uniformity to transform the\\ncircle to the given ellipse.]\\n9. Suppose that Xi ∼Exp(λi), independently, for alli = 1,..., n. Let Π = [Π1,..., Πn]⊤\\nbe the random permutation induced by the ordering XΠ1 < XΠ2 < ··· < XΠn , and\\ndefine Z1 := XΠ1 and Zj := XΠj −XΠj−1 for j = 2,..., n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 133, 'page_label': '116'}, page_content='116 Exercises\\n(a) Determine an n ×n matrix A such that Z = AX and show that det(A) = 1.\\n(b) Denote the joint pdf of X and Π as\\nfX,Π(x,π) =\\nnY\\ni=1\\nλπi exp \\x00−λπi xπi\\n\\x01 ×1{xπ1 <··· < xπn }, x ⩾0, π∈Pn,\\nwhere Pn is the set of all n! permutations of {1,..., n}. Use the multivariate\\ntransformation formula (C.22) to show that☞432\\nfZ,Π(z,π) = exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−\\nnX\\ni=1\\nzi\\nX\\nk⩾i\\nλπk\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nnY\\ni=1\\nλi, z ⩾0, π∈Pn.\\nHence, conclude that the probability mass function of the random permutation\\nΠ is:\\nP[Π = π] =\\nnY\\ni=1\\nλπi\\nP\\nk⩾i λπk\\n, π∈Pn.\\n(c) Write pseudo-code to simulate a uniform random permutation Π ∈Pn; that is,\\nsuch that P[Π = π] = 1\\nn! , and explain how this uniform random permutation\\ncan be used to reshuffle a training set τn.\\n10. Consider the Markov chain with transition graph given in Figure 3.17, starting in\\nstate 1.\\nStart 0.5\\n1\\n0.8\\n0.9 0 .2\\n0.10.5\\n0.2\\n0.3\\n0.5\\n0.3\\n0.7\\n4\\n3\\n1\\n2 6\\n5\\nFigure 3.17: The transition graph for the Markov chain {Xt,t = 0,1,2,... }.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 133, 'page_label': '116'}, page_content='0.5\\n0.3\\n0.7\\n4\\n3\\n1\\n2 6\\n5\\nFigure 3.17: The transition graph for the Markov chain {Xt,t = 0,1,2,... }.\\n(a) Construct a computer program to simulate the Markov chain, and show a real-\\nization for N = 100 steps.\\n(b) Compute the limiting probabilities that the Markov chain is in state 1,2,. . . ,6,\\nby solving the global balance equations (C.42).☞452\\n(c) Verify that the exact limiting probabilities correspond to the average fraction\\nof times that the Markov process visits states 1,2,. . . ,6, for a large number of\\nsteps N.\\n11. As a generalization of Example C.9, consider a random walk on an arbitrary undir-☞453\\nected connected graph with a finite vertex set V. For any vertex v ∈V, let d(v) be'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 134, 'page_label': '117'}, page_content='Monte Carlo Methods 117\\nthe number of neighbors ofv — called the degree of v. The random walk can jump to\\neach one of the neighbors with probability 1/d(v) and can be described by a Markov\\nchain. Show that, if the chain is aperiodic, the limiting probability that the chain is\\nin state v is equal to d(v)/P\\nv′∈Vd(v′).\\n12. Let U,V ∼iid U(0,1). The reason why in Example 3.7 the sample mean and sample ☞ 76\\nmedian behave very di fferently is that E[U/V] = ∞, while the median of U/V is\\nfinite. Show this, and compute the median. [Hint: start by determining the cdf of\\nZ = U/V by writing it as an expectation of an indicator function.]\\n13. Consider the problem of generating samples from Y ∼Gamma(2,10).\\n(a) Direct simulation: Let U1,U2 ∼iid U(0,1). Show that−ln(U1)/10−ln(U2)/10 ∼\\nGamma(2,10). [Hint: derive the distribution of −ln(U1)/10 and use Ex-\\nample C.1.] ☞ 427\\n(b) Simulation via MCMC: Implement an independence sampler to simulate from\\nthe Gamma(2,10) target pdf\\nf (x) = 100 x e−10x, x ⩾0,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 134, 'page_label': '117'}, page_content='the Gamma(2,10) target pdf\\nf (x) = 100 x e−10x, x ⩾0,\\nusing proposal transition density q(y |x) = g(y), where g(y) is the pdf of an\\nExp(5) random variable. Generate N = 500 samples, and compare the true cdf\\nwith the empirical cdf of the data.\\n14. Let X = [X,Y]⊤be a random column vector with a bivariate normal distribution with\\nexpectation vector µ= [1,2]⊤and covariance matrix\\nΣ =\\n\"1 a\\na 4\\n#\\n.\\n(a) What are the conditional distributions of ( Y |X = x) and (X |Y = y)? [Hint: use\\nTheorem C.8.] ☞ 436\\n(b) Implement a Gibbs sampler to draw 103 samples from the bivariate distribution\\nN(µ,Σ) for a = 0, 1, and 1.75, and plot the resulting samples.\\n15. Here the objective is to sample from the 2-dimensional pdf\\nf (x,y) = c e−(xy+x+y), x ⩾0, y ⩾0,\\nfor some normalization constant c, using a Gibbs sampler. Let (X,Y) ∼f .\\n(a) Find the conditional pdf of X given Y = y, and the conditional pdf of Y given\\nX = x.\\n(b) Write working Python code that implements the Gibbs sampler and outputs'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 134, 'page_label': '117'}, page_content='X = x.\\n(b) Write working Python code that implements the Gibbs sampler and outputs\\n1000 points that are approximately distributed according to f .\\n(c) Describe how the normalization constant c could be estimated via Monte Carlo\\nsimulation, using random variables X1,..., XN,Y1,..., YN\\niid\\n∼Exp(1).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 135, 'page_label': '118'}, page_content='118 Exercises\\n16. We wish to estimate µ =\\nR 2\\n−2 e−x2/2 dx =\\nR\\nH(x) f (x) dx via Monte Carlo simulation\\nusing two di fferent approaches: (1) defining H(x) = 4 e−x2/2 and f the pdf of the\\nU[−2,2] distribution and (2) defining H(x) =\\n√\\n2π1{−2 ⩽x ⩽2}and f the pdf of\\nthe N(0,1) distribution.\\n(a) For both cases estimate µvia the estimator bµ\\nbµ= N−1\\nNX\\ni=1\\nH(Xi). (3.34)\\nUse a sample size of N = 1000.\\n(b) For both cases estimate the relative error κof bµusing N = 100.\\n(c) Give a 95% confidence interval for µfor both cases using N = 100.\\n(d) From part (b), assess how large N should be such that the relative width of the\\nconfidence interval is less than 0 .01, and carry out the simulation with this N.\\nCompare the result with the true value of µ.\\n17. Consider estimation of the tail probability µ= P[X ⩾γ] of some random variable X,\\nwhere γis large. The crude Monte Carlo estimator of µis\\nbµ= 1\\nN\\nNX\\ni=1\\nZi, (3.35)\\nwhere X1,..., XN are iid copies of X and Zi = 1{Xi ⩾γ}, i = 1,..., N.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 135, 'page_label': '118'}, page_content='bµ= 1\\nN\\nNX\\ni=1\\nZi, (3.35)\\nwhere X1,..., XN are iid copies of X and Zi = 1{Xi ⩾γ}, i = 1,..., N.\\n(a) Show that bµis unbiased; that is, Ebµ= µ.\\n(b) Express the relative error of bµ, i.e.,\\nRE =\\np\\nVarbµ\\nEbµ ,\\nin terms of N and µ.\\n(c) Explain how to estimate the relative error of bµ from outcomes x1,..., xN of\\nX1,..., XN, and how to construct a 95% confidence interval for µ.\\n(d) An unbiased estimator Z of µis said to be logarithmically efficient if\\nlim\\nγ→∞\\nln EZ2\\nln µ2 = 1. (3.36)\\nShow that the CMC estimator (3.35) withN = 1 is not logarithmically efficient.\\n18. One of the test cases in [70] involves the minimization of the Hougen function. Im-\\nplement a cross-entropy and a simulated annealing algorithm to carry out this optim-\\nization task.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 136, 'page_label': '119'}, page_content='Monte Carlo Methods 119\\n19. In the binary knapsack problem, the goal is to solve the optimization problem:\\nmax\\nx∈{0,1}n\\np⊤x,\\nsubject to the constraints\\nAx ⩽c,\\nwhere p and w are n ×1 vectors of non-negative numbers, A = (ai j) is an m ×n\\nmatrix, and c is an m ×1 vector. The interpretation is that xj = 1 or 0 depending\\non whether item j with value pj is packed into the knapsack or not , j = 1,..., n;\\nThe variable ai j represents the i-th attribute (e.g., volume, weight) of the j-th item.\\nAssociated with each attribute is a maximal capacity, e.g.,c1 could be the maximum\\nvolume of the knapsack, c2 the maximum weight, etc.\\nWrite a CE program to solve the Sento1.dat knapsack problem at http://peop\\nle.brunel.ac.uk/~mastjjb/jeb/orlib/files/mknap2.txt, as described in\\n[16].\\n20. Let ( C1,R1),(C2,R2),... be a renewal reward process, with ER1 <∞ and\\nEC1 <∞. Let At = PNt\\ni=1 Ri/t be the average reward at time t = 1,2,... , where\\nNt = max{n : Tn ⩽t}and we have defined Tn = Pn'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 136, 'page_label': '119'}, page_content='Nt = max{n : Tn ⩽t}and we have defined Tn = Pn\\ni=1 Ci as the time of the n-th re-\\nnewal.\\n(a) Show that Tn/n\\na.s.\\n−→EC1 as n →∞.\\n(b) Show that Nt\\na.s.\\n−→∞as t →∞.\\n(c) Show that Nt/t\\na.s.\\n−→1/EC1 as t →∞. [Hint: Use the fact that TNt ⩽t ⩽TNt+1 for\\nall t = 1,2,... .]\\n(d) Show that\\nAt\\na.s.\\n−→ER1\\nEC1\\nas t →∞.\\n21. Prove Theorem 3.3. ☞ 92\\n22. Prove that if H(x) ⩾0 the importance sampling pdf g∗ in (3.22) gives the zero- ☞ 96\\nvariance importance sampling estimator bµ= µ.\\n23. Let X and Y be random variables (not necessarily independent) and suppose we wish\\nto estimate the expected difference µ= E[X −Y] = EX −EY.\\n(a) Show that if X and Y are positively correlated, the variance of X −Y is smaller\\nthan if X and Y are independent.\\n(b) Suppose now that X and Y have cdfs F and G, respectively, and are\\nsimulated via the inverse-transform method: X = F−1(U), Y = G−1(V), with\\nU,V ∼U(0,1), not necessarily independent. Intuitively, one might expect that'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 136, 'page_label': '119'}, page_content='U,V ∼U(0,1), not necessarily independent. Intuitively, one might expect that\\nif U and V are positively correlated, the variance ofX−Y would be smaller than\\nif U and V are independent. Show that this is not always the case by providing\\na counter-example.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 137, 'page_label': '120'}, page_content='120 Exercises\\n(c) Continuing (b), assume now that F and G are continuous. Show that the vari-\\nance of X −Y by taking common random numbers U = V is no larger than\\nwhen U and V are independent. [Hint: Use the following lemma of Hoe ffding\\n[41]: If (X,Y) have joint cdf H with marginal cdfs of X and Y being F and G,\\nrespectively, then\\nCov(X,Y) =\\nZ ∞\\n−∞\\nZ ∞\\n−∞\\n(H(x,y) −F(x) G(y)) dx dy,\\nprovided Cov(X,Y) exists.]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 138, 'page_label': '121'}, page_content='CHAPTER 4\\nUNSUPERVISED LEARNING\\nWhen there is no distinction between response and explanatory variables, unsu-\\npervised methods are required to learn the structure of the data. In this chapter we\\nlook at various unsupervised learning techniques, such as density estimation, cluster-\\ning, and principal component analysis. Important tools in unsupervised learning in-\\nclude the cross-entropy training loss, mixture models, the Expectation–Maximization\\nalgorithm, and the Singular Value Decomposition.\\n4.1 Introduction\\nIn contrast to supervised learning, where an “output” (response) variable y is explained by\\nan “input” (explanatory) vector x, in unsupervised learning there is no response variable\\nand the overall goal is to extract useful information and patterns from the data, e.g., in\\nthe form τ= {x1,..., xn}or as a matrix X⊤= [x1,..., xn]. In essence, the objective of\\nunsupervised learning is to learn about the underlying probability distribution of the data.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 138, 'page_label': '121'}, page_content='unsupervised learning is to learn about the underlying probability distribution of the data.\\nWe start in Section 4.2 by setting up a framework for unsupervised learning that is\\nsimilar to the framework used for supervised learning in Section 2.3. That is, we formulate ☞ 23\\nunsupervised learning in terms of risk and loss minimization; but now involving the cross-\\nentropy risk, rather than the squared-error risk. In a natural way this leads to fundamental\\nlearning concepts such as likelihood, Fisher information, and the Akaike information cri-\\nterion. Section 4.3 introduces the Expectation–Maximization (EM) algorithm as a useful\\nmethod for maximizing likelihood functions when their solution cannot be found easily in\\nclosed form.\\nIf the data forms an iid sample from some unknown distribution, the “empirical dis-\\ntribution” of the data provides valuable information about the unknown distribution. In'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 138, 'page_label': '121'}, page_content='tribution” of the data provides valuable information about the unknown distribution. In\\nSection 4.4 we formalize the concept of the empirical distribution (a generalization of the\\nempirical cdf) and explain how we can produce an estimate of the underlying probability ☞ 11\\ndensity function of the data using kernel density estimators.\\nMost unsupervised learning techniques focus on identifying certain traits of the under-\\nlying distribution, such as its local maximizers. A related idea is to partition the data into\\nclusters of points that are in some sense “similar” to each other. In Section 4.5 we formu-\\nlate the clustering problem in terms of a mixture model. In particular, the data are assumed ☞ 135\\n121'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 139, 'page_label': '122'}, page_content='122 Risk and Loss in Unsupervised Learning\\nto come from a mixture of (usually Gaussian) distributions, and the objective is to recover\\nthe parameters of the mixture distributions from the data. The principal tool for parameter\\nestimation in mixture models is the EM algorithm.\\nSection 4.6 discusses a more heuristic approach to clustering, where the data are\\ngrouped according to certain “cluster centers”, whose positions are found by solving an\\noptimization problem. Section 4.7 describes how clusters can be constructed in a hierarch-\\nical manner.\\nFinally, in Section 4.8 we discuss the unsupervised learning technique called Principal\\nComponent Analysis (PCA), which is an important tool for reducing the dimensionality of\\nthe data.\\nWe will revisit various unsupervised learning techniques in subsequent chapters onsu-\\npervised learning. For example, cross-entropy training loss minimization will be important'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 139, 'page_label': '122'}, page_content='pervised learning. For example, cross-entropy training loss minimization will be important\\nin logistic regression (Section 5.7) and classification (Chapter 7), and PCA can be used☞204\\n☞251 for variable selection and dimensionality reduction, to make models easier to train and\\nincrease their predictive power; see e.g., Sections 6.8 and 7.4.\\n4.2 Risk and Loss in Unsupervised Learning\\nIn unsupervised learning, the training data T:= {X1,..., Xn}only consists of (what are\\nusually assumed to be) independent copies of a feature vector X; there is no response\\ndata. Suppose our objective is to learn the unknown pdf f of X based on an outcome\\nτ= {x1,..., xn}of the training data T. Conveniently, we can follow the same line of reas-\\noning as for supervised learning, discussed in Sections 2.3–2.5. Table 4.1 gives a summary☞23\\nof definitions for the case of unsupervised learning. Compare this with Table 2.1 for the\\nsupervised case.☞25'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 139, 'page_label': '122'}, page_content='supervised case.☞25\\nSimilar to supervised learning, we wish to find a functiong, which is now a probability\\ndensity (continuous or discrete), that best approximates the pdf f in terms of minimizing a\\nrisk\\nℓ(g) := ELoss( f (X),g(X)), (4.1)\\nwhere Loss is a loss function. In (2.27), we already encountered the Kullback–Leibler risk\\nℓ(g) := Eln f (X)\\ng(X) = Eln f (X) −Eln g(X). (4.2)\\nIf Gis a class of functions that contains f , then minimizing the Kullback–Leibler risk over\\nGwill yield the (correct) minimizer f . Of course, the problem is that minimization of (4.2)\\ndepends on f , which is generally not known. However, since the term Eln f (X) does not\\ndepend on g, it plays no role in the minimization of the Kullback–Leibler risk. By removing\\nthis term, we obtain the cross-entropy riskcross-entropy\\nrisk\\n(for discrete X replace the integral with a sum):\\nℓ(g) := −Eln g(X) = −\\nZ\\nf (x) ln g(x) dx. (4.3)\\nThus, minimizing the cross-entropy risk (4.3) over all g ∈G, again gives the minimizer'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 139, 'page_label': '122'}, page_content='Thus, minimizing the cross-entropy risk (4.3) over all g ∈G, again gives the minimizer\\nf , provided that f ∈G. Unfortunately, solving (4.3) is also infeasible in general, as it still'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 140, 'page_label': '123'}, page_content='Unsupervised Learning 123\\nTable 4.1: Summary of definitions for unsupervised learning.\\nx Fixed feature vector.\\nX Random feature vector.\\nf (x) Pdf of X evaluated at the point x.\\nτor τn Fixed training data {xi,i = 1,..., n}.\\nTor Tn Random training data {Xi,i = 1,..., n}.\\ng Approximation of the pdf f .\\nLoss( f (x),g(x)) Loss incurred when approximating f (x) with g(x).\\nℓ(g) Risk for approximation function g; that is, ELoss( f (X),g(X)).\\ngG Optimal approximation function in function class G; that is,\\nargming∈Gℓ(g).\\nℓτ(g) Training loss for approximation function (guess) g; that is,\\nthe sample average estimate of ℓ(g) based on a fixed training\\nsample τ.\\nℓT(g) The same as ℓτ(g), but now for a random training sample T.\\ngG\\nτ or gτ The learner: argming∈Gℓτ(g). That is, the optimal approxima-\\ntion function based on a fixed training set τand function class\\nG. We suppress the superscript Gif the function class is impli-\\ncit.\\ngG\\nT or gT The learner for a random training set T.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 140, 'page_label': '123'}, page_content='cit.\\ngG\\nT or gT The learner for a random training set T.\\ndepends on f . Instead, we seek to minimize the cross-entropy training loss cross-entropy\\ntraining loss\\n:\\nℓτ(g) := 1\\nn\\nnX\\ni=1\\nLoss( f (xi),g(xi)) = −1\\nn\\nnX\\ni=1\\nln g(xi) (4.4)\\nover the class of functionsG, where τ= {x1,..., xn}is an iid sample from f . This optimiz-\\nation is doable without knowing f and is equivalent to solving the maximization problem\\nmax\\ng∈G\\nnX\\ni=1\\nln g(xi). (4.5)\\nA key step in setting up the learning procedure is to select a suitable function class Gover\\nwhich to optimize. The standard approach is to parameterize g with a parameter θand let\\nGbe the class of functions{g(·|θ),θ∈Θ}for some p-dimensional parameter set Θ. For the\\nremainder of Section 4.2, we will be using this function class, as well as the cross-entropy\\nrisk.\\nThe function θ 7→g(x |θ) is called the likelihood function likelihood\\nfunction\\n. It gives the likelihood of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 140, 'page_label': '123'}, page_content='function\\n. It gives the likelihood of\\nthe observed feature vector x under g(·|θ), as a function of the parameter θ. The natural\\nlogarithm of the likelihood function is called the log-likelihood function and its gradient\\nwith respect to θis called the score function score function, denoted S(x |θ); that is,\\nS(x |θ) := ∂ln g(x |θ)\\n∂θ =\\n∂g(x |θ)\\n∂θ\\ng(x |θ). (4.6)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 141, 'page_label': '124'}, page_content='124 Risk and Loss in Unsupervised Learning\\nThe random score S(X |θ), with X ∼g(·|θ), is of particular interest. In many cases, its\\nexpectation is equal to the zero vector; namely,\\nEθS(X |θ) =\\nZ ∂g(x |θ)\\n∂θ\\ng(x |θ) g(x |θ) dx\\n=\\nZ ∂g(x |θ)\\n∂θ dx =\\n∂\\nR\\ng(x |θ) dx\\n∂θ = ∂1\\n∂θ = 0,\\n(4.7)\\nprovided that the interchange of differentiation and integration is justified. This is true for\\na large number of distributions, including the normal, exponential, and binomial distri-\\nbutions. Notable exceptions are distributions whose support depends on the distributional\\nparameter; for example the U(0,θ) distribution.\\nIt is important to see whether expectations are taken with respect to X ∼g(·|θ) or\\nX ∼f . We use the expectation symbols Eθ and Eto distinguish the two cases.\\nFrom now on we simply assume that the interchange of di fferentiation and integration\\nis permitted; see, e.g., [76] for su fficient conditions. The covariance matrix of the random'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 141, 'page_label': '124'}, page_content='is permitted; see, e.g., [76] for su fficient conditions. The covariance matrix of the random\\nscore S(X |θ) is called the Fisher information matrixFisher\\ninformation\\nmatrix\\n, which we denote by F or F(θ) to\\nshow its dependence on θ. Since the expected score is 0, we have\\nF(θ) = Eθ[S(X |θ) S(X |θ)⊤]. (4.8)\\nA related matrix is the expected Hessian matrix of −ln g(X |θ):☞398\\nH(θ) := E\\n\"\\n−∂S(X |θ)\\n∂θ\\n#\\n= −E\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂2 ln g(X |θ)\\n∂2θ1\\n∂2 ln g(X |θ)\\n∂θ1∂θ2\\n··· ∂2 ln g(X |θ)\\n∂θ1∂θp\\n∂2 ln g(X |θ)\\n∂θ2∂θ1\\n∂2 ln g(X |θ)\\n∂2θ2\\n··· ∂2 ln g(X |θ)\\n∂θ2∂θp\\n... ... ... ...\\n∂2 ln g(X |θ)\\n∂θp∂θ1\\n∂2 ln g(X |θ)\\n∂θp∂θ2\\n··· ∂2 ln g(X |θ)\\n∂2θp\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (4.9)\\nNote that the expectation here is with respect to X ∼f . It turns out that if f = g(·|θ), the\\ntwo matrices are the same; that is,\\nF(θ) = H(θ), (4.10)\\nprovided that we may swap the order of differentiation and integration (expectation). This\\nresult is called the information matrix equalityinformation\\nmatrix equality'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 141, 'page_label': '124'}, page_content='result is called the information matrix equalityinformation\\nmatrix equality\\n. We leave the proof as Exercise 1.\\nThe matrices F(θ) and H(θ) play important roles in approximating the cross-entropy\\nrisk for large n. To set the scene, let gG = g(·|θ∗) be the minimizer of the cross-entropy\\nrisk\\nr(θ) := −Eln g(X |θ).\\nWe assume thatr, as a function ofθ, is well-behaved; in particular, that in the neighborhood\\nof θ∗it is strictly convex and twice continuously differentiable (this holds true, for example,\\nif g is a Gaussian density). It follows that θ∗is a root of ES(X |θ), because\\n0 = ∂r(θ∗)\\n∂θ = −∂Eln g(X |θ∗)\\n∂θ = −E∂ln g(X |θ∗)\\n∂θ = −ES(X |θ∗),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 142, 'page_label': '125'}, page_content='Unsupervised Learning 125\\nagain provided that the order of di fferentiation and integration (expectation) can be\\nswapped. In the same way, H(θ) is then the Hessian matrix of r. Let g(·|bθn) be the minim-\\nizer of the training loss\\nrTn (θ) := −1\\nn\\nnX\\ni=1\\nln g(Xi |θ),\\nwhere Tn = {X1,..., Xn}is a random training set. Let r∗ be the smallest possible cross-\\nentropy risk, taken over all functions; clearly, r∗ = −Eln f (X), where X ∼ f . Similar to\\nthe supervised learning case, we can decompose the generalization risk,ℓ(g(·|bθn)) = r(bθn),\\ninto\\nr(bθn) = r∗+ r(θ∗) −r∗\\n|     {z     }\\napprox. error\\n+ r(bθn) −r(θ∗)|         {z         }\\nstatistical error\\n= r(θ∗) −Eln g(X |θ∗)\\ng(X |bθn)\\n.\\nThe following theorem specifies the asymptotic behavior of the components of the gener-\\nalization risk. In the proof we assume that bθn\\nP\\n−→θ∗as n →∞. ☞ 439\\nTheorem 4.1: Approximating the Cross-Entropy Risk\\nIt holds asymptotically (n →∞) that\\nEr(bθn) −r(θ∗) ≃tr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11\\n/(2n), (4.11)\\nwhere'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 142, 'page_label': '125'}, page_content='It holds asymptotically (n →∞) that\\nEr(bθn) −r(θ∗) ≃tr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11\\n/(2n), (4.11)\\nwhere\\nr(θ∗) ≃ErTn (bθn) + tr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11\\n/(2n). (4.12)\\nProof: A Taylor expansion of r(bθn) around θ∗gives the statistical error ☞ 400\\nr(bθn) −r(θ∗) = (bθn −θ∗)⊤∂r(θ∗)\\n∂θ| {z }\\n= 0\\n+1\\n2(bθn −θ∗)⊤H(θn)(bθn −θ∗), (4.13)\\nwhere θn lies on the line segment betweenθ∗andbθn. For largen we may replace H(θn) with\\nH(θ∗) as, by assumption, bθn converges to θ∗. The matrix H(θ∗) is positive definite because\\nr(θ) is strictly convex atθ∗by assumption, and therefore invertible. It is important to realize\\nthat bθn is in fact an M-estimator of θ∗. In particular, in the notation of Theorem C.19, we ☞ 449\\nhave ψ= S, A = H(θ∗), and B = F(θ∗). Consequently, by that same theorem,\\n√n (bθn −θ∗)\\nd\\n−→N\\n\\x10\\n0,H−1(θ∗) F(θ∗) H−⊤(θ∗)\\n\\x11\\n. (4.14)\\nCombining (4.13) with (4.14), it follows from Theorem C.2 that asymptotically the ☞ 430\\nexpected estimation error is given by (4.11).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 142, 'page_label': '125'}, page_content='expected estimation error is given by (4.11).\\nNext, we consider a Taylor expansion of rTn (θ∗) around bθn:\\nrTn (θ∗) = rTn (bθn) + (θ∗−bθn)⊤∂rTn (bθn)\\n∂θ|   {z   }\\n= 0\\n+1\\n2(θ∗−bθn)⊤HTn (θn)(θ∗−bθn), (4.15)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 143, 'page_label': '126'}, page_content='126 Risk and Loss in Unsupervised Learning\\nwhere HTn (θn) := −1\\nn\\nPn\\ni=1\\n∂S(Xi |θn)\\n∂θ is the Hessian of rTn (θ) at some θn between bθn and θ∗.\\nTaking expectations on both sides of (4.15), we obtain\\nr(θ∗) = ErTn (bθn) + 1\\n2E(θ∗−bθn)⊤HTn (θn)(θ∗−bθn).\\nReplacing HTn (θn) with H(θ∗) for large n and using (4.14), we have\\nn E(θ∗−bθn)⊤HTn (θn)(θ∗−bθn) −→tr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11\\n, n →∞.\\nTherefore, asymptotically as n →∞, we have (4.12). □\\nTheorem 4.1 has a number of interesting consequences:\\n1. Similar to Section 2.5.1, the training loss ℓTn (gTn ) = rTn (bθn) tends to underestimate the☞35\\nrisk ℓ(gG) = r(θ∗), because the training set Tn is used to both train g ∈G (that is, estimate\\nθ∗) and to estimate the risk. The relation (4.12) tells us that on average the training loss\\nunderestimates the true risk by tr(F(θ∗) H−1(θ∗))/(2n).\\n2. Adding equations (4.11) and (4.12), yields the following asymptotic approximation to\\nthe expected generalization risk:\\nEr(bθn) ≃ErTn (bθn) + 1\\nntr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 143, 'page_label': '126'}, page_content='the expected generalization risk:\\nEr(bθn) ≃ErTn (bθn) + 1\\nntr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11\\n(4.16)\\nThe first term on the right-hand side of (4.16) can be estimated (without bias) via the\\ntraining loss rTn (bθn). As for the second term, we have already mentioned that when the\\ntrue model f ∈G, then F(θ∗) = H(θ∗). Therefore, when Gis deemed to be a su fficiently\\nrich class of models parameterized by a p-dimensional vector θ, we may approximate the\\nsecond term as tr(F(θ∗)H−1(θ∗))/n ≈tr(Ip)/n = p/n. This suggests the following heuristic\\napproximation to the (expected) generalization risk:\\nEr(bθn) ≈rTn (bθn) + p\\nn . (4.17)\\n3. Multiplying both sides of (4.16) by 2n and substituting tr\\n\\x10\\nF(θ∗)H−1(θ∗)\\n\\x11\\n≈p, we obtain\\nthe approximation:\\n2n r(bθn) ≈−2\\nnX\\ni=1\\nln g(Xi |bθn) + 2p. (4.18)\\nThe right-hand side of (4.18) is called the Akaike information criterionAkaike\\ninformation\\ncriterion\\n(AIC). Just like\\n(4.17), the AIC approximation can be used to compare the difference in generalization risk'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 143, 'page_label': '126'}, page_content='(4.17), the AIC approximation can be used to compare the difference in generalization risk\\nof two or more learners. We prefer the learner with the smallest (estimated) generalization\\nrisk.\\nSuppose that, for a training set T, the training loss rT(θ) has a unique minimum point\\nbθwhich lies in the interior of Θ. If rT(θ) is a differentiable function with respect to θ, then\\nwe can find the optimal parameter bθby solving\\n∂rT(θ)\\n∂θ = 1\\nn\\nnX\\ni=1\\nS(Xi |θ)\\n|           {z           }\\nST(θ)\\n= 0.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 144, 'page_label': '127'}, page_content='Unsupervised Learning 127\\nIn other words, the maximum likelihood estimatebθfor θis obtained by solving the root of\\nthe average score function, that is, by solving\\nST(θ) = 0. (4.19)\\nIt is often not possible to find bθ in an explicit form. In that case one needs to solve the\\nequation (4.19) numerically. There exist many standard techniques for root-finding, e.g.,\\nvia Newton’s method (see Section B.3.1), whereby, starting from an initial guess θ0, sub- Newton’s\\nmethod\\n☞ 409\\nsequent iterates are obtained via the iterative scheme\\nθt+1 = θt + H−1\\nT (θt) ST(θt),\\nwhere\\nHT(θ) := −∂ST(θ)\\n∂θ = 1\\nn\\nnX\\ni=1\\n−∂S(Xi |θ)\\n∂θ\\nis the average Hessian matrix of {−ln g(Xi |θ)}n\\ni=1. Under f = g(·|θ), the expectation of\\nHT(θ) is equal to the information matrix F(θ), which does not depend on the data. This\\nsuggests an alternative iterative scheme, called Fisher’s scoring method Fisher’s\\nscoring method\\n:\\nθt+1 = θt + F−1(θt) ST(θt), (4.20)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 144, 'page_label': '127'}, page_content='scoring method\\n:\\nθt+1 = θt + F−1(θt) ST(θt), (4.20)\\nwhich is not only easier to implement (if the information matrix can be readily evaluated),\\nbut also is more numerically stable.\\nExample 4.1 (Maximum Likelihood for the Gamma Distribution) We wish to ap-\\nproximate the density of the Gamma(α∗,λ∗) distribution for some true but unknown para-\\nmeters α∗ and λ∗, on the basis of a training set τ = {x1,..., xn}of iid samples from this\\ndistribution. Choosing our approximating function g(·|α,λ) in the same class of gamma\\ndensities,\\ng(x |α,λ) = λαxα−1e−λx\\nΓ(α) , x ⩾0, (4.21)\\nwith α> 0 and λ> 0, we seek to solve (4.19). Taking the logarithm in (4.21), the log-\\nlikelihood function is given by\\nl(x |α,λ) := αln λ−ln Γ(α) + (α−1) ln x −λx.\\nIt follows that\\nS(α,λ) =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂\\n∂αl(x |α,λ)\\n∂\\n∂λl(x |α,λ)\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\"ln λ−ψ(α) + ln x\\nα\\nλ −x\\n#\\n,\\nwhere ψis the derivative of lnΓ: the so-called digamma function digamma\\nfunction\\n. Hence,\\nH(α,λ) = −E\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂2\\n∂α2 l(X |α,λ) ∂2\\n∂α∂λl(X |α,λ)\\n∂2\\n∂α∂λl(X |α,λ) ∂2'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 144, 'page_label': '127'}, page_content='function\\n. Hence,\\nH(α,λ) = −E\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂2\\n∂α2 l(X |α,λ) ∂2\\n∂α∂λl(X |α,λ)\\n∂2\\n∂α∂λl(X |α,λ) ∂2\\n∂λ2 l(X |α,λ)\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb = −E\\n\"−ψ′(α) 1\\nλ1\\nλ −α\\nλ2\\n#\\n=\\n\"ψ′(α) −1\\nλ\\n−1\\nλ\\nα\\nλ2\\n#\\n.\\nFisher’s scoring method (4.20) can now be used to solve (4.19), with\\nSτ(α,λ) =\\n\"ln λ−ψ(α) + n−1 Pn\\ni=1 ln xi\\nα\\nλ −n−1 Pn\\ni=1 xi\\n#\\nand F(α,λ) = H(α,λ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 145, 'page_label': '128'}, page_content='128 Expectation–Maximization (EM) Algorithm\\n4.3 Expectation–Maximization (EM) Algorithm\\nThe Expectation–Maximization algorithm (EM) is a general algorithm for maximization of\\ncomplicated (log-)likelihood functions, through the introduction of auxiliary variables.\\nTo simplify the notation in this section, we use a Bayesian notation system, where\\nthe same symbol is used for different (conditional) probability densities.\\nAs in the previous section, given independent observations τ= {x1,..., xn}from some\\nunknown pdf f , the objective is to find the best approximation to f in a function class\\nG= {g(·|θ),θ∈Θ}by solving the maximum likelihood problem:\\nθ∗= argmax\\nθ∈Θ\\ng(τ|θ), (4.22)\\nwhere g(τ|θ) : = g(x1 |θ) ··· g(xn |θ). The key element of the EM algorithm is the aug-\\nmentation of the data τwith a suitable vector of latent variableslatent\\nvariables\\n, z, such that\\ng(τ|θ) =\\nZ\\ng(τ,z |θ) dz.\\nThe function θ7→g(τ,z |θ) is usually referred to as the complete-data likelihoodcomplete-data\\nlikelihood'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 145, 'page_label': '128'}, page_content='likelihood\\nfunction.\\nThe choice of the latent variables is guided by the desire to make the maximization of\\ng(τ,z |θ) much easier than that of g(τ|θ).\\nSuppose p denotes an arbitrary density of the latent variables z. Then, we can write:\\nln g(τ|θ) =\\nZ\\np(z) lng(τ|θ) dz\\n=\\nZ\\np(z) ln\\n g(τ,z |θ)/p(z)\\ng(z |τ,θ)/p(z)\\n!\\ndz\\n=\\nZ\\np(z) ln\\n g(τ,z |θ)\\np(z)\\n!\\ndz −\\nZ\\np(z) ln\\n g(z |τ,θ)\\np(z)\\n!\\ndz\\n=\\nZ\\np(z) ln\\n g(τ,z |θ)\\np(z)\\n!\\ndz + D(p,g(·|τ,θ)), (4.23)\\nwhere D(p,g(·|τ,θ)) is the Kullback–Leibler divergence from the density p to g(·|τ,θ).☞42\\nSince D⩾0, it follows that\\nln g(τ|θ) ⩾\\nZ\\np(z) ln\\n g(τ,z |θ)\\np(z)\\n!\\ndz =: L(p,θ)\\nfor all θand any density p of the latent variables. In other words, L(p,θ) is a lower bound\\non the log-likelihood that involves the complete-data likelihood. The EM algorithm then\\naims to increase this lower bound as much as possible by starting with an initial guess θ(0)\\nand then, for t = 1,2,... , solving the following two steps:\\n1. p(t) = argmaxp L(p,θ(t−1)),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 146, 'page_label': '129'}, page_content='Unsupervised Learning 129\\n2. θ(t) = argmaxθ∈Θ L(p(t),θ).\\nThe first optimization problem can be solved explicitly. Namely, by (4.23), we have\\nthat\\np(t) = argmin\\np\\nD(p,g(·|τ,θ(t−1))) = g(·|τ,θ(t−1)).\\nThat is, the optimal density is the conditional density of the latent variables given the data\\nτand the parameter θ(t−1). The second optimization problem can be simplified by writing\\nL(p(t),θ) = Q(t)(θ) −Ep(t) ln p(t)(Z), where\\nQ(t)(θ) := Ep(t) ln g(τ,Z |θ)\\nis the expected complete-data log-likelihood under Z ∼p(t). Consequently, the maximiza-\\ntion of L(p(t),θ) with respect to θis equivalent to finding\\nθ(t) = argmax\\nθ∈Θ\\nQ(t)(θ).\\nThis leads to the following generic EM algorithm.\\nAlgorithm 4.3.1: Generic EM Algorithm\\ninput: Data τ, initial guess θ(0).\\noutput: Approximation of the maximum likelihood estimate.\\n1 t ←1\\n2 while a stopping criterion is not met do\\n3 Expectation Step: Find p(t)(z) := g(z |τ,θ(t−1)) and compute the expectation\\nQ(t)(θ) := Ep(t) ln g(τ,Z |θ). (4.24)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 146, 'page_label': '129'}, page_content='Q(t)(θ) := Ep(t) ln g(τ,Z |θ). (4.24)\\n4 Maximization Step: Let θ(t) ←argmaxθ∈Θ Q(t)(θ).\\n5 t ←t + 1\\n6 return θ(t)\\nA possible stopping criterion is to stop when\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\nln g(τ|θ(t)) −ln g(τ|θ(t−1))\\nln g(τ|θ(t))\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c ⩽ε\\nfor some small tolerance ε> 0.\\nRemark 4.1 (Properties of the EM Algorithm) The identity (4.23) can be used to\\nshow that the likelihood g(τ|θ(t)) does not decrease with every iteration of the algorithm.\\nThis property is one of the strengths of the algorithm. For example, it can be used to debug\\ncomputer implementations of the EM algorithm: if the likelihood is observed to decrease\\nat any iteration, then one has detected a bug in the program.\\nThe convergence of the sequence {θ(t)}to a global maximum (if it exists) is highly\\ndependent on the initial valueθ(0) and, in many cases, an appropriate choice ofθ(0) may not\\nbe clear. Typically, practitioners run the algorithm from di fferent random starting points\\nover Θ, to ascertain empirically that a suitable optimum is achieved.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 147, 'page_label': '130'}, page_content='130 Expectation–Maximization (EM) Algorithm\\nExample 4.2 (Censored Data) Suppose the lifetime (in years) of a certain type of\\nmachine is modeled via a N(µ,σ2) distribution. To estimate µ and σ2, the lifetimes of\\nn (independent) machines are recorded up to c years. Denote these censored lifetimes\\nby x1,..., xn. The {xi}are thus realizations of iid random variables {Xi}, distributed as\\nmin{Y,c}, where Y ∼N(µ,σ2).\\nBy the law of total probability (see (C.9)), the marginal pdf of each X can be written☞428\\nas:\\ng(x |µ,σ2) = Φ((c −µ)/σ)|          {z          }\\nP[Y<c]\\nφσ2 (x −µ)\\nΦ((c −µ)/σ)1{x <c}+ Φ((c −µ)/σ)|          {z          }\\nP[Y⩾c]\\n1{x = c},\\nwhere φσ2 (·) is the pdf of the N(0,σ2) distribution, Φ is the cdf of the standard normal\\ndistribution, and Φ := 1 −Φ. It follows that the likelihood of the data τ= {x1,..., xn}as a\\nfunction of the parameter θ:= [µ,σ2]⊤is:\\ng(τ|θ) =\\nY\\ni:xi<c\\nexp\\n\\x10\\n−(xi−µ)2\\n2σ2\\n\\x11\\n√\\n2πσ2\\n×\\nY\\ni:xi=c\\nΦ((c −µ)/σ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 147, 'page_label': '130'}, page_content='g(τ|θ) =\\nY\\ni:xi<c\\nexp\\n\\x10\\n−(xi−µ)2\\n2σ2\\n\\x11\\n√\\n2πσ2\\n×\\nY\\ni:xi=c\\nΦ((c −µ)/σ).\\nLet nc be the total number of xi such that xi = c. Using nc latent variables z = [z1,..., znc ]⊤,\\nwe can write the joint pdf:\\ng(τ,z |θ) = 1\\n(2πσ2)n/2 exp\\n \\n−\\nP\\ni:xi<c(xi −µ)2\\n2σ2 −\\nPnc\\ni=1(zi −µ)2\\n2σ2\\n!\\n1\\n\\x1a\\nmin\\ni\\nzi ⩾c\\n\\x1b\\n,\\nso that\\nR\\ng(τ,z |θ) dz = g(τ|θ). We can thus apply the EM algorithm to maximize the like-\\nlihood, as follows.\\nFor the E(xpectation)-step, we have for a fixed θ:\\ng(z |τ,θ) =\\nncY\\ni=1\\ng(zi |τ,θ),\\nwhere g(z |τ,θ) = 1{z ⩾c}φσ2 (z −µ)/Φ((c −µ)/σ) is simply the pdf of the N(µ,σ2)\\ndistribution, truncated to [c,∞).\\nFor the M(aximization)-step, we compute the expectation of the complete log-\\nlikelihood with respect to a fixed g(z |τ,θ) and use the fact that Z1,..., Znc are iid:\\nEln g(τ,Z |θ) = −\\nP\\ni:xi<c(xi −µ)2\\n2σ2 −ncE(Z −µ)2\\n2σ2 −n\\n2 ln σ2 −n\\n2 ln(2π),\\nwhere Z has a N(µ,σ2) distribution, truncated to [ c,∞). To maximize the last expression'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 147, 'page_label': '130'}, page_content='2 ln(2π),\\nwhere Z has a N(µ,σ2) distribution, truncated to [ c,∞). To maximize the last expression\\nwith respect to µwe set the derivative with respect to µto zero, and obtain:\\nµ= ncEZ + P\\ni:xi<c xi\\nn .\\nSimilarly, setting the derivative with respect toσ2 to zero gives:\\nσ2 = ncE(Z −µ)2 + P\\ni:xi<c(xi −µ)2\\nn .\\nIn summary, the EM iterates for t = 1,2,... are as follows.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 148, 'page_label': '131'}, page_content='Unsupervised Learning 131\\nE-step. Given the current estimate θt := [µt,σ2\\nt ]⊤, compute the expectations νt := EZ and\\nζ2\\nt := E(Z −µt)2, where Z ∼N(µt,σ2\\nt ), conditional on Z ⩾c; that is,\\nνt := µt + σ2\\nt\\nφσ2\\nt\\n(c −µt)\\nΦ((c −µt)/σt)\\nζ2\\nt := σ2\\nt\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 + (c −µt)\\nφσ2\\nt\\n(c −µt)\\nΦ((c −µt)/σt)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.\\nM-step. Update the estimate to θt+1 := [µt+1,σ2\\nt+1]⊤via the formulas:\\nµt+1 = ncνt + P\\ni:xi<c xi\\nn\\nσ2\\nt+1 = ncζ2\\nt + P\\ni:xi<c(xi −µt+1)2\\nn .\\n4.4 Empirical Distribution and Density Estimation\\nIn Section 1.5.2.3 we saw how the empirical cdf bFn, obtained from an iid training set ☞ 11\\nτ= {x1,..., xn}from an unknown distribution on R, gives an estimate of the unknown cdf\\nF of this sampling distribution. The function bFn is a genuine cdf, as it is right-continuous,\\nincreasing, and lies between 0 and 1. The corresponding discrete probability distribution\\nis called the empirical distribution empirical\\ndistribution\\nof the data. A random variable X distributed according'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 148, 'page_label': '131'}, page_content='distribution\\nof the data. A random variable X distributed according\\nto this empirical distribution takes the values x1,..., xn with equal probability 1 /n. The\\nconcept of empirical distribution naturally generalizes to higher dimensions: a random\\nvector X that is distributed according to the empirical distribution ofx1,..., xn has discrete\\npdf P[X = xi] = 1/n,i = 1,..., n. Sampling from such a distribution — in other words\\nresampling the original data — was discussed in Section 3.2.4. The preeminent usage of ☞ 76\\nsuch sampling is the bootstrap method, discussed in Section 3.3.2. ☞ 88\\nIn a way, the empirical distribution is the natural answer to the unsupervised learning\\nquestion: what is the underlying probability distribution of the data? However, the empir-\\nical distribution is, by definition, a discrete distribution, whereas the true sampling distri-\\nbution might be continuous. For continuous data it makes sense to also consider estimation'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 148, 'page_label': '131'}, page_content='bution might be continuous. For continuous data it makes sense to also consider estimation\\nof the pdf of the data. A common approach is to estimate the density via a kernel density\\nestimate (KDE), the most prevalent learner to carry this out is given next.\\nDefinition 4.1: Gaussian KDE\\nLet x1,..., xn ∈Rd be the outcomes of an iid sample from a continuous pdf f . A\\nGaussian kernel density estimate Gaussian\\nkernel density\\nestimate\\nof f is a mixture of normal pdfs, of the form\\ngτn (x |σ) = 1\\nn\\nnX\\ni=1\\n1\\n(2π)d/2σd e−∥x−xi∥2\\n2σ2 , x ∈Rd, (4.25)\\nwhere σ> 0 is called the bandwidth.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 149, 'page_label': '132'}, page_content='132 Empirical Distribution and Density Estimation\\nWe see that gτn in (4.25) is the average of a collection of n normal pdfs, where each\\nnormal distribution is centered at the data pointxi and has covariance matrixσ2Id. A major\\nquestion is how to choose the bandwidth σso as to best approximate the unknown pdf f .\\nChoosing very small σ will result in a “spiky” estimate, whereas a large σ will produce\\nan over-smoothed estimate that may not identify important peaks that are present in the\\nunknown pdf. Figure 4.1 illustrates this phenomenon. In this case the data are comprised\\nof 20 points uniformly drawn from the unit square. The true pdf is thus 1 on [0 ,1]2 and 0\\nelsewhere.\\nFigure 4.1: Two two-dimensional Gaussian KDEs, withσ= 0.01 (left) andσ= 0.1 (right).\\nLet us write the Gaussian KDE in (4.25) as\\ngτn (x |σ) = 1\\nn\\nnX\\ni=1\\n1\\nσd ϕ\\n\\x12x −xi\\nσ\\n\\x13\\n, (4.26)\\nwhere\\nϕ(z) = 1\\n(2π)d/2 e−∥z∥2\\n2 , z ∈Rd (4.27)\\nis the pdf of the d-dimensional standard normal distribution. By choosing a different prob-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 149, 'page_label': '132'}, page_content='is the pdf of the d-dimensional standard normal distribution. By choosing a different prob-\\nability density ϕin (4.26), satisfying ϕ(x) = ϕ(−x) for all x, we can obtain a wide variety\\nof kernel density estimates. A simple pdf ϕis, for example, the uniform pdf on [−1,1]d:\\nϕ(z) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n2−d, if z ∈[−1,1]d,\\n0, otherwise.\\nFigure 4.2 shows the graph of the corresponding KDE, using the same data as in Figure 4.1\\nand with bandwidth σ = 0.1. We observe qualitatively similar behavior for the Gaussian\\nand uniform KDEs. As a rule, the choice of the functionϕis less important than the choice\\nof the bandwidth in determining the quality of the estimate.\\nThe important issue of bandwidth selection has been extensively studied for one-\\ndimensional data. To explain the ideas, we use our usual setup and let τ = {x1,..., xn}\\nbe the observed (one-dimensional) data from the unknown pdf f . First, we define the loss\\nfunction as\\nLoss( f (x),g(x)) = ( f (x) −g(x))2\\nf (x) . (4.28)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 150, 'page_label': '133'}, page_content='Unsupervised Learning 133\\nFigure 4.2: A two-dimensional uniform KDE, with bandwidth σ= 0.1.\\nThe risk to minimize is thus ℓ(g) := Ef Loss( f (X),g(X)) =\\nR\\n( f (x) −g(x))2 dx.We bypass\\nthe selection of a class of approximation functions by choosing the learner to be specified\\nby (4.25) for a fixed σ. The objective is now to find a σthat minimizes the generalization\\nrisk ℓ(gτ(·|σ)) or the expected generalization risk Eℓ(gT(·|σ)). The generalization risk is\\nin this caseZ\\n( f (x) −gτ(x |σ))2 dx =\\nZ\\nf 2(x) dx −2\\nZ\\nf (x)gτ(x |σ) dx +\\nZ\\ng2\\nτ(x |σ) dx.\\nMinimizing this expression with respect toσis equivalent to minimizing the last two terms,\\nwhich can be written as\\n−2 Ef gτ(X |σ) +\\nZ \\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n1\\nn\\nnX\\ni=1\\n1\\nσϕ\\n\\x12x −xi\\nσ\\n\\x13\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n2\\ndx.\\nThis expression in turn can be estimated by using a test sample{x′\\n1 ..., x′\\nn′}from f , yielding\\nthe following minimization problem:\\nmin\\nσ\\n−2\\nn′\\nn′\\nX\\ni=1\\ngτ(x′\\ni |σ) + 1\\nn2\\nnX\\ni=1\\nnX\\nj=1\\nZ 1\\nσ2 ϕ\\n\\x12x −xi\\nσ\\n\\x13\\nϕ\\n\\x12x −xj\\nσ\\n\\x13\\ndx,\\nwhere\\nR 1\\nσ2 ϕ\\n\\x10x−xi\\nσ\\n\\x11\\nϕ\\n\\x10x−xj\\nσ\\n\\x11'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 150, 'page_label': '133'}, page_content='n2\\nnX\\ni=1\\nnX\\nj=1\\nZ 1\\nσ2 ϕ\\n\\x12x −xi\\nσ\\n\\x13\\nϕ\\n\\x12x −xj\\nσ\\n\\x13\\ndx,\\nwhere\\nR 1\\nσ2 ϕ\\n\\x10x−xi\\nσ\\n\\x11\\nϕ\\n\\x10x−xj\\nσ\\n\\x11\\ndx = 1√\\n2σϕ\\n\\x10xi−xj√\\n2σ\\n\\x11\\nin the case of the Gaussian kernel (4.27) with\\nd = 1. To estimate σin this way clearly requires a test sample, or at least an application of ☞ 38\\ncross-validation. Another approach is to minimize the expected generalization risk, (that\\nis, averaged over all training sets):\\nE\\nZ\\n( f (x) −gT(x |σ))2 dx.\\nThis is called the mean integrated squared error mean integrated\\nsquared error\\n(MISE). It can be decomposed into an\\nintegrated squared bias and integrated variance component:\\nZ\\n( f (x) −EgT(x |σ))2 dx +\\nZ\\nVar(gT(x |σ)) dx.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 151, 'page_label': '134'}, page_content='134 Empirical Distribution and Density Estimation\\nA typical analysis now proceeds by investigating how the MISE behaves for largen, under\\nvarious assumptions on f . For example, it is shown in [114] that, for σ→0 and nσ→∞,\\nthe asymptotic approximation to the MISE of the Gaussian kernel density estimator (4.25)\\n(for d = 1) is given by\\n1\\n4 σ4 ∥f ′′∥2 + 1\\n2n\\n√\\nπσ2\\n, (4.29)\\nwhere ∥f ′′∥2 :=\\nR\\n( f ′′(x))2 dx. The asymptotically optimal value of σis the minimizer\\nσ∗:=\\n 1\\n2n √π∥f ′′∥2\\n!1/5\\n. (4.30)\\nTo compute the optimal σ∗ in (4.30), one needs to estimate the functional ∥f ′′∥2. The\\nGaussian rule of thumbGaussian rule\\nof thumb\\nis to assume that f is the density of theN(x,s2) distribution, where\\nx and s2 are the sample mean and variance of the data, respectively [113]. In this case\\n∥f ′′∥2 = s−5π−1/23/8 and the Gaussian rule of thumb becomes:\\nσrot =\\n 4 s5\\n3 n\\n!1/5\\n≈1.06 s n−1/5.\\nWe recommend, however, the fast and reliable theta KDEtheta KDE of [14], which chooses the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 151, 'page_label': '134'}, page_content='We recommend, however, the fast and reliable theta KDEtheta KDE of [14], which chooses the\\nbandwidth in an optimal way via a fixed-point procedure. Figures 4.1 and 4.2 illustrate a\\ncommon problem with traditional KDEs: for distributions on a bounded domain, such as\\nthe uniform distribution on [0,1]2, the KDE assigns positive probability mass outside this\\ndomain. An additional advantage of the theta KDE is that it largely avoids this boundary\\neffect. We illustrate the theta KDE with the following example.\\nExample 4.3 (Comparison of Gaussian and theta KDEs) The following Python pro-\\ngram draws an iid sample from the Exp(1) distribution and constructs a Gaussian kernel\\ndensity estimate. We see in Figure 4.3 that with an appropriate choice of the bandwidth\\na good fit to the true pdf can be achieved, except at the boundary x = 0. The theta KDE\\ndoes not exhibit this boundary e ffect. Moreover, it chooses the bandwidth automatically,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 151, 'page_label': '134'}, page_content='does not exhibit this boundary e ffect. Moreover, it chooses the bandwidth automatically,\\nto achieve a superior fit. The theta KDE source code is available as kde.py on the book’s\\nGitHub site.\\n0\\n 1\\n 2\\n 3\\n 4\\n 5\\n 6\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n Gaussian KDE\\nTheta KDE\\nTrue density\\nFigure 4.3: Kernel density estimates for Exp(1)-distributed data.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 152, 'page_label': '135'}, page_content='Unsupervised Learning 135\\ngausthetakde.py\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom kde import *\\nsig = 0.1; sig2 = sig**2; c = 1/np.sqrt(2*np.pi)/sig #Constants\\nphi = lambda x,x0: np.exp(-(x-x0)**2/(2*sig2)) #Unscaled Kernel\\nf = lambda x: np.exp(-x)*(x >= 0) # True PDF\\nn = 10**4 # Sample Size\\nx = -np.log(np.random.uniform(size=n)) # Generate Data via IT method\\nxx = np.arange(-0.5,6,0.01, dtype = \"d\") # Plot Range\\nphis = np.zeros( len (xx))\\nfor i in range (0,n):\\nphis = phis + phi(xx,x[i])\\nphis = c*phis/n\\nplt.plot(xx,phis, \\'r\\')# Plot Gaussian KDE\\n[bandwidth,density,xmesh,cdf] = kde(x,2**12,0, max (x))\\nidx = (xmesh <= 6)\\nplt.plot(xmesh[idx],density[idx]) # Plot Theta KDE\\nplt.plot(xx,f(xx)) # Plot True PDF\\n4.5 Clustering via Mixture Models\\nClustering is concerned with the grouping of unlabeled feature vectors into clusters, such\\nthat samples within a cluster are more similar to each other than samples belonging to'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 152, 'page_label': '135'}, page_content='that samples within a cluster are more similar to each other than samples belonging to\\ndifferent clusters. Usually, it is assumed that the number of clusters is known in advance,\\nbut otherwise no prior information is given about the data. Applications of clustering can\\nbe found in the areas of communication, data compression and storage, database searching,\\npattern matching, and object recognition.\\nA common approach to clustering analysis is to assume that the data comes from a mix-\\nture of (usually Gaussian) distributions, and thus the objective is to estimate the parameters\\nof the mixture model by maximizing the likelihood function for the data. Direct optimiza-\\ntion of the likelihood function in this case is not a simple task, due to necessary constraints\\non the parameters (more about this later) and the complicated nature of the likelihood func-\\ntion, which in general has a great number of local maxima and saddle-points. A popular'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 152, 'page_label': '135'}, page_content='tion, which in general has a great number of local maxima and saddle-points. A popular\\nmethod to estimate the parameters of the mixture model is the EM algorithm, which was\\ndiscussed in a more general setting in Section 4.3. In this section we explain the basics of ☞ 128\\nmixture modeling and explain the workings of the EM method in this context. In addition,\\nwe show how direct optimization methods can be used to maximize the likelihood.\\n4.5.1 Mixture Models\\nLet T := {X1,..., Xn}be iid random vectors taking values in some set X⊆ Rd, each Xi\\nbeing distributed according to the mixture density mixture density\\ng(x |θ) = w1ϕ1(x) + ··· + wKϕK(x), x ∈X, (4.31)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 153, 'page_label': '136'}, page_content='136 Clustering via Mixture Models\\nwhere ϕ1,...,ϕ K are probability densities (discrete or continuous) on X, and the positive\\nweights w1,..., wK sum up to 1. This mixture pdf can be interpreted in the following way.weights\\nLet Z be a discrete random variable taking values 1,2,..., K with probabilities w1,..., wK,\\nand let X be a random vector whose conditional pdf, givenZ = z, is ϕz. By the product rule\\n(C.17), the joint pdf of Z and X is given by☞431\\nϕZ,X(z,x) = ϕZ(z) ϕX |Z(x |z) = wz ϕz(x)\\nand the marginal pdf of X is found by summing the joint pdf over the values of z, which\\ngives (4.31). A random vector X ∼g can thus be simulated in two steps:\\n1. First, draw Z according to the probabilities P[Z = z] = wz, z = 1,..., K.\\n2. Then draw X according to the pdf ϕZ.\\nAs Tonly contain the {Xi}variables, the {Zi}are viewed as latent variables. We can inter-\\npret Zi as the hidden label of the cluster to which Xi belongs.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 153, 'page_label': '136'}, page_content='pret Zi as the hidden label of the cluster to which Xi belongs.\\nTypically, eachϕk in (4.31) is assumed to be known up to some parameter vector ηk. It\\nis customary1 in clustering analysis to work with Gaussian mixtures; that is, each density\\nϕk is Gaussian with some unknown expectation vector µk and covariance matrix Σk. We\\ngather all unknown parameters, including the weights {wk}, into a parameter vector θ. As\\nusual, τ = {x1,..., xn}denotes the outcome of T. As the components of Tare iid, their\\n(joint) pdf is given by\\ng(τ|θ) :=\\nnY\\ni=1\\ng(xi |θ) =\\nnY\\ni=1\\nKX\\nk=1\\nwk ϕk(xi |µk,Σk). (4.32)\\nFollowing the same reasoning as for (4.5), we can estimate θfrom an outcome τby max-\\nimizing the log-likelihood function\\nl(θ|τ) :=\\nnX\\ni=1\\nln g(xi |θ) =\\nnX\\ni=1\\nln\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nKX\\nk=1\\nwk ϕk(xi |µk,Σk)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8. (4.33)\\nHowever, finding the maximizer of l(θ|τ) is not easy in general, since the function is typ-\\nically multiextremal.\\nExample 4.4 (Clustering via Mixture Models) The data depicted in Figure 4.4 con-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 153, 'page_label': '136'}, page_content='Example 4.4 (Clustering via Mixture Models) The data depicted in Figure 4.4 con-\\nsists of 300 data points that were independently generated from three bivariate normal\\ndistributions, whose parameters are given in that same figure. For each of these three dis-\\ntributions, exactly 100 points were generated. Ideally, we would like to cluster the data into\\nthree clusters that correspond to the three cases.\\nTo cluster the data into three groups, a possible model for the data is to assume that\\nthe points are iid draws from an (unknown) mixture of three 2-dimensional Gaussian dis-\\ntributions. This is a sensible approach, although in reality the data were not simulated\\nin this way. It is instructive to understand the di fference between the two models. In the\\nmixture model, each cluster label Z takes the value {1,2,3}with equal probability, and\\nhence, drawing the labels independently, the total number of points in each cluster would'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 153, 'page_label': '136'}, page_content='hence, drawing the labels independently, the total number of points in each cluster would\\n1Other common mixture distributions include Student t and Beta distributions.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 154, 'page_label': '137'}, page_content='Unsupervised Learning 137\\n-8 -6 -4 -2 0 2 4\\n-6\\n-4\\n-2\\n0\\n2\\n4\\ncluster mean vector covariance matrix\\n1\\n\"−4\\n0\\n# \" 2 1 .4\\n1.4 1 .5\\n#\\n2\\n\"0.5\\n−1\\n# \" 2 −0.95\\n−0.95 1\\n#\\n3\\n\"−1.5\\n−3\\n# \" 2 0 .1\\n0.1 0 .1\\n#\\nFigure 4.4: Cluster the 300 data points (left) into three clusters, without making any as-\\nsumptions about the probability distribution of the data. In fact, the data were generated\\nfrom three bivariate normal distributions, whose parameters are listed on the right.\\nbe Bin(300,1/3) distributed. However, in the actual simulation, the number of points in\\neach cluster is exactly 100. Nevertheless, the mixture model would be an accurate (al-\\nthough not exact) model for these data. Figure 4.5 displays the “target” Gaussian mixture\\ndensity for the data in Figure 4.4; that is, the mixture with equal weights and with the exact\\nparameters as specified in Figure 4.4.\\nFigure 4.5: The target mixture density for the data in Figure 4.4.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 154, 'page_label': '137'}, page_content='Figure 4.5: The target mixture density for the data in Figure 4.4.\\nIn the next section we will carry out the clustering by using the EM algorithm.\\n4.5.2 EM Algorithm for Mixture Models\\nAs we saw in Section 4.3, instead of maximizing the log-likelihood function (4.33) directly\\nfrom the data τ= {x1,..., xn}, the EM algorithm first augments the data data\\naugmentation\\nwith the vector of\\nlatent variables — in this case the hidden cluster labelsz = {z1,..., zn}. The idea is that τis'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 155, 'page_label': '138'}, page_content='138 Clustering via Mixture Models\\nonly the observed part of the complete random data ( T,Z), which were generated via the\\ntwo-step procedure described above. That is, for each data point X, first draw the cluster\\nlabel Z ∈{1,..., K}according to probabilities {w1,..., wK}and then, given Z = z, draw X\\nfrom ϕz. The joint pdf of Tand Z is\\ng(τ,z |θ) =\\nnY\\ni=1\\nwzi ϕzi (xi),\\nwhich is of a much simpler form than (4.32). It follows that the complete-data log-\\nlikelihoodcomplete-data\\nlog-likelihood\\nfunction\\nel(θ|τ,z) =\\nnX\\ni=1\\nln[wzi ϕzi (xi)] (4.34)\\nis often easier to maximize than the original log-likelihood (4.33), for any given (τ,z). But,\\nof course the latent variables z are not observed and soel(θ|τ,z) cannot be evaluated. In the\\nE-step of the EM algorithm, the complete-data log-likelihood is replaced with the expect-\\nation Ep el(θ|τ,Z), where the subscript p in the expectation indicates that Z is distributed\\naccording to the conditional pdf of Z given T= τ; that is, with pdf'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 155, 'page_label': '138'}, page_content='according to the conditional pdf of Z given T= τ; that is, with pdf\\np(z) = g(z |τ,θ) ∝g(τ,z |θ). (4.35)\\nNote that p(z) is of the form p1(z1) ··· pn(zn) so that, givenT= τ, the components of Z are\\nindependent of each other. The EM algorithm for mixture models can now be formulated\\nas follows.\\nAlgorithm 4.5.1: EM Algorithm for Mixture Models\\ninput: Data τ, initial guess θ(0).\\noutput: Approximation of the maximum likelihood estimate.\\n1 t ←1\\n2 while a stopping criterion is not met do\\n3 Expectation Step: Find p(t)(z) := g(z |τ,θ(t−1)) and Q(t)(θ) := Ep(t)el(θ|τ,Z).\\n4 Maximization Step: Let θ(t) ←argmaxθ Q(t)(θ).\\n5 t ←t + 1\\n6 return θ(t)\\nA possible termination condition is to stop when\\n\\x0c\\x0c\\x0cl(θ(t) |τ) −l(θ(t−1) |τ)\\n\\x0c\\x0c\\x0c/\\n\\x0c\\x0c\\x0cl(θ(t) |τ)\\n\\x0c\\x0c\\x0c < ε\\nfor some small tolerance ε >0. As was mentioned in Section 4.3, the sequence of log-\\nlikelihood values does not decrease with each iteration. Under certain continuity con-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 155, 'page_label': '138'}, page_content='likelihood values does not decrease with each iteration. Under certain continuity con-\\nditions, the sequence {θ(t)}is guaranteed to converge to a local maximizer of the log-\\nlikelihood l. Convergence to a global maximizer (if it exists) depends on the appropriate\\nchoice for the starting value. Typically, the algorithm is run from different random starting\\npoints.\\nFor the case of Gaussian mixtures, each ϕk = ϕ(·|µk,Σk),k = 1,..., K is the density\\nof a d-dimensional Gaussian distribution. Let θ(t−1) be the current guess for the optimal\\nparameter vector, consisting of the weights {w(t−1)\\nk }, mean vectors {µ(t−1)\\nk }, and covariance\\nmatrices {Σ(t−1)\\nk }. We first determine p(t) — the pdf of Z conditional on T = τ— for the\\ngiven guessθ(t−1). As mentioned before, the components ofZ given T= τare independent,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 156, 'page_label': '139'}, page_content='Unsupervised Learning 139\\nso it suffices to specify the discrete pdf,p(t)\\ni say, of eachZi given the observed pointXi = xi.\\nThe latter can be found from Bayes’ formula:\\np(t)\\ni (k) ∝w(t−1)\\nk ϕk(xi |µ(t−1)\\nk ,Σ(t−1)\\nk ), k = 1,..., K. (4.36)\\nNext, in view of (4.34), the function Q(t)(θ) can be written as\\nQ(t)(θ) = Ep(t)\\nnX\\ni=1\\n\\x10\\nln wZi + ln ϕZi (xi |µZi ,ΣZi )\\n\\x11\\n=\\nnX\\ni=1\\nEp(t)\\ni\\nh\\nln wZi + ln ϕZi (xi |µZi ,ΣZi )\\ni\\n,\\nwhere the {Zi}are independent and Zi is distributed according to p(t)\\ni in (4.36). This com-\\npletes the E-step. In the M-step we maximize Q(t) with respect to the parameter θ; that is,\\nwith respect to the {wk}, {µk}, and {Σk}. In particular, we maximize\\nnX\\ni=1\\nKX\\nk=1\\np(t)\\ni (k) \\x02ln wk + ln ϕk(xi |µk,Σk)\\x03,\\nunder the conditionP\\nk wk = 1. Using Lagrange multipliers and the fact thatPK\\nk=1 p(t)\\ni (k) = 1\\ngives the solution for the {wk}:\\nwk = 1\\nn\\nnX\\ni=1\\np(t)\\ni (k), k = 1,..., K. (4.37)\\nThe solutions forµk and Σk now follow from maximizingPn\\ni=1 p(t)\\ni (k) ln ϕk(xi |µk,Σk), lead-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 156, 'page_label': '139'}, page_content='The solutions forµk and Σk now follow from maximizingPn\\ni=1 p(t)\\ni (k) ln ϕk(xi |µk,Σk), lead-\\ning to\\nµk =\\nPn\\ni=1 p(t)\\ni (k) xi\\nPn\\ni=1 p(t)\\ni (k)\\n, k = 1,..., K (4.38)\\nand\\nΣk =\\nPn\\ni=1 p(t)\\ni (k) (xi −µk)(xi −µk)⊤\\nPn\\ni=1 p(t)\\ni (k)\\n, k = 1,..., K, (4.39)\\nwhich are very similar to the well-known formulas for the MLEs of the parameters of a\\nGaussian distribution. After assigning the solution parameters to θ(t) and increasing the\\niteration counter t by 1, the steps (4.36), (4.37), (4.38), and (4.39) are repeated until con-\\nvergence is reached. Convergence of the EM algorithm is very sensitive to the choice of\\ninitial parameters. It is therefore recommended to try various different starting conditions.\\nFor a further discussion of the theoretical and practical aspects of the EM algorithm we\\nrefer to [85].\\nExample 4.5 (Clustering via EM) We return to the data in Example 4.4, depicted in\\nFigure 4.4, and adopt the model that the data is coming from a mixture of three bivariate'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 156, 'page_label': '139'}, page_content='Figure 4.4, and adopt the model that the data is coming from a mixture of three bivariate\\nGaussian distributions.\\nThe Python code below implements the EM procedure described in Algorithm 4.5.1.\\nThe initial mean vectors{µk}of the bivariate Gaussian distributions are chosen (from visual\\ninspection) to lie roughly in the middle of each cluster, in this case [−2,−3]⊤,[−4,1]⊤, and\\n[0,−1]⊤. The corresponding covariance matrices are initially chosen as identity matrices,\\nwhich is appropriate given the observed spread of the data in Figure 4.4. Finally, the initial\\nweights are 1/3,1/3,1/3. For simplicity, the algorithm stops after 100 iterations, which in\\nthis case is more than enough to guarantee convergence. The code and data are available\\nfrom the book’s website in the GitHub folder Chapter4.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 157, 'page_label': '140'}, page_content=\"140 Clustering via Mixture Models\\nEMclust.py\\nimport numpy as np\\nfrom scipy.stats import multivariate_normal\\nXmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')\\nK = 3\\nn, D = Xmat.shape\\nW = np.array([[1/3,1/3,1/3]])\\nM = np.array([[-2.0,-4,0],[-3,1,-1]], dtype=np.float32)\\n# Note that if above *all* entries were written as integers , M would\\n# be defined to be of integer type , which will give the wrong answer\\nC = np.zeros((3,2,2))\\nC[:,0,0] = 1\\nC[:,1,1] = 1\\np = np.zeros((3,300))\\nfor i in range (0,100):\\n#E-step\\nfor k in range (0,K):\\nmvn = multivariate_normal( M[:,k].T, C[k,:,:] )\\np[k,:] = W[0,k]*mvn.pdf(Xmat)\\n# M-Step\\np = (p/ sum (p,0)) #normalize\\nW = np.mean(p,1).reshape(1,3)\\nfor k in range (0,K):\\nM[:,k] = (Xmat.T @ p[k,:].T)/ sum (p[k,:])\\nxm = Xmat.T - M[:,k].reshape(2,1)\\nC[k,:,:] = xm @ (xm*p[k,:]).T/ sum (p[k,:])\\nThe estimated parameters of the mixture distribution are given on the right-hand side\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 157, 'page_label': '140'}, page_content='The estimated parameters of the mixture distribution are given on the right-hand side\\nof Figure 4.6. After relabeling of the clusters, we can observe a close match with the\\nparameters in Figure 4.4.\\nThe ellipses on the left-hand side of Figure 4.6 show a close match between the 95%\\nprobability ellipses2 of the original Gaussian distributions (in gray) and the estimated ones.\\nA natural way to cluster each pointxi is to assign it to the clusterk for which the conditional\\nprobability pi(k) is maximal (with ties resolved arbitrarily). This gives the clustering of the\\npoints into red, green, and blue clusters in the figure.\\n2For each mixture component, the contour of the corresponding bivariate normal pdf is shown that en-\\ncloses 95% of the probability mass.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 158, 'page_label': '141'}, page_content='Unsupervised Learning 141\\n-6 -4 -2 0 2 4\\n-4\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\nweight mean vector covariance matrix\\n0.33\\n\"−1.51\\n−3.01\\n# \" 1.75 0 .03\\n0.03 0 .095\\n#\\n0.32\\n\" −4.08\\n−0.033\\n# \" 1.37 0 .92\\n0.92 1 .03\\n#\\n0.35\\n\" 0.36\\n−0.88\\n# \" 1.93 −1.20\\n−1.20 1 .44\\n#\\nFigure 4.6: The results of the EM clustering algorithm applied to the data depicted in\\nFigure 4.4.\\nAs an alternative to the EM algorithm, one can of course use continuous multiextremal\\noptimization algorithms to directly optimize the log-likelihood functionl(θ|τ) = ln g(τ|θ)\\nin (4.33) over the set Θ of all possible θ. This is done for example in [15], demonstrating\\nsuperior results to EM when there are few data points. Closer investigation of the likelihood\\nfunction reveals that there is a hidden problem with any maximum likelihood approach for\\nclustering if Θ is chosen as large as possible — i.e., any mixture distribution is possible. To\\ndemonstrate this problem, consider Figure 4.7, depicting the probability density function,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 158, 'page_label': '141'}, page_content='demonstrate this problem, consider Figure 4.7, depicting the probability density function,\\ng(·|θ) of a mixture of two Gaussian distributions, where θ = [w,µ1,σ2\\n1,µ2,σ2\\n2]⊤ is the\\nvector of parameters for the mixture distribution. The log-likelihood function is given by\\nl(θ|τ) = P4\\ni=1 ln g(xi |θ), where x1,..., x4 are the data (indicated by dots in the figure).\\n-4 -2 0 2 4 6 8\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nFigure 4.7: Mixture of two Gaussian distributions.\\nIt is clear that by fixing the mixing constant w at 0.25 (say) and centering the first\\ncluster at x1, one can obtain an arbitrarily large likelihood value by taking the variance of\\nthe first cluster to be arbitrarily small. Similarly, for higher dimensional data, by choosing\\n“point” or “line” clusters, or in general “degenerate” clusters, one can make the value of\\nthe likelihood infinite. This is a manifestation of the familiar overfitting problem for the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 159, 'page_label': '142'}, page_content='142 Clustering via Vector Quantization\\ntraining loss that we already encountered in Chapter 2. Thus, the unconstrained maximiza-\\ntion of the log-likelihood function is an ill-posed problem, irrespective of the choice of the\\noptimization algorithm!\\nTwo possible solutions to this “overfitting” problem are:\\n1. Restrict the parameter set Θ in such a way that degenerate clusters (sometimes called\\nspurious clusters) are not allowed.\\n2. Run the given algorithm and if the solution is degenerate, discard it and run the\\nalgorithm afresh. Keep restarting the algorithm until a non-degenerate solution is\\nobtained.\\nThe first approach is usually applied to multiextremal optimization algorithms and the\\nsecond is used for the EM algorithm.\\n4.6 Clustering via Vector Quantization\\nIn the previous section we introduced clustering via mixture models, as a form of paramet-\\nric density estimation (as opposed to the nonparametric density estimation in Section 4.4).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 159, 'page_label': '142'}, page_content='ric density estimation (as opposed to the nonparametric density estimation in Section 4.4).\\nThe clusters were modeled in a natural way via the latent variables and the EM algorithm\\nprovided a convenient way to assign the cluster members. In this section we consider a\\nmore heuristic approach to clustering by ignoring the distributional properties of the data.\\nThe resulting algorithms tend to scale better with the number of samples n and the dimen-\\nsionality d.\\nIn mathematical terms, we consider the following clustering (also called data segment-\\nation) problem. Given a collection τ = {x1,..., xn}of data points in some d-dimensional\\nspace X, divide this data set into K clusters (groups) such that some loss function is min-\\nimized. A convenient way to determine these clusters is to first divide up the entire space\\nX, using some distance function dist(·,·) on this space. A standard choice is the Euclidean\\n(or L2) distance:\\ndist(x,x′) = ∥x −x′∥=\\nvt dX\\ni=1\\n(xi −x′\\ni )2.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 159, 'page_label': '142'}, page_content='(or L2) distance:\\ndist(x,x′) = ∥x −x′∥=\\nvt dX\\ni=1\\n(xi −x′\\ni )2.\\nOther commonly used distance measures on Rd include the Manhattan distanceManhattan\\ndistance\\n:\\ndX\\ni=1\\n|xi −x′\\ni |\\nand the maximum distancemaximum\\ndistance\\n:\\nmax\\ni=1,...,d\\n|xi −x′\\ni |.\\nOn the set of strings of length d, an often-used distance measure is the Hamming distanceHamming\\ndistance\\n:\\ndX\\ni=1\\n1{xi , x′\\ni },\\nthat is, the number of mismatched characters. For example, the Hamming distance between\\n010101 and 011010 is 4.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 160, 'page_label': '143'}, page_content='Unsupervised Learning 143\\nWe can partition the space Xinto regions as follows: First, we choose K points\\nc1,..., cK called cluster centers or source vectors source vectors. For each k = 1,..., K, let\\nRk = {x ∈X : dist(x,ck) ⩽dist(x,ci) for all i , k}\\nbe the set of points in Xthat lie closer to ck than any other center. The regions or cells\\n{Rk}divide the space Xinto what is called a Voronoi diagram or a Voronoi tessellation Voronoi\\ntessellation\\n.\\nFigure 4.8 shows a V oronoi tessellation of the plane into ten regions, using the Euclidean\\ndistance. Note that here the boundaries between the V oronoi cells are straight line seg-\\nments. In particular, if cellRi and Rj share a border, then a point on this border must satisfy\\n∥x −ci∥= ∥x −cj∥; that is, it must lie on the line that passes through the point ( cj + ci)/2\\n(that is, the midway point of the line segment between ci and cj) and be perpendicular to\\ncj −ci.\\n-2 0 2 4\\n-2\\n0\\n2'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 160, 'page_label': '143'}, page_content='cj −ci.\\n-2 0 2 4\\n-2\\n0\\n2\\nFigure 4.8: A V oronoi tessellation of the plane into ten cells, determined by the (red) cen-\\nters.\\nOnce the centers (and thus the cells {Rk}) are chosen, the points in τcan be clustered\\naccording to their nearest center. Points on the boundary have to be treated separately. This\\nis a moot point for continuous data, as generally no data points will lie exactly on the\\nboundary.\\nThe main remaining issue is how to choose the centers so as to cluster the data in some\\noptimal way. In terms of our (unsupervised) learning framework, we wish to approximate\\na vector x via one of c1,..., cK, using a piecewise constant vector-valued function\\ng(x |C) :=\\nKX\\nk=1\\nck 1{x ∈Rk},\\nwhere C is the d ×K matrix [c1,..., cK]. Thus, g(x |C) = ck when x falls in region Rk (we\\nignore ties). Within this class Gof functions, parameterized by C, our aim is to minimize\\nthe training loss. In particular, for the squared-error loss, Loss(x,x′) = ∥x−x′∥2, the training\\nloss is\\nℓτn (g(·|C)) = 1\\nn'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 160, 'page_label': '143'}, page_content='loss is\\nℓτn (g(·|C)) = 1\\nn\\nnX\\ni=1\\n∥xi −g(xi |C)∥2 = 1\\nn\\nKX\\nk=1\\nX\\nx∈Rk∩τn\\n||x −ck||2. (4.40)\\nThus, the training loss minimizes the average squared distance between the centers. This\\nframework also combines both the encoding and decoding steps in vector quantization vector\\nquantization'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 161, 'page_label': '144'}, page_content='144 Clustering via Vector Quantization\\n[125]. Namely, we wish to “quantize” or “encode” the vectors in τin such a way that each\\nvector is represented by one of K source vectors c1,..., cK, such that the loss (4.40) of this\\nrepresentation is minimized.\\nMost well-known clustering and vector quantization methods update the vector of cen-\\nters, starting from some initial choice and using iterative (typically gradient-based) proced-\\nures. It is important to realize that in this case (4.40) is seen as a function of the centers,\\nwhere each point x is assigned to the nearest center, thus determining the clusters. It is well\\nknown that this type of problem — optimization with respect to the centers — is highly\\nmultiextremal and, depending on the initial clusters, gradient-based procedures tend to\\nconverge to a local minimum rather than a global minimum.\\n4.6.1 K-Means\\nOne of the simplest methods for clustering is theK-means method. It is an iterative method'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 161, 'page_label': '144'}, page_content='One of the simplest methods for clustering is theK-means method. It is an iterative method\\nwhere, starting from an initial guess for the centers, new centers are formed by taking\\nsample means of the current points in each cluster. The new centers are thus the centroidscentroids\\nof the points in each cell. Although there exist many di fferent varieties of the K-means\\nalgorithm, they are all essentially of the following form:\\nAlgorithm 4.6.1: K-Means\\ninput: Collection of points τ= {x1,..., xn}, number of clusters K, initial centers\\nc1,..., cK.\\noutput: Cluster centers and cells (regions).\\n1 while a stopping criterion is not met do\\n2 R1,..., RK ←∅ (empty sets).\\n3 for i = 1 to n do\\n4 d ←[dist(xi,c1),..., dist(xi,cK)] // distances to centers\\n5 k ←argminj dj\\n6 Rk ←Rk ∪{xi} // assign xi to cluster k\\n7 for k = 1 to K do\\n8 ck ←\\nP\\nx∈Rk x\\n|Rk| // compute the new center as a centroid of points\\n9 return {ck}, {Rk}'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 161, 'page_label': '144'}, page_content='8 ck ←\\nP\\nx∈Rk x\\n|Rk| // compute the new center as a centroid of points\\n9 return {ck}, {Rk}\\nThus, at each iteration, for a given choice of centers, each point in τ is assigned to\\nits nearest center. After all points have been assigned, the centers are recomputed as the\\ncentroids of all the points in the current cluster (Line 8). A typical stopping criterion is to\\nstop when the centers no longer change very much. As the algorithm is quite sensitive to\\nthe choice of the initial centers, it is prudent to try multiple starting values, e.g., chosen\\nrandomly from the bounding box of the data points.\\nWe can see the K-means method as a deterministic (or “hard”) version of the probab-\\nilistic (or “soft”) EM algorithm as follows. Suppose in the EM algorithm we have Gaus-\\nsian mixtures with a fixed covariance matrix Σk = σ2Id, k = 1,..., K, where σ2 should be\\nthought of as being infinitesimally small. Consider iterationt of the EM algorithm. Having\\nobtained the expectation vectors µ(t−1)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 161, 'page_label': '144'}, page_content='obtained the expectation vectors µ(t−1)\\nk and weights w(t−1)\\nk ,k = 1,..., K, each point xi is as-\\nsigned a cluster label Zi according to the probabilities p(t)\\ni (k),k = 1,..., K given in (4.36).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 162, 'page_label': '145'}, page_content='Unsupervised Learning 145\\nBut for σ2 →0 the probability distribution {p(t)\\ni (k)}becomes degenerate, putting all its\\nprobability mass on argmink ∥xi −µk∥2. This corresponds to the K-means rule of assigning\\nxi to its nearest cluster center. Moreover, in the M-step (4.38) each cluster centerµ(t)\\nk is now\\nupdated according to the average of the {xi}that have been assigned to cluster k. We thus\\nobtain the same deterministic updating rule as in K-means.\\nExample 4.6 (K-means Clustering) We cluster the data from Figure 4.4 viaK-means,\\nusing the Python implementation below. Note that the data points are stored as a 300 ×2\\nmatrix Xmat. We take the same starting centers as in the EM example:c1 = [−2,−3]⊤,c2 =\\n[−4,1]⊤, and c3 = [0,−1]⊤. Note also that squared Euclidean distances are used in the\\ncomputations, as these are slightly faster to compute than Euclidean distances (as no square\\nroot computations are required) while yielding exactly the same cluster center evaluations.\\nKmeans.py'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 162, 'page_label': '145'}, page_content=\"Kmeans.py\\nimport numpy as np\\nXmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')\\nK = 3\\nn, D = Xmat.shape\\nc = np.array([[-2.0,-4,0],[-3,1,-1]]) #initialize centers\\ncold = np.zeros(c.shape)\\ndist2 = np.zeros((K,n))\\nwhile np.abs (c - cold). sum () > 0.001:\\ncold = c.copy()\\nfor i in range (0,K): #compute the squared distances\\ndist2[i,:] = np. sum ((Xmat - c[:,i].T)**2, 1)\\nlabel = np.argmin(dist2,0) #assign the points to nearest centroid\\nminvals = np.amin(dist2,0)\\nfor i in range (0,K): # recompute the centroids\\nc[:,i] = np.mean(Xmat[np.where(label == i),:],1).reshape(1,2)\\nprint ('Loss = {:3.3f} '.format (minvals.mean()))\\nLoss = 2.288\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 163, 'page_label': '146'}, page_content='146 Clustering via Vector Quantization\\n-6 -4 -2 0 2 4\\n-5\\n-4\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\nFigure 4.9: Results of the K-means algorithm applied to the data in Figure 4.4. The thick\\nblack circles are the centroids and the dotted lines define the cell boundaries.\\nWe found the cluster centers c1 = [−1.9286,−3.0416]⊤,c2 = [−3.9237,0.0131]⊤, and\\nc3 = [0.5611,−1.2980]⊤, giving the clustering depicted in Figure 4.9. The corresponding\\nloss (4.40) was found to be 2.288.\\n4.6.2 Clustering via Continuous Multiextremal Optimization\\nAs already mentioned, the exact minimization of the loss function (4.40) is di fficult to\\naccomplish via standard local search methods such as gradient descent, as the function\\nis highly multimodal. However, nothing is preventing us from using global optimization\\nmethods such as the CE or SCO methods discussed in Sections 3.4.2 and 3.4.3.☞100\\nExample 4.7 (Clustering via CE) We take the same data set as in Example 4.6 and'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 163, 'page_label': '146'}, page_content=\"Example 4.7 (Clustering via CE) We take the same data set as in Example 4.6 and\\ncluster the points via minimization of the loss (4.40) using the CE method. The Python\\ncode below is very similar to the code in Example 3.16, except that now we are dealing☞101\\nwith a six-dimensional optimization problem. The loss function is implemented in the func-\\ntion Scluster, which essentially reuses the squared distance computation of theK-means\\ncode in Example 4.6. The CE program typically converges to a loss of 2.287, correspond-\\ning to the (global) minimizers c1 = [−1.9286,−3.0416]⊤,c2 = [−3.8681,0.0456]⊤, and\\nc3 = [0.5880,−1.3526]⊤, which slightly differs from the local minimizers for the K-means\\nalgorithm.\\nclustCE.py\\nimport numpy as np\\nnp.set_printoptions(precision=4)\\nXmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')\\nK = 3\\nn, D = Xmat.shape\\ndef Scluster(c):\\nn, D = Xmat.shape\\ndist2 = np.zeros((K,n))\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 164, 'page_label': '147'}, page_content='Unsupervised Learning 147\\ncc = c.reshape(D,K)\\nfor i in range (0,K):\\ndist2[i,:] = np. sum ((Xmat - cc[:,i].T)**2, 1)\\nminvals = np.amin(dist2,0)\\nreturn minvals.mean()\\nnumvar = K*D\\nmu = np.zeros(numvar) #initialize centers\\nsigma = np.ones(numvar)*2\\nrho = 0.1\\nN = 500; Nel = int (N*rho); eps = 0.001\\nfunc = Scluster\\nbest_trj = np.array(numvar)\\nbest_perf = np.Inf\\ntrj = np.zeros(shape=(N,numvar))\\nwhile (np. max (sigma)>eps):\\nfor i in range (0,numvar):\\ntrj[:,i] = (np.random.randn(N,1)*sigma[i]+ mu[i]).reshape(N,)\\nS = np.zeros(N)\\nfor i in range (0,N):\\nS[i] = func(trj[i])\\nsortedids = np.argsort(S) # from smallest to largest\\nS_sorted = S[sortedids]\\nbest_trj = np.array(n)\\nbest_perf = np.Inf\\neliteids = sortedids[ range (0,Nel)]\\neliteTrj = trj[eliteids,:]\\nmu = np.mean(eliteTrj,axis=0)\\nsigma = np.std(eliteTrj,axis=0)\\nif (best_perf>S_sorted[0]):\\nbest_perf = S_sorted[0]\\nbest_trj = trj[sortedids[0]]\\nprint (best_perf)\\nprint (best_trj.reshape(2,3))\\n2.2874901831572947\\n[[-3.9238 -1.8477 0.5895]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 164, 'page_label': '147'}, page_content='print (best_perf)\\nprint (best_trj.reshape(2,3))\\n2.2874901831572947\\n[[-3.9238 -1.8477 0.5895]\\n[ 0.0134 -3.0292 -1.2442]]\\n4.7 Hierarchical Clustering\\nIt is sometimes useful to determine data clusters in a hierarchical manner; an example\\nis the construction of evolutionary relationships between animal species. Establishing a\\nhierarchy of clusters can be done in a bottom-up or a top-down manner. In the bottom-up\\napproach, also called agglomerative clustering agglomerative\\nclustering\\n, the data points are merged in larger and\\nlarger clusters until all the points have been merged into a single cluster. In the top-down'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 165, 'page_label': '148'}, page_content='148 Hierarchical Clustering\\nor divisive clusteringdivisive\\nclustering\\napproach, the data set is divided up into smaller and smaller clusters.\\nThe left panel of Figure 4.10 depicts a hierarchy of clusters.\\n7 8 6 1 2 3 4 5\\nLabels\\n10\\n20\\n30\\n40Distance\\nFigure 4.10: Left: a cluster hierarchy of 15 clusters. Right: the corresponding dendrogram.\\nIn Figure 4.10, each cluster is given a cluster identifier. At the lowest level are clusters\\ncomprised of the original data points (identifiers 1 ,..., 8). The union of clusters 1 and 2\\nform a cluster with identifier 9, and the union of 3 and 4 form a cluster with identifier 10.\\nIn turn the union of clusters 9 and 10 constitutes cluster 12, and so on.\\nThe right panel of Figure 4.10 shows a convenient way to visualize cluster hierarchies\\nusing a dendrogramdendrogram (from the Greek dendro for tree). A dendrogram not only summarizes\\nhow clusters are merged or split, but also shows the distance between clusters, here on the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 165, 'page_label': '148'}, page_content='how clusters are merged or split, but also shows the distance between clusters, here on the\\nvertical axis. The horizontal axis shows which cluster each data point (label) belongs to.\\nMany different types of hierarchical clustering can be performed, depending on how\\nthe distance is defined between two data points and between two clusters. Denote the data\\nset by X= {xi,i = 1,..., n}. As in Section 4.6, let dist(xi,xj) be the distance between data\\npoints xi and xj. The default choice is the Euclidean distance dist(xi,xj) = ∥xi −xj∥.\\nLet Iand Jbe two disjoint subsets of {1,..., n}. These sets correspond to two disjoint\\nsubsets (that is, clusters) of X: {xi,i = I}and {xj, j = J}. We denote the distance between\\nthese two clusters by d(I,J). By specifying the function d, we indicate how the clusters\\nare linked. For this reason it is also referred to as the linkagelinkage criterion. We give a number\\nof examples:\\n• Single linkage. The closest distance between the clusters.\\ndmin(I,J) := min'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 165, 'page_label': '148'}, page_content='of examples:\\n• Single linkage. The closest distance between the clusters.\\ndmin(I,J) := min\\ni∈I,j∈J\\ndist(xi,xj).\\n• Complete linkage. The furthest distance between the clusters.\\ndmax(I,J) := max\\ni∈I,j∈J\\ndist(xi,xj).\\n• Group average. The mean distance between the clusters. Note that this depends on\\nthe cluster sizes.\\ndavg(I,J) := 1\\n|I||J|\\nX\\ni∈I\\nX\\nj∈J\\ndist(xi,xj).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 166, 'page_label': '149'}, page_content='Unsupervised Learning 149\\nFor these linkage criteria, Xis usually assumed to be Rd with the Euclidean distance.\\nAnother notable measure for the distance between clusters is Ward’s minimum vari-\\nance linkage criterion. Ward’s linkageHere, the distance between clusters is expressed as the additional\\namount of “variance” (expressed in terms of the sum of squares) that would be intro-\\nduced if the two clusters were merged. More precisely, for any setKof indices (labels) let\\nxK = P\\nk∈K xk/|K|denote its corresponding cluster mean. Then\\ndWard(I,J) :=\\nX\\nk∈I∪J\\n∥xk −xI∪J∥2 −\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nX\\ni∈I\\n∥xi −xI∥2 +\\nX\\nj∈J\\n∥xj −xJ∥2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8. (4.41)\\nIt can be shown (see Exercise 8) that the Ward linkage depends only on the cluster means\\nand the cluster sizes for Iand J:\\ndWard(I,J) = |I||J|\\n|I|+ |J|∥xI−xJ∥2.\\nIn software implementations, the Ward linkage function is often rescaled by mul-\\ntiplying it by a factor of 2. In this way, the distance between one-point clusters {xi}'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 166, 'page_label': '149'}, page_content='tiplying it by a factor of 2. In this way, the distance between one-point clusters {xi}\\nand {xj}is the squared Euclidean distance ∥xi −xj∥2.\\nHaving chosen a distance on Xand a linkage criterion, a general agglomerative clus-\\ntering algorithm proceeds in the following “greedy” manner.\\nAlgorithm 4.7.1: Greedy Agglomerative Clustering\\ninput: Distance function dist, linkage function d, number of clusters K.\\noutput: The label sets for the tree.\\n1 Initialize the set of cluster identifiers: I= {1,..., n}.\\n2 Initialize the corresponding label sets: Li = {i}, i ∈I.\\n3 Initialize a distance matrix D = [di j] with di j = d({i},{j}).\\n4 for k = n + 1 to 2n −K do\\n5 Find i and j >i in Isuch that di j is minimal.\\n6 Create a new label set Lk := Li ∪Lj.\\n7 Add the new identifier k to Iand remove the old identifiers i and j from I.\\n8 Update the distance matrix D with respect to the identifiers i, j, and k.\\n9 return Li,i = 1,..., 2n −K'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 166, 'page_label': '149'}, page_content='9 return Li,i = 1,..., 2n −K\\nInitially, the distance matrix D contains the (linkage) distances between the one-point\\nclusters containing one of the data points x1,..., xn, and hence with identifiers 1 ,..., n.\\nFinding the shortest distance amounts to a table lookup in D. When the closest clusters\\nare found, they are merged into a new cluster, and a new identifier k (the smallest positive\\ninteger that has not yet been used as an identifier) is assigned to this cluster. The old iden-\\ntifiers i and j are removed from the cluster identifier set I. The matrix D is then updated\\nby adding a k-th column and row that contain the distances betweenk and any m ∈I. This\\nupdating step could be computationally quite costly if the cluster sizes are large and the\\nlinkage distance between the clusters depends on all points within the clusters. Fortunately,\\nfor many linkage functions, the matrix D can be updated in an efficient manner.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 167, 'page_label': '150'}, page_content='150 Hierarchical Clustering\\nSuppose that at some stage in the algorithm, clusters Iand J, with identifiers i and j,\\nrespectively, are merged into a cluster K= I∪J with identifier k. Let M, with identifier\\nm, be a previously assigned cluster. An update rule of the linkage distance dkm between K\\nand Mis called a Lance–WilliamsLance–\\nWilliams\\nupdate if it can be written in the form\\ndkm = αdim + βdjm + γdi j + δ|dim −djm|,\\nwhere α,...,δ depend only on simple characteristics of the clusters involved, such as the\\nnumber of elements within the clusters. Table 4.2 shows the update constants for a number\\nof common linkage functions. For example, for single linkage, dim is the minimal distance\\nbetween Iand M, and djm is the minimal distance between Jand M. The smallest of\\nthese is the minimal distance between Kand M. That is, dkm = min{dim,djm}= dim/2 +\\ndjm/2 −|dim −djm|/2.\\nTable 4.2: Constants for the Lance–Williams update rule for various linkage functions,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 167, 'page_label': '150'}, page_content='Table 4.2: Constants for the Lance–Williams update rule for various linkage functions,\\nwith ni,nj,nm denoting the number of elements in the corresponding clusters.\\nLinkage α β γ δ\\nSingle 1 /2 1 /2 0 −1/2\\nComplete 1 /2 1 /2 0 1 /2\\nGroup avg. ni\\nni + nj\\nnj\\nni + nj\\n0 0\\nWard ni + nm\\nni + nj + nm\\nnj + nm\\nni + nj + nm\\n−nm\\nni + nj + nm\\n0\\nIn practice, Algorithm 4.7.1 is run until a single cluster is obtained. Instead of returning\\nthe label sets of all 2 n −1 clusters, a linkage matrixlinkage matrix is returned that contains the same\\ninformation. At the end of each iteration (Line 8) the linkage matrix stores the merged\\nlabels i and j, as well as the (minimal) distance di j. Other information such as the number\\nof elements in the merged cluster can also be stored. Dendrograms and cluster labels can be\\ndirectly constructed from the linkage matrix. In the following example, the linkage matrix\\nis returned by the method agg_cluster.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 167, 'page_label': '150'}, page_content='is returned by the method agg_cluster.\\nExample 4.8 (Agglomerative Hierarchical Clustering) The Python code below gives\\na basic implementation of Algorithm 4.7.1 using the Ward linkage function. The methods\\nfcluster and dendrogram from the scipy module can be used to identify the labels in\\na cluster and to draw the corresponding dendrogram.\\nAggCluster.py\\nimport numpy as np\\nfrom scipy.spatial.distance import cdist\\ndef update_distances(D,i,j, sizes): # distances for merged cluster\\nn = D.shape[0]\\nd = np.inf * np.ones(n+1)\\nfor k in range (n): # Update distances\\nd[k] = ((sizes[i]+sizes[k])*D[i,k] +\\n(sizes[j]+sizes[k])*D[j,k] -'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 168, 'page_label': '151'}, page_content=\"Unsupervised Learning 151\\nsizes[k]*D[i,j])/(sizes[i] + sizes[j] + sizes[k])\\ninfs = np.inf * np.ones(n) # array of infinity\\nD[i,:],D[:,i],D[j,:],D[:,j] = infs,infs,infs,infs # deactivate\\nnew_D = np.inf * np.ones((n+1,n+1))\\nnew_D[0:n,0:n] = D # copy old matrix into new_D\\nnew_D[-1,:], new_D[:,-1] = d,d # add new row and column\\nreturn new_D\\ndef agg_cluster(X):\\nn = X.shape[0]\\nsizes = np.ones(n)\\nD = cdist(X, X,metric = 'sqeuclidean ') # initialize dist. matrix\\n.\\nnp.fill_diagonal(D, np.inf * np.ones(D.shape[0]))\\nZ = np.zeros((n-1,4)) #linkage matrix encodes hierarchy tree\\nfor t in range (n-1):\\ni,j = np.unravel_index(D.argmin(), D.shape) # minimizer pair\\nsizes = np.append(sizes, sizes[i] + sizes[j])\\nZ[t,:]=np.array([i, j, np.sqrt(D[i,j]), sizes[-1]])\\nD = update_distances(D, i,j, sizes) # update distance matr.\\nreturn Z\\nimport scipy.cluster.hierarchy as h\\nX = np.genfromtxt( 'clusterdata.csv ',delimiter= ',') # read the data\\nZ = agg_cluster(X) # form the linkage matrix\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 168, 'page_label': '151'}, page_content=\"Z = agg_cluster(X) # form the linkage matrix\\nh.dendrogram(Z) # SciPy can produce a dendrogram from Z\\n# fcluster function assigns cluster ids to all points based on Z\\ncl = h.fcluster(Z, criterion = 'maxclust ', t=3)\\nimport matplotlib.pyplot as plt\\nplt.figure(2), plt.clf()\\ncols = [ 'red','green ','blue ']\\ncolors = [cols[i-1] for i in cl]\\nplt.scatter(X[:,0], X[:,1],c=colors)\\nplt.show()\\nNote that the distance matrix is initialized with the squared Euclidean distance, so that\\nthe Ward linkage is rescaled by a factor of 2. Also, note that the linkage matrix stores\\nthe square root of the minimal cluster distances rather than the distances themselves. We\\nleave it as an exercise to check that by using these modifications the results agree with the\\nlinkage method from scipy; see Exercise 9.\\nIn contrast to the bottom-up (agglomerative) approach to hierarchical clustering, the\\ndivisive approach starts with one cluster, which is divided into two clusters that are as\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 168, 'page_label': '151'}, page_content='divisive approach starts with one cluster, which is divided into two clusters that are as\\n“dissimilar” as possible, which can then be further divided, and so on. We can use the same\\nlinkage criteria as for agglomerative clustering to divide a parent cluster into two child\\nclusters by maximizing the distance between the child clusters. Although it is a natural to try\\nto group together data by separating dissimilar ones as far as possible, the implementation\\nof this idea tends to scale poorly with n. The problem is related to the well-knownmax-cut\\nproblem max-cut\\nproblem\\n: given an n ×n matrix of positive costs ci j,i, j ∈{1,..., n}, partition the index set'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 169, 'page_label': '152'}, page_content='152 Hierarchical Clustering\\nI= {1,..., n}into two subsets Jand Ksuch that the total cost across the sets, that is,\\nX\\nj∈J\\nX\\nk∈K\\ndjk,\\nis maximal. If instead we maximize according to theaverage distance, we obtain the group\\naverage linkage criterion.\\nExample 4.9 (Divisive Clustering via CE) The following Python code is used to di-\\nvide a small data set (of size 300) into two parts according to maximal group average link-\\nage. It uses a short cross-entropy algorithm similar to the one presented in Example 3.19.\\nGiven a vector of probabilities {pi,i = 1,..., n}, the algorithm generates an n ×n matrix☞111\\nof Bernoulli random variables with success probability pi for column i. For each row, the\\n0s and 1s divide the index set into two clusters, and the corresponding average linkage\\ndistance is computed. The matrix is then sorted row-wise according to these distances. Fi-\\nnally, the probabilities{pi}are updated according to the mean values of the best 10% rows.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 169, 'page_label': '152'}, page_content='nally, the probabilities{pi}are updated according to the mean values of the best 10% rows.\\nThe process is repeated until the {pi}degenerate to a binary vector. This then presents the\\n(approximate) solution.\\nclustCE2.py\\nimport numpy as np\\nfrom numpy import genfromtxt\\nfrom scipy.spatial.distance import squareform\\nfrom scipy.spatial.distance import pdist\\nimport matplotlib.pyplot as plt\\ndef S(x,D):\\nV1 = np.where(x==0)[0] # {V1,V2} is the partition\\nV2 = np.where(x==1)[0]\\ntmp = D[V1]\\ntmp = tmp[:,V2]\\nreturn np.mean(tmp) # the size of the cut\\ndef maxcut(D,N,eps,rho,alpha):\\nn = D.shape[1]\\nNe = int (rho*N)\\np = 1/2*np.ones(n)\\np[0] = 1.0\\nwhile (np. max (np.minimum(p,np.subtract(1,p))) > eps):\\nx = np.array(np.random.uniform(0,1,(N,n))<=p, dtype=np.int64)\\nsx = np.zeros(N)\\nfor i in range (N):\\nsx[i] = S(x[i],D)\\nsortSX = np.flip(np.argsort(sx))\\n#print(\"gamma = \",sx[sortSX[Ne -1]], \" best=\",sx[sortSX [0]])\\nelIds = sortSX[0:Ne]\\nelites = x[elIds]\\npnew = np.mean(elites, axis=0)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 169, 'page_label': '152'}, page_content='elIds = sortSX[0:Ne]\\nelites = x[elIds]\\npnew = np.mean(elites, axis=0)\\np = alpha*pnew + (1.0-alpha)*p'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 170, 'page_label': '153'}, page_content='Unsupervised Learning 153\\nreturn np.round (p)\\nXmat = genfromtxt( \\'clusterdata.csv \\', delimiter= \\',\\')\\nn = Xmat.shape[0]\\nD = squareform(pdist(Xmat))\\nN = 1000\\neps = 10**-2\\nrho = 0.1\\nalpha = 0.9\\n# CE\\npout = maxcut(D,N,eps,rho, alpha);\\ncutval = S(pout,D)\\nprint (\"cutvalue \",cutval)\\n#plot\\nV1 = np.where(pout==0)[0]\\nxblue = Xmat[V1]\\nV2 = np.where(pout==1)[0]\\nxred = Xmat[V2]\\nplt.scatter(xblue[:,0],xblue[:,1], c=\"blue\")\\nplt.scatter(xred[:,0],xred[:,1], c=\"red\")\\ncutvalue 4.625207676517948\\n6\\n 4\\n 2\\n 0\\n 2\\n 4\\n4\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\nFigure 4.11: Division of the data in Figure 4.4 into two clusters, via the cross-entropy\\nmethod.\\n4.8 Principal Component Analysis (PCA)\\nThe main idea of principal component analysis principal\\ncomponent\\nanalysis\\n(PCA) is to reduce the dimensionality of\\na data set consisting of many variables. PCA is a feature reduction (or feature extraction)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 171, 'page_label': '154'}, page_content='154 Principal Component Analysis (PCA)\\nmechanism, that helps us to handle high-dimensional data with more features than is con-\\nvenient to interpret.\\n4.8.1 Motivation: Principal Axes of an Ellipsoid\\nConsider a d-dimensional normal distribution with mean vector 0 and covariance matrix\\nΣ. The corresponding pdf (see (2.33)) is☞46\\nf (x) = 1√(2π)n |Σ|\\ne−1\\n2 x⊤Σ−1 x, x ∈Rd.\\nIf we were to draw many iid samples from this pdf, the points would roughly have an\\nellipsoid pattern, as illustrated in Figure 3.1, and correspond to the contours of f : sets of☞71\\npoints x such that x⊤Σ−1 x = c, for some c ⩾0. In particular, consider the ellipsoid\\nx⊤Σ−1 x = 1, x ∈Rd. (4.42)\\nLet Σ = BB⊤, where B is for example the (lower) Cholesky matrix. Then, as explained☞373\\nin Example A.5, the ellipsoid (4.42) can also be viewed as the linear transformation of☞366\\nd-dimensional unit sphere via matrix B. Moreover, the principal axes of the ellipsoid canprincipal axes'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 171, 'page_label': '154'}, page_content='be found via a singular value decomposition (SVD) of B (or Σ); see Section A.6.5 andsingular value\\ndecomposition Example A.8. In particular, suppose that an SVD of B is\\n☞378 B = UDV⊤ (note that an SVD of Σ is then UD2U⊤).\\nThe columns of the matrix UD correspond to the principal axes of the ellipsoid, and the\\nrelative magnitudes of the axes are given by the elements of the diagonal matrixD. If some\\nof these magnitudes are small compared to the others, a reduction in the dimension of the\\nspace may be achieved by projecting each point x ∈Rd onto the subspace spanned by the\\nmain (say k ≪d) columns of U — the so-called principal componentsprincipal\\ncomponents\\n. Suppose without\\nloss of generality that the first k principal components are given by the first k columns of\\nU, and let Uk be the corresponding d ×k matrix.\\nWith respect to the standard basis{ei}, the vector x = x1e1 +···+ xded is represented by\\nthe d-dimensional vector [x1,..., xd]⊤. With respect to the orthonormal basis {ui}formed'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 171, 'page_label': '154'}, page_content='the d-dimensional vector [x1,..., xd]⊤. With respect to the orthonormal basis {ui}formed\\nby the columns of matrix U, the representation of x is U⊤x. Similarly, the projection of\\nany point x onto the subspace spanned by the first k principal vectors is represented by the\\nk-dimensional vector U⊤\\nk x, with respect to the orthonormal basis formed by the columns of\\nUk. So, the idea is that if a pointx lies close to its projectionUkU⊤\\nk x, we may represent it via\\nk numbers instead of d, using the combined features given by the k principal components.\\nSee Section A.4 for a review of projections and orthonormal bases.☞362\\nExample 4.10 (Principal Components) Consider the matrix\\nΣ =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n14 8 3\\n8 5 2\\n3 2 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\nwhich can be written as Σ = BB⊤, with\\nB =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 2 3\\n0 1 2\\n0 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 172, 'page_label': '155'}, page_content='Unsupervised Learning 155\\nFigure 4.12 depicts the ellipsoid x⊤Σ−1 x = 1, which can be obtained by linearly transform-\\ning the points on the unit sphere by means of the matrix B. The principal axes and sizes of\\nthe ellipsoid are found through a singular value decomposition B = UDV⊤, where U and\\nD are\\nU =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0.8460 0 .4828 0 .2261\\n0.4973 −0.5618 −0.6611\\n0.1922 −0.6718 0 .7154\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb and D =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n4.4027 0 0\\n0 0 .7187 0\\n0 0 0 .3160\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nThe columns of U show the directions of the principal axes of the ellipsoid, and the di-\\nagonal elements of D indicate the relative magnitudes of the principal axes. We see that\\nthe first principal component is given by the first column of U, and the second principal\\ncomponent by the second column of U.\\nThe projection of the point x = [1.052,0.6648,0.2271]⊤onto the 1-dimensional space\\nspanned by the first principal component u1 = [0.8460,0.4972,0.1922]⊤ is z = u1u⊤\\n1 x ='),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 172, 'page_label': '155'}, page_content='spanned by the first principal component u1 = [0.8460,0.4972,0.1922]⊤ is z = u1u⊤\\n1 x =\\n[1.0696,0.6287,0.2429]⊤. With respect to the basis vectoru1, z is represented by the num-\\nber u⊤\\n1 z = 1.2643. That is, z = 1.2643u1.\\nFigure 4.12: A “surfboard” ellipsoid where one principal axis is significantly larger than\\nthe other two.\\n4.8.2 PCA and Singular Value Decomposition (SVD)\\nIn the setting above, we did not consider any data set drawn from a multivariate pdff . The\\nwhole analysis rested on linear algebra. In principal component analysis principal\\ncomponent\\nanalysis\\n(PCA) we start\\nwith data x1,..., xn, where each x is d-dimensional. PCA does not require assumptions\\nhow the data were obtained, but to make the link with the previous section, we can think\\nof the data as iid draws from a multivariate normal pdf.\\nLet us collect the data in a matrix X in the usual way; that is, ☞ 44\\nX =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx11 x12 ... x1d\\nx21 x22 ... x2d\\n... ... ... ...\\nxn1 xn2 ... xnd\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n='),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 172, 'page_label': '155'}, page_content='x11 x12 ... x1d\\nx21 x22 ... x2d\\n... ... ... ...\\nxn1 xn2 ... xnd\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx⊤\\n1\\nx⊤\\n2\\n...\\nx⊤\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 173, 'page_label': '156'}, page_content='156 Principal Component Analysis (PCA)\\nThe matrix X will be the PCA’s input. Under this setting, the data consists of points in d-\\ndimensional space, and our goal is to present the data using n feature vectors of dimension\\nk <d.\\nIn accordance with the previous section, we assume that underlying distribution of the\\ndata has expectation vector 0. In practice, this means that before PCA is applied, the data\\nneeds to be centered by subtracting the column mean in every column:\\nx′\\ni j = xi j −xj,\\nwhere xj = 1\\nn\\nPn\\ni=1 xi j.\\nWe assume from now on that the data comes from a generald-dimensional distribution\\nwith mean vector0 and some covariance matrixΣ. The covariance matrixΣ is by definition\\nequal to the expectation of the random matrix XX⊤, and can be estimated from the data\\nx1,..., xn via the sample average\\nbΣ = 1\\nn\\nnX\\ni=1\\nxi x⊤\\ni = 1\\nnX⊤X.\\nAs bΣ is a covariance matrix, we may conduct the same analysis forbΣ as we did for Σ in the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 173, 'page_label': '156'}, page_content='nX⊤X.\\nAs bΣ is a covariance matrix, we may conduct the same analysis forbΣ as we did for Σ in the\\nprevious section. Specifically, supposebΣ = UD2U⊤is an SVD ofbΣ and let Uk be the matrix\\nwhose columns are thek principal components; that is, thek columns of U corresponding to\\nthe largest diagonal elements in D2. Note that we have used D2 instead of D to be compat-\\nible with the previous section. The transformation zi = UkU⊤\\nk xi maps each vector xi ∈Rd\\n(thus, with d features) to a vector zi ∈Rd lying in the subspace spanned by the columns of\\nUk. With respect to this basis, the pointzi has representation zi = U⊤\\nk (UkU⊤\\nk xi) = U⊤\\nk xi ∈Rk\\n(thus with k features). The corresponding covariance matrix of the zi,i = 1,..., n is diag-\\nonal. The diagonal elements {dℓℓ}of D can be interpreted as standard deviations of the data\\nin the directions of the principal components. The quantityv = P\\nℓ=1 d2\\nℓℓ (that is, the trace of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 173, 'page_label': '156'}, page_content='in the directions of the principal components. The quantityv = P\\nℓ=1 d2\\nℓℓ (that is, the trace of\\nD2) is thus a measure for the amount of variance in the data. The proportiond2\\nℓℓ/v indicates\\nhow much of the variance in the data is explained by the ℓ-th principal component.\\nAnother way to look at PCA is by considering the question: How can we best project the\\ndata onto a k-dimensional subspace in such a way that the total squared distance between\\nthe projected points and the original points is minimal? From Section A.4, we know that☞362\\nany orthogonal projection to a k-dimensional subspace Vk can be represented by a matrix\\nUkU⊤\\nk , where Uk = [u1,..., uk] and the {uℓ,ℓ = 1,..., k}are orthogonal vectors of length 1\\nthat span Vk. The above question can thus be formulated as the minimization program:\\nmin\\nu1,...,uk\\nnX\\ni=1\\n∥xi −UkU⊤\\nk xi∥2. (4.43)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 174, 'page_label': '157'}, page_content='Unsupervised Learning 157\\nNow observe that\\n1\\nn\\nnX\\ni=1\\n∥xi −UkU⊤\\nk xi∥2 = 1\\nn\\nnX\\ni=1\\n(x⊤\\ni −x⊤\\ni UkU⊤\\nk )(xi −UkU⊤\\nk xi)\\n= 1\\nn\\nnX\\ni=1\\n∥xi∥2\\n|       {z       }\\nc\\n−1\\nn\\nnX\\ni=1\\nx⊤\\ni UkU⊤\\nk xi = c −1\\nn\\nnX\\ni=1\\nkX\\nℓ=1\\ntr(x⊤\\ni uℓu⊤\\nℓ xi)\\n= c −1\\nn\\nkX\\nℓ=1\\nnX\\ni=1\\nu⊤\\nℓ xi x⊤\\ni uℓ = c −\\nkX\\nℓ=1\\nu⊤\\nℓ bΣ uℓ,\\nwhere we have used the cyclic property of a trace (Theorem A.1) and the fact that UkU⊤\\nk ☞ 357\\ncan be written as Pk\\nℓ=1 uℓu⊤\\nℓ . It follows that the minimization problem(4.43) is equivalent\\nto the maximization problem\\nmax\\nu1,...,uk\\nkX\\nℓ=1\\nu⊤\\nℓ bΣ uℓ. (4.44)\\nThis maximum can be at most Pk\\nℓ=1 d2\\nℓℓ and is attained precisely when u1,..., uk are the\\nfirst k principal components of bΣ.\\nExample 4.11 (Singular Value Decomposition) The following data set consists of in-\\ndependent samples from the three-dimensional Gaussian distribution with mean vector 0\\nand covariance matrix Σ given in Example 4.10:\\nX =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3.1209 1 .7438 0 .5479\\n−2.6628 −1.5310 −0.2763'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 174, 'page_label': '157'}, page_content='X =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3.1209 1 .7438 0 .5479\\n−2.6628 −1.5310 −0.2763\\n3.7284 3 .0648 1 .8451\\n0.4203 0 .3553 0 .4268\\n−0.7155 −0.6871 −0.1414\\n5.8728 4 .0180 1 .4541\\n4.8163 2 .4799 0 .5637\\n2.6948 1 .2384 0 .1533\\n−1.1376 −0.4677 −0.2219\\n−1.2452 −0.9942 −0.4449\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nAfter replacing X with its centered version, an SVD UD2U⊤ of bΣ = X⊤X/n yields the\\nprincipal component matrix U and diagonal matrix D:\\nU =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n−0.8277 0 .4613 0 .3195\\n−0.5300 −0.4556 −0.7152\\n−0.1843 −0.7613 0 .6216\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb and D =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3.3424 0 0\\n0 0 .4778 0\\n0 0 0 .1038\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nWe also observe that, apart from the sign of the first column, the principal component\\nmatrix U is similar to that in Example 4.10. Likewise for the matrixD. We see that 97.90%\\nof the total variance is explained by the first principal component. Figure 4.13 shows the\\nprojection of the centered data onto the subspace spanned by this principal component.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 175, 'page_label': '158'}, page_content=\"158 Principal Component Analysis (PCA)\\nx\\n4 2 0 2 4\\ny\\n2\\n1\\n0\\n1\\n2\\n3\\nz\\n1.0\\n0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nFigure 4.13: Data from the “surfboard” pdf is projected onto the subspace spanned by the\\nlargest principal component.\\nThe following Python code was used.\\nPCAdat.py\\nimport numpy as np\\nX = np.genfromtxt( 'pcadat.csv ', delimiter= ',')\\nn = X.shape[0]\\nX = X - X.mean(axis=0)\\nG = X.T @ X\\nU, _ , _ = np.linalg.svd(G/n)\\n# projected points\\nY = X @ np.outer(U[:,0],U[:,0])\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nfig = plt.figure()\\nax = fig.add_subplot(111, projection= '3d')\\nax.w_xaxis.set_pane_color((0, 0, 0, 0))\\nax.plot(Y[:,0], Y[:,1], Y[:,2], c= 'k', linewidth=1)\\nax.scatter(X[:,0], X[:,1], X[:,2], c= 'b')\\nax.scatter(Y[:,0], Y[:,1], Y[:,2], c= 'r')\\nfor i in range (n):\\nax.plot([X[i,0], Y[i,0]], [X[i,1],Y[i,1]], [X[i,2],Y[i,2]], 'b')\\nax.set_xlabel( 'x')\\nax.set_ylabel( 'y')\\nax.set_zlabel( 'z')\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 175, 'page_label': '158'}, page_content=\"ax.set_xlabel( 'x')\\nax.set_ylabel( 'y')\\nax.set_zlabel( 'z')\\nplt.show()\\nNext is an application of PCA to Fisher’s famous iris data set, already mentioned in\\nSection 1.1, and Exercise 1.5.☞2\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 176, 'page_label': '159'}, page_content=\"Unsupervised Learning 159\\nExample 4.12 (PCA for the Iris Data Set) The iris data set contains measurements\\non four features of the iris plant: sepal length and width, and petal length and width, for a\\ntotal of 150 specimens. The full data set also contains the species name, but for the purpose\\nof this example we ignore it.\\nFigure 1.9 shows that there is a significant correlation between the di fferent features. ☞ 17\\nCan we perhaps describe the data using fewer features by taking certain linear combin-\\nations of the original features? To investigate this, let us perform a PCA, first centering\\nthe data. The following Python code implements the PCA. It is assumed that a CSV file\\nirisX.csvhas been made that contains the iris data set (without the species information).\\nPCAiris.py\\nimport seaborn as sns, numpy as np\\nnp.set_printoptions(precision=4)\\nX = np.genfromtxt( 'IrisX.csv ',delimiter= ',')\\nn = X.shape[0]\\nX = X - np.mean(X, axis=0)\\n[U,D2,UT]= np.linalg.svd((X.T @ X)/n)\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 176, 'page_label': '159'}, page_content=\"n = X.shape[0]\\nX = X - np.mean(X, axis=0)\\n[U,D2,UT]= np.linalg.svd((X.T @ X)/n)\\nprint ('U = \\\\n ', U); print ('\\\\n diag(D^2) = ', D2)\\nz = U[:,0].T @ X.T\\nsns.kdeplot(z, bw=0.15)\\nU =\\n[[-0.3614 -0.6566 0.582 0.3155]\\n[ 0.0845 -0.7302 -0.5979 -0.3197]\\n[-0.8567 0.1734 -0.0762 -0.4798]\\n[-0.3583 0.0755 -0.5458 0.7537]]\\ndiag(D^2) = [4.2001 0.2411 0.0777 0.0237]\\nThe output above shows the principal component matrix (which we calledU) as well as\\nthe diagonal of matrix D2. We see that a large proportion of the variance, 4.2001/(4.2001+\\n0.2411+0.0777+0.0237) = 92.46%, is explained by the first principal component. Thus, it\\nmakes sense to transform each data point x ∈R4 to u⊤\\n1 x ∈R. Figure 4.14 shows the kernel\\ndensity estimate of the transformed data. Interestingly, we see two modes, indicating at\\nleast two clusters in the data.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 177, 'page_label': '160'}, page_content='160 Exercises\\n-4 -3 -2 -1 0 1 2 3 4\\nPCA-combined data\\n0\\n0.2\\n0.4\\n0.6kernel density estimate\\nFigure 4.14: Kernel density estimate of the PCA-combined iris data.\\nFurther Reading\\nVarious information-theoretic measures to quantify uncertainty, including the Shannon en-\\ntropy and Kullback–Leibler divergence, may be found in [28]. The Fisher information, the\\nprominent information measure in statistics, is discussed in detail in [78]. Akaike’s inform-\\nation criterion appeared in [2]. The EM algorithm was introduced in [31] and [85] gives an\\nin-depth treatment. Convergence proofs for the EM algorithm may be found in [19, 128].\\nA classical reference on kernel density estimation is [113], and [14] is the main reference\\nfor the theta kernel density estimator. Theory and applications on finite mixture models\\nmay be found in [86]. For more details on clustering applications and algorithms as well\\nas references on data compression, vector quantization, and pattern recognition, we refer'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 177, 'page_label': '160'}, page_content='as references on data compression, vector quantization, and pattern recognition, we refer\\nto [1, 35, 107, 125]. A useful modification of the K-means algorithm is the fuzzy K-means\\nalgorithm; see, e.g., [9]. A popular way to choose the starting positions inK-means is given\\nby the K-means++ heuristic, introduced in [4].\\nExercises\\n1. This exercise is to show that the Fisher information matrix F(θ) in (4.8) is equal to the\\nmatrix H(θ) in (4.9), in the special case where f = g(·|θ), and under the assumption that\\nintegration and differentiation orders can be interchanged.\\n(a) Let h be a vector-valued function and k a real-valued function. Prove the following\\nquotient rule for differentiationquotient rule\\nfor\\ndifferentiation\\n:\\n∂[h(θ)/k(θ)]\\n∂θ = 1\\nk(θ)\\n∂h(θ)\\n∂θ − 1\\nk2(θ)\\n∂k(θ)\\n∂θ h(θ)⊤. (4.45)\\n(b) Now take h(θ) = ∂g(X |θ)\\n∂θ and k(θ) = g(X |θ) in (4.45) and take expectations with\\nrespect to Eθ on both sides to show that\\n−H(θ) = Eθ\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\ng(X |θ)\\n∂∂g(X |θ)\\n∂θ\\n∂θ\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 177, 'page_label': '160'}, page_content='respect to Eθ on both sides to show that\\n−H(θ) = Eθ\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\ng(X |θ)\\n∂∂g(X |θ)\\n∂θ\\n∂θ\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|                    {z                    }\\nA\\n−F(θ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 178, 'page_label': '161'}, page_content='Unsupervised Learning 161\\n(c) Finally show that A is the zero matrix.\\n2. Plot the mixture of N(0,1), U(0,1), and Exp(1) distributions, with weights w1 = w2 =\\nw3 = 1/3.\\n3. Denote the pdfs in Exercise 2 by f1, f2, f3, respectively. Suppose that X is simulated via\\nthe two-step procedure: First, draw Z from {1,2,3}, then draw X from fZ. How likely is it\\nthat the outcome x = 0.5 of X has come from the uniform pdf f2?\\n4. Simulate an iid training set of size 100 from the Gamma(2.3,0.5) distribution, and\\nimplement the Fisher scoring method in Example 4.1 to find the maximum likelihood es-\\ntimate. Plot the true and approximate pdfs.\\n5. Let T = {X1,..., Xn}be iid data from a pdf g(x |θ) with Fisher matrix F(θ). Explain\\nwhy, under the conditions where (4.7) holds,\\nST(θ) := 1\\nn\\nnX\\ni=1\\nS(Xi |θ)\\nfor large n has approximately a multivariate normal distribution with expectation vector 0\\nand covariance matrix F(θ)/n.\\n6. Figure 4.15 shows a Gaussian KDE with bandwidth σ= 0.2 on the points −0.5,0,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 178, 'page_label': '161'}, page_content='6. Figure 4.15 shows a Gaussian KDE with bandwidth σ= 0.2 on the points −0.5,0,\\n0.2,0.9, and 1 .5. Reproduce the plot in Python. Using the same bandwidth, plot also the\\nKDE for the same data, but now with ϕ(z) = 1/2,z ∈[−1,1].\\n-1 0 1 2\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nFigure 4.15: The Gaussian KDE (solid line) is the equally weighted mixture of normal pdfs\\ncentered around the data and with standard deviation σ= 0.2 (dashed).\\n7. For fixed x′, the Gaussian kernel function\\nf (x |t) := 1√\\n2πt\\ne−1\\n2\\n(x−x′)2\\nt\\nis the solution to Fourier’s heat equation\\n∂\\n∂t f (x |t) = 1\\n2\\n∂2\\n∂x2 f (x |t), x ∈R, t >0,\\nwith initial condition f (x |0) = δ(x −x′) (the Dirac function at x′). Show this. As a con-\\nsequence, the Gaussian KDE is the solution to the same heat equation, but now with initial\\ncondition f (x |0) = n−1 Pn\\ni=1 δ(x −xi). This was the motivation for the theta KDE [14],\\nwhich is a solution to the same heat equation but now on a bounded interval.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 179, 'page_label': '162'}, page_content='162 Exercises\\n8. Show that the Ward linkage given in (4.41) is equal to\\ndWard(I,J) = |I||J|\\n|I|+ |J|∥xI−xJ∥2.\\n9. Carry out the agglomerative hierarchical clustering of Example 4.8 via the linkage\\nmethod from scipy.cluster.hierarchy. Show that the linkage matrices are the same.\\nGive a scatterplot of the data, color coded into K = 3 clusters.\\n10. Suppose that we have the data τn = {x1,..., xn}in R and decide to train the two-\\ncomponent Gaussian mixture model\\ng(x |θ) = w1\\n1q\\n2πσ2\\n1\\nexp\\n \\n−(x −µ1)2\\n2σ2\\n1\\n!\\n+ w2\\n1q\\n2πσ2\\n2\\nexp\\n \\n−(x −µ2)2\\n2σ2\\n2\\n!\\n,\\nwhere the parameter vector θ= [µ1,µ2,σ1,σ2,w1,w2]⊤belongs to the set\\nΘ = {θ: w1 + w2 = 1,w1 ∈[0,1],µi ∈R,σi >0, ∀i}.\\nSuppose that the training is via the maximum likelihood in (2.28). Show that\\nsup\\nθ∈Θ\\n1\\nn\\nnX\\ni=1\\nln g(xi |θ) = ∞.\\nIn other words, find a sequence of values for θ∈Θ such that the likelihood grows without\\nbound. How can we restrict the set Θ to ensure that the likelihood remains bounded?'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 179, 'page_label': '162'}, page_content='bound. How can we restrict the set Θ to ensure that the likelihood remains bounded?\\n11. A d-dimensional normal random vector X ∼N(µ,Σ) can be defined via an a ffine\\ntransformation, X = µ+ Σ1/2 Z, of a standard normal random vector Z ∼N(0,Id), where\\nΣ1/2(Σ1/2)⊤= Σ. In a similar way, we can define a d-dimensional Student random vector\\nX ∼tα(µ,Σ) via a transformation\\nX = µ+ 1√\\nS\\nΣ1/2 Z, (4.46)\\nwhere, Z ∼N(0,Id) and S ∼Gamma(α\\n2 ,α\\n2 ) are independent, α >0, and Σ1/2(Σ1/2)⊤ = Σ.\\nNote that we obtain the multivariate normal distribution as a limiting case for α→∞.\\n(a) Show that the density of the tα(0,Id) distribution is given by\\ntα(x) := Γ((α+ d)/2)\\n(πα)d/2Γ(α/2)\\n \\n1 + 1\\nα∥x∥2\\n!−α+d\\n2\\n.\\nBy the transformation rule (C.23), it follows that the density of X ∼tα(µ,Σ) is given☞433\\nby tα,Σ(x −µ), where\\ntα,Σ(x) := 1\\n|Σ1/2|tα(Σ−1/2 x).\\n[Hint: conditional on S = s, X has a N(0,Id/s) distribution.]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 180, 'page_label': '163'}, page_content='Unsupervised Learning 163\\n(b) We wish to fit a tν(µ,Σ) distribution to given data τ= {x1,..., xn}in Rd via the EM\\nmethod. We use the representation (4.46) and augment the data with the vector S =\\n[S 1,..., S n]⊤of hidden variables. Show that the complete-data likelihood is given by\\ng(τ,s |θ) =\\nY\\ni\\n(α/2)α/2 s(α+d)/2−1\\ni exp(−si\\n2 α−si\\n2 ∥Σ−1/2(xi −µ)∥2)\\nΓ(α/2)(2π)d/2|Σ1/2| . (4.47)\\n(c) Show that, as a consequence, conditional on the data τand parameter θ, the hidden\\ndata are mutually independent, and\\n(S i |τ,θ) ∼Gamma\\n α+ d\\n2 ,α+ ∥Σ−1/2(xi −µ)∥2\\n2\\n!\\n, i = 1,..., n.\\n(d) At iteration t of the EM algorithm, let g(t)(s) = g(s |τ,θ(t−1)) be the density of the\\nmissing data, given the observed data τand the current parameter guess θ(t−1). Verify\\nthat the expected complete-data log-likelihood is given by:\\nEg(t) ln g(τ,S |θ) = nα\\n2 ln α\\n2 −nd\\n2 ln(2π) −n ln Γ\\n\\x12α\\n2\\n\\x13\\n−n\\n2 ln |Σ|\\n+ α+ d −2\\n2\\nnX\\ni=1\\nEg(t) ln S i −\\nnX\\ni=1\\nα+ ∥Σ−1/2(xi −µ)∥2\\n2 Eg(t) S i.\\nShow that\\nEg(t) S i = α(t−1) + d'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 180, 'page_label': '163'}, page_content='2\\nnX\\ni=1\\nEg(t) ln S i −\\nnX\\ni=1\\nα+ ∥Σ−1/2(xi −µ)∥2\\n2 Eg(t) S i.\\nShow that\\nEg(t) S i = α(t−1) + d\\nα(t−1) + ∥(Σ(t−1))−1/2(xi −µ(t−1))∥2 =: w(t−1)\\ni\\nEg(t) ln S i = ψ\\n α(t−1) + d\\n2\\n!\\n−ln\\n α(t−1) + d\\n2\\n!\\n+ ln w(t−1)\\ni ,\\nwhere ψ:= (ln Γ)′is digamma function.\\n(e) Finally, show that in the M-step of the EM algorithm θ(t) is updated from θ(t−1) as\\nfollows:\\nµ(t) =\\nPn\\ni=1 w(t−1)\\ni xi\\nPn\\ni=1 w(t−1)\\ni\\nΣ(t) = 1\\nn\\nnX\\ni=1\\nw(t−1)\\ni (xi −µ(t))(xi −µ(t))⊤,\\nand α(t) is defined implicitly through the solution of the nonlinear equation:\\nln\\n\\x12α\\n2\\n\\x13\\n−ψ\\n\\x12α\\n2\\n\\x13\\n+ ψ\\n α(t) + d\\n2\\n!\\n−ln\\n α(t) + d\\n2\\n!\\n+ 1 +\\nPn\\ni=1\\n\\x10\\nln(w(t−1)\\ni ) −w(t−1)\\ni\\n\\x11\\nn = 0.\\n12. A generalization of both the gamma and inverse-gamma distribution is thegeneralized\\ninverse-gamma distribution generalized\\ninverse-gamma\\ndistribution\\n, which has density\\nf (s) = (a/b)p/2\\n2Kp(\\n√\\nab)\\nsp−1e−1\\n2 (as+b/s), a,b,s >0, p ∈R, (4.48)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 181, 'page_label': '164'}, page_content='164 Exercises\\nwhere Kp is the modified Bessel function of the second kindmodified Bessel\\nfunction of the\\nsecond kind\\n, which can be defined as the\\nintegral\\nKp(x) =\\nZ ∞\\n0\\ne−x cosh(t) cosh(pt) dt, x >0, p ∈R. (4.49)\\nWe write S ∼GIG(a,b,p) to denote that S has a pdf of the form (4.48). The function Kp\\nhas many interesting properties. Special cases include\\nK1/2(x) =\\nr\\nx π\\n2 e−x 1\\nx\\nK3/2(x) =\\nr\\nx π\\n2 e−x\\n 1\\nx + 1\\nx2\\n!\\nK5/2(x) =\\nr\\nx π\\n2 e−x\\n 1\\nx + 3\\nx2 + 3\\nx3\\n!\\n.\\nMore generally, Kp satisfies the recursion\\nKp+1(x) = Kp−1(x) + 2p\\nx Kp(x). (4.50)\\n(a) Using the change of variables e z = s √a/b, show that\\nZ ∞\\n0\\nsp−1e−1\\n2 (as+b/s) ds = 2Kp(\\n√\\nab)(b/a)p/2.\\n(b) Let S ∼GIG(a,b,p). Show that\\nES =\\n√\\nb Kp+1(\\n√\\nab)\\n√a Kp(\\n√\\nab)\\n(4.51)\\nand\\nES −1 =\\n√a Kp+1(\\n√\\nab)√\\nb Kp(\\n√\\nab)\\n−2p\\nb . (4.52)\\n13. In Exercise 11 we viewed the multivariate Student tα distribution as a scale-mixturescale-mixture\\nof the N(0,Id) distribution. In this exercise, we consider a similar transformation, but now'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 181, 'page_label': '164'}, page_content='of the N(0,Id) distribution. In this exercise, we consider a similar transformation, but now\\nΣ1/2 Z ∼N(0,Σ) is not divided but is multiplied by\\n√\\nS , with S ∼Gamma(α/2,α/2):\\nX = µ+\\n√\\nS Σ1/2 Z, (4.53)\\nwhere S and Z are independent and α> 0.\\n(a) Show, using Exercise 12, that for Σ1/2 = Id and µ= 0, the random vector X has a\\nd-dimensional Bessel distributionBessel\\ndistribution\\n, with density:\\nκα(x) := 21−(α+d)/2α(α+d)/4 ∥x∥(α−d)/2\\nπd/2Γ(α/2) K(α−d)/2\\n\\x10\\n∥x∥√α\\n\\x11\\n, x ∈Rd,\\nwhere Kp is the modified Bessel function of the second kind given in (4.49). We write\\nX ∼Besselα(0,Id). A random vector X is said to have a Besselα(µ,Σ) distribution if'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 182, 'page_label': '165'}, page_content='Unsupervised Learning 165\\nit can be written in the form (4.53). By the transformation rule (C.23), its density is\\ngiven by 1√|Σ|κα(Σ−1/2(x −µ)). Special instances of the Bessel pdf include:\\nκ2(x) = exp(−\\n√\\n2 |x|)√\\n2\\nκ4(x) = 1 + 2 |x|\\n2 exp(−2 |x|)\\nκ4(x1,x2,x3) = 1\\nπexp\\n\\x12\\n−2\\nq\\nx2\\n1 + x2\\n2 + x2\\n3\\n\\x13\\nκd+1(x) = ((d + 1)/2)d/2 √π\\n(2π)d/2Γ((d + 1)/2) exp\\n\\x10\\n−\\n√\\nd + 1 ∥x∥\\n\\x11\\n, x ∈Rd.\\nNote that k2 is the (scaled) pdf of the double-exponential or Laplace distribution.\\n(b) Given the data τ = {x1,..., xn}in Rd, we wish to fit a Bessel pdf to the data by\\nemploying the EM algorithm, augmenting the data with the vector S = [S 1,..., S n]⊤\\nof missing data. We assume that α is known and α >d. Show that conditional on\\nτ (and given θ), the missing data vector S has independent components, with S i ∼\\nGIG(α,bi,(α−d)/2), with bi := ∥Σ−1/2(xi −µ)∥2, i = 1,..., n.\\n(c) At iteration t of the EM algorithm, let g(t)(s) = g(s |τ,θ(t−1)) be the density of the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 182, 'page_label': '165'}, page_content='(c) At iteration t of the EM algorithm, let g(t)(s) = g(s |τ,θ(t−1)) be the density of the\\nmissing data, given the observed data τand the current parameter guess θ(t−1). Show\\nthat the expected complete-data log-likelihood is given by:\\nQ(t)(θ) := Eg(t) ln g(τ,S |θ) = −1\\n2\\nnX\\ni=1\\nbi(θ) w(t−1)\\ni + constant, (4.54)\\nwhere bi(θ) = ∥Σ−1/2(xi −µ)∥2 and\\nw(t−1)\\ni :=\\n√αK(α−d+2)/2\\n\\x10p\\nαbi(θ(t−1))\\n\\x11\\np\\nbi(θ(t−1)) K(α−d)/2\\n\\x10p\\nαbi(θ(t−1))\\n\\x11 − α−d\\nbi(θ(t−1)), i = 1,..., n.\\n(d) From (4.54) derive the M-step of the EM algorithm. That is, show how θ(t) is updated\\nfrom θ(t−1).\\n14. Consider the ellipsoid E = {x ∈Rd : xΣ−1 x = 1}in (4.42). Let UD2U⊤be an SVD of\\nΣ. Show that the linear transformation x 7→U⊤D−1 x maps the points on E onto the unit\\nsphere {z ∈Rd : ∥z∥= 1}.\\n15. Figure 4.13 shows how the centered “surfboard” data are projected onto the first\\ncolumn of the principal component matrix U. Suppose we project the data instead onto'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 182, 'page_label': '165'}, page_content='column of the principal component matrix U. Suppose we project the data instead onto\\nthe plane spanned by the first two columns of U. What are a and b in the representation\\nax1 + bx2 = x3 of this plane?\\n16. Figure 4.14 suggests that we can assign each feature vector x in the iris data set to\\none of two clusters, based on the value of u⊤\\n1 x, where u1 is the first principal component.\\nPlot the sepal lengths against petal lengths and color the points for whichu⊤\\n1 x <1.5 differ-\\nently to points for which u⊤\\n1 x ⩾1.5. To which species of iris do these clusters correspond?'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 183, 'page_label': '166'}, page_content='166'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 184, 'page_label': '167'}, page_content='CHAPTER 5\\nREGRESSION\\nMany supervised learning techniques can be gathered under the name “regression”.\\nThe purpose of this chapter is to explain the mathematical ideas behind regression\\nmodels and their practical aspects. We analyze the fundamental linear model in detail,\\nand also discuss nonlinear and generalized linear models.\\n5.1 Introduction\\nFrancis Galton observed in an article in 1889 that the heights of adult offspring are, on the\\nwhole, more “average” than the heights of their parents. Galton interpreted this as a degen-\\nerative phenomenon, using the term “regression” to indicate this “return to mediocrity”.\\nNowadays, regression regressionrefers to a broad class of supervised learning techniques where the\\naim is to predict a quantitative response (output) variable y via a function g(x) of an ex-\\nplanatory (input) vector x = [x1,..., xp]⊤, consisting of p features, each of which can be\\ncontinuous or discrete. For instance, regression could be used to predict the birth weight of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 184, 'page_label': '167'}, page_content='continuous or discrete. For instance, regression could be used to predict the birth weight of\\na baby (the response variable) from the weight of the mother, her socio-economic status,\\nand her smoking habits (the explanatory variables).\\nLet us recapitulate the framework of supervised learning established in Chapter 2. The ☞ 19\\naim is to find a prediction function g that best guesses1 what the random output Y will be\\nfor a random input vector X. The joint pdf f (x,y) of X and Y is unknown, but a training\\nset τ= {(x1,y1),..., (xn,yn)}is available, which is thought of as the outcome of a random\\ntraining set T= {(X1,Y1),..., (Xn,Yn)}of iid copies of ( X,Y). Once we have selected a\\nloss function Loss(y,by), such as the squared-error loss squared-error\\nloss\\nLoss(y,by) = (y −by)2, (5.1)\\nthen the “best” prediction function g is defined as the one that minimizes the risk riskℓ(g) =\\nELoss(Y,g(X)). We saw in Section 2.2 that for the squared-error loss this optimal predic-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 184, 'page_label': '167'}, page_content='ELoss(Y,g(X)). We saw in Section 2.2 that for the squared-error loss this optimal predic-\\ntion function is the conditional expectation\\ng∗(x) = E[Y |X = x].\\n1Recall the mnemonic use of “g” for “guess”\\n167'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 185, 'page_label': '168'}, page_content='168 Introduction\\nAs the squared-error loss is the most widely-used loss function for regression, we will\\nadopt this loss function in most of this chapter.\\nThe optimal prediction function g∗has to be learned from the training set τby minim-\\nizing the training loss\\nℓτ(g) = 1\\nn\\nnX\\ni=1\\n(yi −g(xi))2 (5.2)\\nover a suitable class of functions G. Note that in the above definition, the training set τis\\nassumed to be fixed. For a random training set T, we will write the training loss as ℓT(g).\\nThe function gG\\nτ that minimizes the training loss is the function we use for prediction —\\nthe so-called learnerlearner . When the function class Gis clear from the context, we drop the\\nsuperscript in the notation.\\nAs we already saw in (2.2), conditional on X = x, the response Y can be written as☞21\\nY = g∗(x) + ε(x),\\nwhere Eε(x) = 0. This motivates a standard modeling assumption in supervised learn-\\ning, in which the responses Y1,..., Yn, conditional on the explanatory variables X1 ='),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 185, 'page_label': '168'}, page_content='ing, in which the responses Y1,..., Yn, conditional on the explanatory variables X1 =\\nx1,..., Xn = xn, are assumed to be of the form\\nYi = g(xi) + εi, i = 1,..., n,\\nwhere the {εi}are independent with Eεi = 0 and Var εi = σ2 for some function g ∈G and\\nvariance σ2. The above model is usually further specified by assuming thatg is completely\\nknown up to an unknown parameter vector; that is,\\nYi = g(xi |β) + εi, i = 1,..., n. (5.3)\\nWhile the model (5.3) is described conditional on the explanatory variables, it will be\\nconvenient to make one further model simplification, and view (5.3) as if the {xi}were\\nfixed, while the {Yi}are random.\\nFor the remainder of this chapter, we assume that the training feature vectors{xi}are\\nfixed and only the responses are random; that is, T= {(x1,Y1),..., (xn,Yn)}.\\nThe advantage of the model (5.3) is that the problem of estimating the function g from\\nthe training data is reduced to the (much simpler) problem of estimating the parameter'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 185, 'page_label': '168'}, page_content='the training data is reduced to the (much simpler) problem of estimating the parameter\\nvector β. An obvious disadvantage is that functions of the form g(·|β) may not accurately\\napproximate the true unknown g∗. The remainder of this chapter deals with the analysis\\nof models of the form (5.3). In the important case where the function g(·|β) is linear, the\\nanalysis proceeds through the class of linear models. If, in addition, the error terms{εi}are\\nassumed to be Gaussian, this analysis can be carried out using the rich theory of normal\\nlinear models.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 186, 'page_label': '169'}, page_content='Regression 169\\n5.2 Linear Regression\\nThe most basic regression model involves a linear relationship between the response and a\\nsingle explanatory variable. In particular, we have measurements ( x1,y1),..., (xn,yn) that\\nlie approximately on a straight line, as in Figure 5.1.\\n-3 -2 -1 0 1 2 3\\n-5\\n0\\n5\\n10\\n15\\nFigure 5.1: Data from a simple linear regression model.\\nFollowing the general scheme captured in (5.3), a simple model for these data is that\\nthe {xi}are fixed and variables {Yi}are random such that\\nYi = β0 + β1 xi + εi, i = 1,..., n, (5.4)\\nfor certain unknown parameters β0 and β1. The {εi}are assumed to be independent with\\nexpectation 0 and unknown variance σ2. The unknown line\\ny = β0 + β1 x|    {z    }\\ng(x |β)\\n(5.5)\\nis called the regression line regression line. Thus, we view the responses as random variables that would\\nlie exactly on the regression line, were it not for some “disturbance” or “error” term repres-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 186, 'page_label': '169'}, page_content='lie exactly on the regression line, were it not for some “disturbance” or “error” term repres-\\nented by the {εi}. The extent of the disturbance is modeled by the parameterσ2. The model\\nin (5.4) is calledsimple linear regression simple linear\\nregression\\nmodel\\n. This model can easily be extended to incorporate\\nmore than one explanatory variable, as follows.\\nDefinition 5.1: Multiple Linear Regression Model\\nIn a multiple linear regression model multiple linear\\nregression\\nmodel\\nthe response Y depends on a d-dimensional\\nexplanatory vector x = [x1,..., xd]⊤, via the linear relationship\\nY = β0 + β1 x1 + ··· + βd xd + ε, (5.6)\\nwhere Eε= 0 and Var ε= σ2.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 187, 'page_label': '170'}, page_content='170 Linear Regression\\nThus, the data lie approximately on a d-dimensional affine hyperplane\\ny = β0 + β1 x1 + ··· + βd xd|                      {z                      }\\ng(x |β)\\n,\\nwhere we define β= [β0,β1,...,β d]⊤. The function g(x |β) is linear in β, but not linear in\\nthe feature vector x, due to the constant β0. However, augmenting the feature space with\\nthe constant 1, the mapping [1 ,x⊤]⊤ 7→g(x |β) := [1,x⊤] βbecomes linear in the feature\\nspace and so (5.6) becomes a linear model (see Section 2.1). Most software packages for☞44\\nregression include 1 as a feature by default.\\nNote that in (5.6) we only specified the model for a single pair (x,Y). The model for the\\ntraining set T = {(x1,Y1),..., (xn,Yn)}is simply that each Yi satisfies (5.6) (with x = xi)\\nand that the {Yi}are independent. Setting Y = [Y1,..., Yn]⊤, we can write the multiple\\nlinear regression model for the training data compactly as\\nY = Xβ+ ε, (5.7)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 187, 'page_label': '170'}, page_content='linear regression model for the training data compactly as\\nY = Xβ+ ε, (5.7)\\nwhere ε= [ε1,...,ε n]⊤is a vector of iid copies of εand X is the model matrixmodel matrix given by\\nX =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 x11 x12 ··· x1d\\n1 x21 x22 ··· x2d\\n... ... ... ... ...\\n1 xn1 xn2 ··· xnd\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 x⊤\\n1\\n1 x⊤\\n2\\n... ...\\n1 x⊤\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nExample 5.1 (Multiple Linear Regression Model) Figure 5.2 depicts a realization of\\nthe multiple linear regression model\\nYi = xi1 + xi2 + εi, i = 1,..., 100,\\nwhere ε1,...,ε 100 ∼iid N(0,1/16). The fixed feature vectors (vectors of explanatory vari-\\nables) xi = [xi1,xi2]⊤,i = 1,..., 100 lie in the unit square.\\n1\\n0\\n0\\n1\\n2\\n1 0\\nFigure 5.2: Data from a multiple linear regression model.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 188, 'page_label': '171'}, page_content='Regression 171\\n5.3 Analysis via Linear Models\\nAnalysis of data from a linear regression model is greatly simplified through the linear\\nmodel representation (5.7). In this section we present the main ideas for parameter estima-\\ntion and model selection for a general linear model of the form\\nY = Xβ+ ε, (5.8)\\nwhere X is an n×p matrix, β= [β1,...,β p]⊤a vector of p parameters, and ε= [ε1,...,ε n]⊤\\nan n-dimensional vector of independent error terms, with Eεi = 0 and Var εi = σ2, i =\\n1,..., n. Note that the model matrix X is assumed to be fixed, and Y and εare random. A\\nspecific outcome of Y is denoted by y (in accordance with the notation in Section 2.8). ☞ 47\\nNote that the multiple linear regression model in (5.7) was defined using a different\\nparameterization; in particular, there we used β= [β0,β1,...,β d]⊤. So, when apply-\\ning the results in the present section to such models, be aware that p = d + 1. Also,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 188, 'page_label': '171'}, page_content='ing the results in the present section to such models, be aware that p = d + 1. Also,\\nin this section a feature vector x includes the constant 1, so that X⊤= [x1,..., xn].\\n5.3.1 Parameter Estimation\\nThe linear model Y = Xβ+ εcontains two unknown parameters, βand σ2, which have\\nto be estimated from the training data τ. To estimate β, we can repeat exactly the same\\nreasoning used in our recurring polynomial regression Example 2.1 as follows. For a linear ☞ 26\\nprediction function g(x) = x⊤β, the (squared-error) training loss can be written as\\nℓτ(g) = 1\\nn ∥y −Xβ∥2,\\nand the optimal learner gτ minimizes this quantity, leading to the least-squares estimate bβ,\\nwhich satisfies the normal equations\\nX⊤X β= X⊤y. (5.9)\\nThe corresponding training loss can be taken as an estimate of σ2; that is,\\ncσ2 = 1\\nn ∥y −Xbβ∥2. (5.10)\\nTo justify the latter, note that σ2 is the second moment of the model errors εi,i = 1,..., n,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 188, 'page_label': '171'}, page_content='To justify the latter, note that σ2 is the second moment of the model errors εi,i = 1,..., n,\\nin (5.8) and could be estimated via the method of moments (see Section C.12.1) using the ☞ 455\\nsample average n−1 P\\ni ε2\\ni = ∥ε∥2/n = ∥Y −Xβ∥2/n, if βwere known. By replacing βwith\\nits estimator, we arrive at (5.10). Note that no distributional properties of the{εi}were used\\nother than Eεi = 0 and Var εi = σ2,i = 1,..., n. The vector e := y −Xbβ is called the\\nvector of residuals residualsand approximates the (unknown) vector of model errorsε. The quantity\\n∥e∥2 = Pn\\ni=1 e2\\ni is called the residual sum of squares(RSS). Dividing the RSS byn −p gives residual sum of\\nsquaresan unbiased estimate of σ2, which we call the estimated residual squared error(RSE); see\\nresidual\\nsquared errorExercise 12.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 189, 'page_label': '172'}, page_content='172 Analysis via Linear Models\\nIn terms of the notation given in the summary Table 2.1 for supervised learning, we\\nthus have:☞25\\n1. The (observed) training data is τ= {X,y}.\\n2. The function class Gis the class of linear functions of x; that is G= {g(·|β) : x 7→\\nx⊤β, β∈Rp}.\\n3. The (squared-error) training loss is ℓτ(g(·|β)) = ∥y −Xβ∥2/n.\\n4. The learner gτ is given by gτ(x) = x⊤bβ, where bβ= argminβ∈Rp ∥y −Xβ∥2.\\n5. The minimal training loss is ℓτ(gτ) = ∥y −Xbβ∥2/n = cσ2.\\n5.3.2 Model Selection and Prediction\\nEven if we restrict the learner to be a linear function, there is still the issue of which explan-\\natory variables (features) to include. While including too few features may result in large\\napproximation error (underfitting), including too many may result in largestatistical error\\n(overfitting). As discussed in Section 2.4, we need to select the features which provide the☞31\\nbest tradeoff between the approximation and statistical errors, so that the (expected) gener-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 189, 'page_label': '172'}, page_content='best tradeoff between the approximation and statistical errors, so that the (expected) gener-\\nalization risk of the learner is minimized. Depending on how the (expected) generalization\\nrisk is estimated, there are a number of strategies for feature selection:\\n1. Use test data τ′= (X′,y′) that are obtained independently from the training data τ,\\nto estimate the generalization risk E∥Y −gτ(X)∥2 via the test loss (2.7). Then choose☞24\\nthe collection of features that minimizes the test loss. When there is an abundance of\\ndata, part of the data can be reserved as test data, while the remaining data is used as\\ntraining data.\\n2. When there is a limited amount of data, we can use cross-validation to estimate the\\nexpected generalization risk E∥Y −gT(X)∥2 (where Tis a random training set), as\\nexplained in Section 2.5.2. This is then minimized over the set of possible choices☞38\\nfor the explanatory variables.\\n3. When one has to choose between many potential explanatory variables, techniques'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 189, 'page_label': '172'}, page_content='3. When one has to choose between many potential explanatory variables, techniques\\nsuch as regularized least-squares and lasso regression become important. Such\\nmethods offer another approach to model selection, via the regularization (or ho-\\nmotopy) paths. This will be the topic of Section 6.2 in the next chapter.☞216\\n4. Rather than using computer-intensive techniques, such as the ones above, one can\\nuse theoretical estimates of the expected generalization risk, such as the in-sample\\nrisk, AIC, and BIC, as in Section 2.5, and minimize this to determine a good set of☞35\\nexplanatory variables.\\n5. All of the above approaches do not assume any distributional properties of the error\\nterms {εi}in the linear model, other than that they are independent with expectation\\n0 and variance σ2. If, however, they are assumed to have anormal (Gaussian) distri-\\nbution, (that is, {εi}∼iid N(0,σ2)), then the inclusion and exclusion of variables can'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 190, 'page_label': '173'}, page_content='Regression 173\\nbe decided by means of hypotheses tests. This is the classical approach to model\\nselection, and will be discussed in Section 5.4. As a consequence of the central limit\\ntheorem, one can use the same approach when the error terms are not necessarily\\nnormal, provided their variance is finite and the sample size n is large.\\n6. Finally, when using a Bayesian approach, comparison of two models can be achieved\\nby computing their so-called Bayes factor (see Section 2.9).\\nAll of the above strategies can be thought of as specifications of a simple rule formu-\\nlated by William of Occam, which can be interpreted as:\\nWhen presented with competing models, choose the simplest one that explains\\nthe data.\\nThis age-old principle, known asOccam’s razor Occam’s razor, is mirrored in a famous quote of Einstein:\\nEverything should be made as simple as possible, but not simpler.\\nIn linear regression, the number of parameters or predictors is usually a reasonable measure'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 190, 'page_label': '173'}, page_content='In linear regression, the number of parameters or predictors is usually a reasonable measure\\nof the simplicity of the model.\\n5.3.3 Cross-Validation and Predictive Residual Sum of Squares\\nWe start by considering the n-fold cross-validation, also called leave-one-out cross-\\nvalidation leave-one-out\\ncross-validation\\n, for the linear model (5.8). We partition the data into n data sets, leaving out\\nprecisely one observation per data set, which we then predict based on then −1 remaining\\nobservations; see Section 2.5.2 for the general case. Let by−i denote the prediction for the ☞ 38\\ni-th observation using all the data except yi. The error in the prediction, yi −by−i, is called a\\npredicted residual predicted\\nresidual\\n— in contrast to an ordinary residual,ei = yi −byi, which is the difference\\nbetween an observation and its fitted valuebyi = gτ(xi) obtained using the whole sample. In\\nthis way, we obtain the collection of predicted residuals {yi −by−i}n\\ni=1 and summarize them'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 190, 'page_label': '173'}, page_content='this way, we obtain the collection of predicted residuals {yi −by−i}n\\ni=1 and summarize them\\nthrough the predicted residual sum of squares (PRESS PRESS):\\nPRESS =\\nnX\\ni=1\\n(yi −by−i)2.\\nDividing the PRESS by n gives an estimate of the expected generalization risk.\\nIn general, computing the PRESS is computationally intensive as it involves training\\nand predicting n separate times. For linear models, however, the predicted residuals can be ☞ 171\\ncalculated quickly using only the ordinary residuals and the projection matrix P = XX+\\nonto the linear space spanned by the columns of the model matrix X (see (2.13)). The i-th ☞ 28\\ndiagonal element Pii of the projection matrix is called thei-th leverage leverage, and it can be shown\\nthat 0 ⩽Pii ⩽1 (see Exercise 10).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 191, 'page_label': '174'}, page_content='174 Analysis via Linear Models\\nTheorem 5.1: PRESS for Linear Models\\nConsider the linear model (5.8), where then×p model matrix X is of full rank. Given\\nan outcome y = [y1,..., yn]⊤of Y, the fitted values can be obtained asby = Py,where\\nP = XX+ = X(X⊤X)−1X⊤is the projection matrix. If the leverage valuepi := Pii , 1\\nfor all i = 1,..., n, then the predicted residual sum of squares can be written as\\nPRESS =\\nnX\\ni=1\\n ei\\n1 −pi\\n!2\\n,\\nwhere ei = yi −byi = yi −(Xbβ)i is the i-th residual.\\nProof: It su ffices to show that the i-th predicted residual can be written as yi −by−i =\\nei/(1 −pi). Let X−i denote the model matrix X with the i-th row, x⊤\\ni , removed, and define\\ny−i similarly. Then, the least-squares estimate for β using all but the i-th observation is\\nbβ−i = (X⊤\\n−iX−i)−1X⊤\\n−i y−i. Writing X⊤X = X⊤\\n−iX−i + xi x⊤\\ni , we have by the Sherman–Morrison\\nformula☞371\\n(X⊤\\n−iX−i)−1 = (X⊤X)−1 + (X⊤X)−1 xi x⊤\\ni (X⊤X)−1\\n1 −x⊤\\ni (X⊤X)−1 xi\\n,\\nwhere x⊤\\ni (X⊤X)−1 xi = pi < 1. Also, X⊤'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 191, 'page_label': '174'}, page_content='i (X⊤X)−1\\n1 −x⊤\\ni (X⊤X)−1 xi\\n,\\nwhere x⊤\\ni (X⊤X)−1 xi = pi < 1. Also, X⊤\\n−i y−i = X⊤y −xiyi. Combining all these identities,\\nwe have\\nbβ−i = (X⊤\\n−iX−i)−1X⊤\\n−i y−i\\n=\\n \\n(X⊤X)−1 + (X⊤X)−1 xi x⊤\\ni (X⊤X)−1\\n1 −pi\\n!\\n(X⊤y −xiyi)\\n= bβ+ (X⊤X)−1 xi x⊤\\ni\\nbβ\\n1 −pi\\n−(X⊤X)−1 xiyi −(X⊤X)−1 xi piyi\\n1 −pi\\n= bβ+ (X⊤X)−1 xi x⊤\\ni\\nbβ\\n1 −pi\\n−(X⊤X)−1 xiyi\\n1 −pi\\n= bβ−(X⊤X)−1 xi(yi −x⊤\\ni\\nbβ)\\n1 −pi\\n= bβ−(X⊤X)−1 xiei\\n1 −pi\\n.\\nIt follows that the predicted value for the i-th observation is given by\\nby−i = x⊤\\ni bβ−i = x⊤\\ni bβ−x⊤\\ni (X⊤X)−1 xiei\\n1 −pi\\n= byi − piei\\n1 −pi\\n.\\nHence, yi −by−i = ei + piei/(1 −pi) = ei/(1 −pi). □\\nExample 5.2 (Polynomial Regression (cont.)) We return to Example 2.1, where we☞26\\nestimated the generalization risk for various polynomial prediction functions using inde-\\npendent validation data. Instead, let us estimate the expected generalization risk via cross-\\nvalidation (thus using only the training set) and apply Theorem 5.1 to compute the PRESS.\\n☞174'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 192, 'page_label': '175'}, page_content='Regression 175\\npolyregpress.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\ndef generate_data(beta , sig, n):\\nu = np.random.rand(n, 1)\\ny = u ** np.arange(0, 4) @ beta.reshape(4,1) + (\\nsig * np.random.randn(n, 1))\\nreturn u, y\\nnp.random.seed(12)\\nbeta = np.array([[10.0, -140, 400, -250]]).T;\\nsig=5; n = 10**2;\\nu,y = generate_data(beta,sig,n)\\nX = np.ones((n, 1))\\nK = 12 #maximum number of parameters\\npress = np.zeros(K+1)\\nfor k in range (1,K):\\nif k > 1:\\nX = np.hstack((X, u**(k-1))) # add column to matrix\\nP = X @ np.linalg.pinv(X) # projection matrix\\ne = y - P @ y\\npress[k] = np. sum ((e/(1-np.diag(P).reshape(n,1)))**2)\\nplt.plot(press[1:K]/n)\\nThe PRESS values divided by n = 100 for the constant, linear, quadratic, cubic, and\\nquartic order polynomial regression models are, respectively, 152 .487,56.249,51.606,\\n30.999, and 31.634. Hence, the cubic polynomial regression model has the lowest PRESS,\\nindicating that it has the best predictive performance.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 192, 'page_label': '175'}, page_content='indicating that it has the best predictive performance.\\n5.3.4 In-Sample Risk and Akaike Information Criterion\\nIn Section 2.5.1 we introduced the in-sample risk as a measure for the accuracy of the ☞ 35\\nprediction function. To recapitulate, given a fixed data setτwith associated response vector\\ny and n ×p matrix of explanatory variables X, the in-sample risk of a prediction function\\ng is defined as\\nℓin(g) := EX Loss(Y,g(X)), (5.11)\\nwhere EX signifies that the expectation is taken under a di fferent probability model, in\\nwhich X takes the values x1,..., xn with equal probability, and given X = xi the random\\nvariable Y is drawn from the conditional pdf f (y |xi). The difference between the in-sample\\nrisk and the training loss is called theoptimism. For the squared-error loss, Theorem 2.2 ex- ☞ 36\\npresses the expected optimism of a learnergT as two times the average covariance between\\nthe predicted values and the responses.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 192, 'page_label': '175'}, page_content='the predicted values and the responses.\\nIf the conditional variance of the error Y −g∗(X) given X = x does not depend on x,\\nthen the expected in-sample risk of a learnergτ, averaged over all training sets, has a simple\\nexpression:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 193, 'page_label': '176'}, page_content='176 Analysis via Linear Models\\nTheorem 5.2: Expected In-Sample Risk for Linear Models\\nLet X be the model matrix for a linear model, of dimension n ×p. If Var[Y −\\ng∗(X) |X = x] =: v2 does not depend on x, then the expected in-sample risk (with\\nrespect to the squared-error loss) for a random learner gT is given by\\nEX ℓin(gT) = EX ℓT(gT) + 2ℓ∗p\\nn , (5.12)\\nwhere ℓ∗is the irreducible risk.\\nProof: The expected optimism is, by definition, EX[ℓin(gT) −ℓT(gT)] which, for the\\nsquared-error loss, is equal to 2ℓ∗p/n, using exactly the same reasoning as in Example 2.3.\\nNote that here ℓ∗= v2. □\\nEquation (5.12) is the basis of the following model comparison heuristic: Estimate the\\nirreducible risk ℓ∗= v2 via bv2, using a model with relatively high complexity. Then choose\\nthe linear model with the lowest value of\\n∥y −Xbβ∥2 + 2 bv2 p. (5.13)\\nWe can also use the Akaike information criterion (AIC) as a heuristic for model com-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 193, 'page_label': '176'}, page_content='We can also use the Akaike information criterion (AIC) as a heuristic for model com-\\nparison. We discussed the AIC in the unsupervised learning setting in Section 4.2, but the☞122\\narguments used there can also be applied to the supervised case, under the in-sample model\\nfor the data. In particular, let Z = (X,Y). We wish to predict the joint density\\nf (z) = f (x,y) := 1\\nn\\nnX\\ni=1\\n1{x=xi} f (y |xi),\\nusing a prediction function g(z |θ) from a family G:= {g(z |θ),θ∈Rq}, where\\ng(z |θ) = g(x,y |θ) := 1\\nn\\nnX\\ni=1\\n1{x=xi}gi(y |θ).\\nNote that q is the number of parameters (typically larger than p for a linear model with a\\nn ×p design matrix).\\nFollowing Section 4.2, the in-sample cross-entropy risk in this case is\\nr(θ) := −EX ln g(Z |θ),\\nand to approximate the optimal parameter θ∗we minimize the corresponding training loss\\nrτn (θ) := −1\\nn\\nnX\\nj=1\\nln g(zj |θ).\\nThe optimal parameter bθn for the training loss is thus found by minimizing\\n−1\\nn\\nnX\\nj=1\\n\\x10\\n−ln n + ln gj(yj |θ)\\n\\x11\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 194, 'page_label': '177'}, page_content='Regression 177\\nThat is, it is the maximum likelihood estimate of θ:\\nbθn = argmax\\nθ\\nnX\\ni=1\\nln gi(yi |θ).\\nUnder the assumption that f = g(·|θ∗) for some parameter θ∗, we have from Theorem 4.1\\nthat the estimated in-sample generalization risk can be approximated as ☞ 125\\nEX r(bθn) ≈rTn (bθn) + q\\nn = ln n −1\\nn\\nnX\\nj=1\\nln gj(yj |bθn) + q\\nn.\\nThis leads to the heuristic of selecting the learner g(·|bθn) with the smallest value of the\\nAIC:\\n−2\\nnX\\ni=1\\nln gi(yi |bθn) + 2q. (5.14)\\nExample 5.3 (Normal Linear Model) For the normal linear model Y ∼ N(x⊤β,σ2)\\n(see (2.34)), with a p-dimensional vector β, we have ☞ 47\\ngi(yi |β,σ2\\n|{z}\\n= θ\\n) = 1√\\n2πσ2\\nexp\\n \\n−1\\n2\\n(yi −x⊤\\ni β)2\\nσ2\\n!\\n, i = 1,..., n,\\nso that the AIC is\\nn ln(2π) + n ln bσ2 + ∥y −Xbβ∥2\\nbσ2 + 2q, (5.15)\\nwhere (bβ,bσ2) is the maximum likelihood estimate andq = p+1 is the number of parameters\\n(including σ2). For model comparison we may remove the n ln(2π) term if all the models\\nare normal linear models.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 194, 'page_label': '177'}, page_content='are normal linear models.\\nCertain software packages report the AIC without the n ln bσ2 term in (5.15). This\\nmay lead to sub-optimal model selection if normal models are compared with non-\\nnormal ones.\\n5.3.5 Categorical Features\\nSuppose that, as described in Chapter 1, the data is given in the form of a spreadsheet or\\ndata frame with n rows and p + 1 columns, where the first element of row i is the response\\nvariable yi, and the remaining p elements form the vector of explanatory variables x⊤\\ni .\\nWhen all the explanatory variables (features, predictors) are quantitative, then the model\\nmatrix X can be directly read off from the data frame as the n ×p matrix with rows x⊤\\ni ,i =\\n1,..., n.\\nHowever, when some explanatory variables arequalitative (categorical), such a one-to-\\none correspondence between data frame and model matrix no longer holds. The solution is\\nto include indicator or dummy variables.\\nLinear models with continuous responses and categorical explanatory variables often'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 194, 'page_label': '177'}, page_content='Linear models with continuous responses and categorical explanatory variables often\\narise in factorial experiments. These are controlled statistical experiments in which the factorial\\nexperiments'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 195, 'page_label': '178'}, page_content='178 Analysis via Linear Models\\naim is to assess how a response variable is affected by one or more factors tested at several\\nfactors levels. A typical example is an agricultural experiment where one wishes to investigate\\nlevels how the yield of a food crop depends on factors such as location, pesticide, and fertilizer.\\nExample 5.4 (Crop Yield) The data in Table 5.1 lists the yield of a food crop for four\\ndifferent crop treatments (e.g., strengths of fertilizer) on four different blocks (plots).\\nTable 5.1: Crop yield for different treatments and blocks.\\nTreatment\\nBlock 1 2 3 4\\n1 9.2988 9.4978 9.7604 10.1025\\n2 8.2111 8.3387 8.5018 8.1942\\n3 9.0688 9.1284 9.3484 9.5086\\n4 8.2552 7.8999 8.4859 8.9485\\nThe corresponding data frame, given in Table 5.2, has 16 rows and 3 columns: one\\ncolumn for the crop yield (the response variable), one column for the Treatment, with\\nlevels 1, 2, 3, 4, and one column for the Block, also with levels 1, 2, 3, 4. The values 1,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 195, 'page_label': '178'}, page_content='levels 1, 2, 3, 4, and one column for the Block, also with levels 1, 2, 3, 4. The values 1,\\n2, 3, and 4 have no quantitative meaning (it does not make sense to take their average, for\\nexample) — they merely identify the category of the treatment or block.\\nTable 5.2: Crop yield data organized as a data frame in standard format.\\nYield Treatment Block\\n9.2988 1 1\\n8.2111 1 2\\n9.0688 1 3\\n8.2552 1 4\\n9.4978 2 1\\n8.3387 2 2\\n... ... ...\\n9.5086 4 3\\n8.9485 4 4\\nIn general, suppose there are r factor (categorical) variables u1,..., ur, where the j-\\nth factor has pj mutually exclusive levels, denoted by 1 ,..., pj. In order to include these\\ncategorical variables in a linear model, a common approach is to introduce an indicator\\nfeatureindicator\\nfeature\\nxjk = 1{uj = k}for each factor j at level k. Thus, xjk = 1 if the value of factor j\\nis k and 0 otherwise. Since P\\nk 1{uj = k}= 1, it su ffices to consider only pj −1 of these'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 195, 'page_label': '178'}, page_content='is k and 0 otherwise. Since P\\nk 1{uj = k}= 1, it su ffices to consider only pj −1 of these\\nindicator features for each factor j (this prevents the model matrix from being rank defi-\\ncient). For a single responseY, the feature vector x⊤is thus a row vector of binary variables'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 196, 'page_label': '179'}, page_content='Regression 179\\nthat indicates which levels were observed for each factor. The model assumption is that Y\\ndepends in a linear way on the indicator features, apart from an error term. That is,\\nY = β0 +\\nrX\\nj=1\\npjX\\nk=2\\nβjk 1{uj = k}|     {z     }\\nxjk\\n+ ε,\\nwhere we have omitted one indicator feature (corresponding to level 1) for each factor\\nj. For independent responses Y1,..., Yn, where each Yi corresponds to the factor values\\nui1,..., uir, let xi jk = 1{ui j = k}. Then, the linear model for the data becomes\\nYi = β0 +\\nrX\\nj=1\\npjX\\nk=2\\nβjk xi jk + εi, (5.16)\\nwhere the {εi}are independent with expectation 0 and some variance σ2. By gathering the\\nβ0 and {βjk}into a vector β, and the {xi jk}into a matrix X, we have again a linear model of\\nthe form (5.8). The model matrix X has n rows and 1 + Pr\\nj=1(pj −1) columns. Using the\\nabove convention that the βj1 parameters are subsumed in the parameter β0 (correspond-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 196, 'page_label': '179'}, page_content='above convention that the βj1 parameters are subsumed in the parameter β0 (correspond-\\ning to the “constant” feature), we can interpret β0 as a baseline response when using the\\nexplanatory vector x⊤for which xj1 = 1 for all factors j = 1,..., r. The other parameters\\n{βjk}can be viewed as incremental effects incremental\\neffects\\nrelative to this baseline effect. For example, β12\\ndescribes by how much the response is expected to change if level 2 is used instead of level\\n1 for factor 1.\\nExample 5.5 (Crop Yield (cont.)) In Example 5.4, the linear model (5.16) has eight\\nparameters: β0,β12,β13,β14,β22,β23,β24, and σ2. The model matrix X depends on how\\nthe crop yields are organized in a vector y and on the ordering of the factors. Let\\nus order y column-wise from Table 5.1, as in y = [9.2988,8.2111,9.0688,8.2552,\\n9.4978,..., 8.9485]⊤, and let Treatment be Factor 1 and Block be Factor 2. Then we can\\nwrite (5.16) as\\nY =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 0 0 0 C\\n1 1 0 0 C\\n1 0 1 0 C\\n1 0 0 1 C\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 196, 'page_label': '179'}, page_content='write (5.16) as\\nY =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 0 0 0 C\\n1 1 0 0 C\\n1 0 1 0 C\\n1 0 0 1 C\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|               {z               }\\nX\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nβ0\\nβ12\\nβ13\\nβ14\\nβ22\\nβ23\\nβ24\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nβ\\n+ ε, where C =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 0\\n1 0 0\\n0 1 0\\n0 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n,\\nand with 1 = [1,1,1,1]⊤ and 0 = [0,0,0,0]⊤. Estimation of βand σ2, model selection,\\nand prediction can now be carried out in the usual manner for linear models.\\nIn the context of factorial experiments, the model matrix is often called the design\\nmatrix design matrix, as it specifies the design of the experiment; e.g., how many replications are taken\\nfor each combination of factor levels. The model (5.16) can be extended by adding products\\nof indicator variables as new features. Such features are called interaction interactionterms.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 197, 'page_label': '180'}, page_content='180 Analysis via Linear Models\\n5.3.6 Nested Models\\nLet X be a n ×p model matrix of the form X = [X1,X2], where X1 and X2 are model\\nmatrices of dimension n ×k and n ×(p −k), respectively. The linear modelsY = X1β1 + ε\\nand Y = X2β2 + εare said to benested models nested within the linear model Y = Xβ+ ε. This simply\\nmeans that certain features inX are ignored in each of the first two models. Note thatβ, β1,\\nand β2 are parameter vectors of dimension p, k, and p −k, respectively. In what follows,\\nwe assume that n ⩾p and that all model matrices are full-rank.\\nSuppose we wish to assess whether to use the full model matrixX or the reduced model\\nmatrix X1. Let bβbe the estimate of βunder the full model (that is, obtained via (5.9)), and\\nlet bβ1 denote the estimate of β1 for the reduced model. LetY(2) = Xbβbe the projection of Y\\nonto the space Span(X) spanned by the columns ofX; and let Y(1) = X1 bβ1 be the projection'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 197, 'page_label': '180'}, page_content='onto the space Span(X) spanned by the columns ofX; and let Y(1) = X1 bβ1 be the projection\\nof Y onto the space Span(X1) spanned by the columns of X1 only; see Figure 5.3. In order\\nto decide whether the features inX2 are needed, we may compare the estimated error terms\\nof the two models, as calculated by (5.10); that is, by the residual sum of squares divided\\nby the number of observations n. If the outcome of this comparison is that there is little\\ndifference between the model error for the full and reduced model, then it is appropriate to\\nadopt the reduced model, as it has fewer parameters than the full model, while explaining\\nthe data just as well. The comparison is thus between the squared norms ∥Y −Y(2)∥2 and\\n∥Y −Y(1)∥2. Because of the nested nature of the linear models, Span( X1) is a subspace of\\nSpan(X) and, consequently, the orthogonal projection of Y(2) onto Span(X1) is the same\\nas the orthogonal projection of Y onto Span(X1); that is, Y(1). By Pythagoras’ theorem, we'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 197, 'page_label': '180'}, page_content='as the orthogonal projection of Y onto Span(X1); that is, Y(1). By Pythagoras’ theorem, we\\nthus have the decomposition∥Y(2) −Y(1)∥2 +∥Y −Y(2)∥2 = ∥Y −Y(1)∥2. This is also illustrated\\nin Figure 5.3.\\nY\\nY −Y(1) Y −Y(2)\\nY(2)\\nO\\nSpan(X)\\nSpan(X1 )\\nY(2) −Y(1)\\nY(1)\\nFigure 5.3: The residual sum of squares for the full model corresponds to∥Y−Y(2)∥2 and for\\nthe reduced model it is∥Y −Y(1)∥2. By Pythagoras’s theorem, the difference is ∥Y(2) −Y(1)∥2.\\nThe above decomposition can be generalized to more than two model matrices. Sup-\\npose that the model matrix can be decomposed into d submatrices: X = [X1,X2,..., Xd],\\nwhere the matrix Xi has pi columns and n rows, i = 1,..., d. Thus, the number of columns2\\n2As always, we assume the columns are linearly independent.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 198, 'page_label': '181'}, page_content='Regression 181\\nin the full model matrix isp = p1 +···+ pd. This creates an increasing sequence of “nested”\\nmodel matrices: X1,[X1,X2],..., [X1,X2,..., Xd], from (say) the baseline normal model\\nmatrix X1 = 1 to the full model matrix X. Think of each model matrix corresponding to\\nspecific variables in the model.\\nWe follow a similar projection procedure as in Figure 5.3: First projectY onto Span(X)\\nto yield the vector Y(d), then project Y(d) onto Span([X1,..., Xd−1]) to obtain Y(d−1), and so\\non, until Y(2) is projected onto Span(X1) to yield Y(1) = Y1 (in the case that X1 = 1).\\nBy applying Pythagoras’ theorem, the total sum of squares can be decomposed as\\n∥Y −Y(1)∥2\\n|       {z       }\\ndf=n−p1\\n= ∥Y −Y(d)∥2\\n|       {z       }\\ndf=n−p\\n+ ∥Y(d) −Y(d−1)∥2\\n|            {z            }\\ndf=pd\\n+ ··· + ∥Y(2) −Y(1)∥2\\n|          {z          }\\ndf=p2\\n. (5.17)\\nSoftware packages typically report the sums of squares as well as the corresponding de-\\ngrees of freedom (df): n −p,pd,..., p2. degrees of\\nfreedom'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 198, 'page_label': '181'}, page_content='grees of freedom (df): n −p,pd,..., p2. degrees of\\nfreedom\\n5.3.7 Coefficient of Determination\\nTo assess how a linear model Y = Xβ+ εcompares to the default model Y = β01 + ε, we\\ncan compare the variance of the original data, estimated via P\\ni(Yi −Y)2/n = ∥Y −Y1∥2/n,\\nwith the variance of the fitted data; estimated via P\\ni(bYi −Y)2/n = ∥bY −Y1∥2/n, where\\nbY = Xbβ. The sum P\\ni(Yi −Y)2/n = ∥Y −Y1∥2 is sometimes called the total sum of squares total sum of\\nsquares(TSS), and the quantity\\nR2 = ∥bY −Y1∥2\\n∥Y −Y1∥2\\n(5.18)\\nis called the coefficient of determination coefficient of\\ndetermination\\nof the linear model. In the notation of Figure 5.3,\\nbY = Y(2) and Y1 = Y(1), so that\\nR2 = ∥Y(2) −Y(1)∥2\\n∥Y −Y(1)∥2 = ∥Y −Y(1)∥2 −∥Y −Y(2)∥2\\n∥Y −Y(1)∥2 = TSS −RSS\\nTSS .\\nNote that R2 lies between 0 and 1. An R2 value close to 1 indicates that a large propor-\\ntion of the variance in the data has been explained by the model.\\nMany software packages also give the adjusted coefficient of determination adjusted'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 198, 'page_label': '181'}, page_content='Many software packages also give the adjusted coefficient of determination adjusted\\ncoefficient of\\ndetermination\\n, or simply\\nthe adjusted R2, defined by\\nR2\\nadjusted = 1 −(1 −R2) n −1\\nn −p.\\nThe regular R2 is always non-decreasing in the number of parameters (see Exercise 15),\\nbut this may not indicate better predictive power. The adjusted R2 compensates for this\\nincrease by decreasing the regular R2 as the number of variables increases. This heuristic\\nadjustment can make it easier to compare the quality of two competing models.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 199, 'page_label': '182'}, page_content='182 Inference for Normal Linear Models\\n5.4 Inference for Normal Linear Models\\nSo far we have not assumed any distribution for the random vector of errors ε =\\n[ε1,...,ε n]⊤ in a linear model Y = Xβ+ ε. When the error terms {εi}are assumed to be\\nnormally distributed (that is, {εi}∼iid N(0,σ2)), whole new avenues open up for inference\\non linear models. In Section 2.8 we already saw that for suchnormal linear models, estim-☞47\\nation of βand σ2 can be carried out via maximum likelihood methods, yielding the same\\nestimators from (5.9) and (5.10).\\nThe following theorem lists the properties of these estimators. In particular, it shows\\nthat bβand cσ2n/(n −p) are independent and unbiased estimators of βand σ2, respectively.\\nTheorem 5.3: Properties of the Estimators for a Normal Linear Model\\nConsider the linear model Y = Xβ+ ε, with ε∼N(0,σ2In), where β is a p-\\ndimensional vector of parameters and σ2 a dispersion parameter. The following res-\\nults hold.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 199, 'page_label': '182'}, page_content='dimensional vector of parameters and σ2 a dispersion parameter. The following res-\\nults hold.\\n1. The maximum likelihood estimators bβand cσ2 are independent.\\n2. bβ∼N(β, σ2(X⊤X)+).\\n3. n cσ2/σ2 ∼χ2\\nn−p, where p = rank(X).\\nProof: Using the pseudo-inverse (Definition A.2), we can write the random vector bβ as☞360\\nX+Y, which is a linear transformation of a normal random vector. Consequently, bβhas a\\nmultivariate normal distribution; see Theorem C.6. The mean vector and covariance matrix☞435\\nfollow from the same theorem:\\nEbβ= X+ EY = X+X β= β\\nand\\nCov(bβ) = X+σ2In(X+)⊤= σ2(X⊤X)+.\\nTo show that bβand cσ2 are independent, define Y(2) = Xbβ. Note that Y/σ has a N(µ,In)\\ndistribution, with expectation vector µ = Xβ/σ. A direct application of Theorem C.10\\nnow shows that ( Y −Y(2))/σ is independent of Y(2)/σ. Since bβ = X+Xbβ = X+Y(2) and☞438\\ncσ2 = ∥Y −Y(2)∥2/n, it follows that cσ2 is independent of bβ. Finally, by the same theorem,\\nthe random variable ∥Y −Y(2)∥2/σ2 has a χ2'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 199, 'page_label': '182'}, page_content='the random variable ∥Y −Y(2)∥2/σ2 has a χ2\\nn−p distribution, as Y(2) has the same expectation\\nvector as Y. □\\nAs a corollary, we see that each estimatorbβi of βi has a normal distribution with expect-\\nation βi and variance σ2u⊤\\ni X+(X+)⊤ui = σ2∥u⊤\\ni X+∥2, where ui = [0,..., 0,1,0,..., 0]⊤ is\\nthe i-th unit vector; in other words, the variance is σ2[(X⊤X)+]ii.\\nIt is of interest to test whether certain regression parameters βi are 0 or not, since if\\nβi = 0, the i-th explanatory variable has no direct e ffect on the expected response and so\\ncould be removed from the model. A standard procedure is to conduct a hypothesis test\\n(see Section C.14 for a review of hypothesis testing) to test the null hypothesis H0 : βi = 0☞458'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 200, 'page_label': '183'}, page_content='Regression 183\\nagainst the alternative H1 : βi , 0, using the test statistic\\nT =\\nbβi/∥u⊤\\ni X+∥\\n√\\nRSE\\n, (5.19)\\nwhere RSE is the residual squared error; that is RSE = RSS/(n −p). This test statistic has\\na tn−p distribution under H0. To see this, write T = Z/\\np\\nV/(n −p), with\\nZ =\\nbβi\\nσ∥u⊤\\ni X+∥ and V = n cσ2/σ2.\\nThen, by Theorem 5.3, Z ∼N(0,1) under H0, V ∼χ2\\nn−p, and Z and V are independent. The\\nresult now follows directly from Corollary C.1. ☞ 439\\n5.4.1 Comparing Two Normal Linear Models\\nSuppose we have the following normal linear model for data Y = [Y1,..., Yn]⊤:\\nY = X1β1 + X2β2|          {z          }\\nXβ\\n+ε, ε∼N(0,σ2In), (5.20)\\nwhere β1 and β2 are unknown vectors of dimension k and p −k, respectively; and X1\\nand X2 are full-rank model matrices of dimensions n ×k and n ×(p −k), respectively.\\nAbove we implicitly defined X = [X1,X2] and β⊤ = [β⊤\\n1 ,β⊤\\n2 ]. Suppose we wish to test the\\nhypothesis H0 : β2 = 0 against H1 : β2 , 0. Following Section 5.3.6, the idea is to compare'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 200, 'page_label': '183'}, page_content='hypothesis H0 : β2 = 0 against H1 : β2 , 0. Following Section 5.3.6, the idea is to compare\\nthe residual sum of squares for both models, expressed as∥Y −Y(2)∥2 and ∥Y −Y(1)∥2. Using\\nPythagoras’ theorem we saw that ∥Y −Y(2)∥2 −∥Y −Y(1)∥2 = ∥Y(2) −Y(1)∥2, and so it makes\\nsense to base the decision whether to retain or reject H0 on the basis of the quotient of\\n∥Y(2) −Y(1)∥2 and ∥Y −Y(2)∥2. This leads to the following test statistics.\\nTheorem 5.4: Test Statistic for Comparing Two Normal Linear Models\\nFor the model (5.20), letY(2) and Y(1) be the projections of Y onto the space spanned\\nby the p columns of X and the k columns of X1, respectively. Then underH0 : β2 = 0\\nthe test statistic\\nT = ∥Y(2) −Y(1)∥2/(p −k)\\n∥Y −Y(2)∥2/(n −p) (5.21)\\nhas an F(p −k,n −p) distribution.\\nProof: Define X := Y/σwith expectation µ:= Xβ/σ, and Xj := Y( j)/σwith expectation\\nµj, j = k,p. Note that µp = µand, under H0, µk = µp. We can directly apply Theorem C.10\\nto find that ∥Y −Y(2)∥2/σ2 = ∥X −Xp∥2 ∼χ2'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 200, 'page_label': '183'}, page_content='to find that ∥Y −Y(2)∥2/σ2 = ∥X −Xp∥2 ∼χ2\\nn−p and, under H0, ∥Y(2) −Y(1)∥2/σ2 = ∥Xp − ☞ 438\\nXk∥2 ∼χ2\\np−k. Moreover, these random variables are independent of each other. The proof\\nis completed by applying Theorem C.11. □'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 201, 'page_label': '184'}, page_content='184 Inference for Normal Linear Models\\nNote that H0 is rejected for large values of T. The testing procedure thus proceeds as\\nfollows:\\n1. Compute the outcome, t say, of the test statistic T in (5.21).\\n2. Evaluate the P-value P(T ⩾t), with T ∼F(p −k,n −p).\\n3. Reject H0 if this P-value is too small, say less than 0.05.\\nFor nested models [X1,X2,..., Xi], i = 1,2,..., d, as in Section 5.3.6, theF test statistic\\nin Theorem 5.4 can now be used to test whether certain Xi are needed or not. In particular,☞183\\nsoftware packages will report the outcomes of\\nFi = ∥Y(i) −Y(i−1)∥2/pi\\n∥Y −Y(d)∥2/(n −p)\\n, (5.22)\\nin the order i = 2,3,..., d. Under the null hypothesis that Y(i) and Y(i−1) have the same ex-\\npectation (that is, adding Xi to Xi−1 has no additional effect on reducing the approximation\\nerror), the test statistic Fi has an F(pi,n −p) distribution, and the corresponding P-values\\nquantify the strength of the decision to include an additional variable in the model or not.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 201, 'page_label': '184'}, page_content='quantify the strength of the decision to include an additional variable in the model or not.\\nThis procedure is called analysis of variance (ANOV A).analysis of\\nvariance\\nNote that the output of an ANOV A table depends on the order in which the variables\\nare considered.\\nExample 5.6 (Crop Yield (cont.)) We continue Examples 5.4 and 5.5. Decompose the\\nlinear model as\\nY =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nX1\\nβ0|{z}\\nβ1\\n+\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 0\\n1 0 0\\n0 1 0\\n0 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|     {z     }\\nX2\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nβ12\\nβ13\\nβ14\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nβ2\\n+\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nC\\nC\\nC\\nC\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nX3\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nβ22\\nβ23\\nβ24\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nβ3\\n+ ε.\\nIs the crop yield dependent on treatment levels as well as blocks? We first test whether we\\ncan remove Block as a factor in the model against it playing a significant role in explain-\\ning the crop yields. Specifically, we test β3 = 0 versus β3 , 0 using Theorem 5.4. Now'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 201, 'page_label': '184'}, page_content='ing the crop yields. Specifically, we test β3 = 0 versus β3 , 0 using Theorem 5.4. Now\\nthe vector Y(2) is the projection of Y onto the ( p = 7)-dimensional space spanned by the\\ncolumns of X = [X1,X2,X3]; and Y(1) is the projection of Y onto the (k = 4)-dimensional\\nspace spanned by the columns of X12 := [X1,X2]. The test statistic, T12 say, under H0 has\\nan F(3,9) distribution.\\nThe Python code below calculates the outcome of the test statistic T12 and the corres-\\nponding P-value. We find t12 = 34.9998, which gives a P-value 2 .73 ×10−5. This shows\\nthat the block effects are extremely important for explaining the data.\\nUsing the extended model (including the block effects), we can test whether β2 = 0 or\\nnot; that is, whether the treatments have a significant effect on the crop yield in the presence\\nof the Block factor. This is done in the last six lines of the code below. The outcome of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 202, 'page_label': '185'}, page_content='Regression 185\\nthe test statistic is 4 .4878, with a P-value of 0 .0346. By including the block e ffects, we\\neffectively reduce the uncertainty in the model and are able to more accurately assess the\\neffects of the treatments, to conclude that the treatment seems to have an effect on the crop\\nyield. A closer look at the data shows that within each block (row) the crop yield roughly\\nincreases with the treatment level.\\ncrop.py\\nimport numpy as np\\nfrom scipy.stats import f\\nfrom numpy.linalg import lstsq, norm\\nyy = np.array([9.2988, 9.4978, 9.7604, 10.1025,\\n8.2111, 8.3387, 8.5018, 8.1942,\\n9.0688, 9.1284, 9.3484, 9.5086,\\n8.2552, 7.8999, 8.4859, 8.9485]).reshape(4,4).T\\nnrow, ncol = yy.shape[0], yy.shape[1]\\nn = nrow * ncol\\ny = yy.reshape(16,)\\nX_1 = np.ones((n,1))\\nKM = np.kron(np.eye(ncol),np.ones((nrow,1)))\\nKM[:,0]\\nX_2 = KM[:,1:ncol]\\nIM = np.eye(nrow)\\nC = IM[:,1:nrow]\\nX_3 = np.vstack((C, C))\\nX_3 = np.vstack((X_3, C))\\nX_3 = np.vstack((X_3, C))\\nX = np.hstack((X_1,X_2))\\nX = np.hstack((X,X_3))'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 202, 'page_label': '185'}, page_content='X_3 = np.vstack((X_3, C))\\nX_3 = np.vstack((X_3, C))\\nX = np.hstack((X_1,X_2))\\nX = np.hstack((X,X_3))\\np = X.shape[1] #number of parameters in full model\\nbetahat = lstsq(X, y,rcond=None)[0] #estimate under the full model\\nym = X @ betahat\\nX_12 = np.hstack((X_1, X_2)) #omitting the block effect\\nk = X_12.shape[1] #number of parameters in reduced model\\nbetahat_12 = lstsq(X_12, y,rcond=None)[0]\\ny_12 = X_12 @ betahat_12\\nT_12=(n-p)/(p-k)*(norm(y-y_12)**2 - norm(y-ym)**2)/norm(y-ym)**2\\npval_12 = 1 - f.cdf(T_12,p-k,n-p)\\nX_13 = np.hstack((X_1, X_3)) #omitting the treatment effect\\nk = X_13.shape[1] #number of parameters in reduced model\\nbetahat_13 = lstsq(X_13, y,rcond=None)[0]\\ny_13 = X_13 @ betahat_13\\nT_13=(n-p)/(p-k)*(norm(y-y_13)**2 - norm(y-ym)**2)/norm(y-ym)**2\\npval_13 = 1 - f.cdf(T_13,p-k,n-p)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 203, 'page_label': '186'}, page_content='186 Inference for Normal Linear Models\\n5.4.2 Confidence and Prediction Intervals\\nAs in all supervised learning settings, linear regression is most useful when we wish to\\npredict how a new response variable will behave on the basis of a new explanatory vector\\nx. For example, it may be di fficult to measure the response variable, but by knowing the\\nestimated regression line and the value for x, we will have a reasonably good idea what Y\\nor the expected value of Y is going to be.\\nThus, consider a new x and let Y ∼N(x⊤β,σ2), with β and σ2 unknown. First we\\nare going to look at the expected value of Y, that is EY = x⊤β. Since βis unknown, we\\ndo not know EY either. However, we can estimate it via the estimator bY = x⊤bβ, where\\nbβ ∼N(β, σ2(X⊤X)+), by Theorem 5.3. Being linear in the components of β, bY therefore\\nhas a normal distribution with expectation x⊤βand variance σ2∥x⊤X+∥2. Let Z ∼N(0,1)\\nbe the standardized version of bY and V = ∥Y −Xbβ∥2/σ2 ∼χ2\\nn−p. Then the random variable'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 203, 'page_label': '186'}, page_content='be the standardized version of bY and V = ∥Y −Xbβ∥2/σ2 ∼χ2\\nn−p. Then the random variable\\nT := (x⊤bβ−x⊤β) /∥x⊤X+∥\\n∥Y −Xbβ∥/\\np\\n(n −p)\\n= Zp\\nV/(n −p)\\n(5.23)\\nhas, by Corollary C.1, atn−p distribution. After rearranging the identityP(|T|⩽tn−p;1−α/2) =☞439\\n1 −α, where tn−p;1−α/2 is the (1 −α/2) quantile of the tn−p distribution, we arrive at the\\nstochastic confidence intervalconfidence\\ninterval\\nx⊤bβ±tn−p;1−α/2\\n√\\nRSE ∥x⊤X+∥, (5.24)\\nwhere we have identified ∥Y −Xbβ∥2/(n −p) with RSE. This confidence interval quantifies\\nthe uncertainty in the learner (regression surface).\\nA prediction intervalprediction\\ninterval\\nfor a new response Y is different from a confidence interval for\\nEY. Here the idea is to construct an interval such that Y lies in this interval with a certain\\nguaranteed probability. Note that now we havetwo sources of variation:\\n1. Y ∼N(x⊤β,σ2) itself is a random variable.\\n2. Estimating x⊤βvia bY brings another source of variation.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 203, 'page_label': '186'}, page_content='2. Estimating x⊤βvia bY brings another source of variation.\\nWe can construct a (1−α) prediction interval, by finding two random bounds such that\\nthe random variable Y lies between these bounds with probability 1 −α. We can reason as\\nfollows. Firstly, note thatY ∼N(x⊤β,σ2) and bY ∼N(x⊤β,σ2∥x⊤X+∥2) are independent. It\\nfollows that Y −bY has a normal distribution with expectation 0 and variance\\nσ2(1 + ∥x⊤X+∥2). (5.25)\\nSecondly, letting Z ∼ N(0,1) be the standardized version of Y −bY, and repeating the\\nsteps used for the construction of the confidence interval (5.24), we arrive at the prediction\\ninterval\\nx⊤bβ±tn−p;1−α/2\\n√\\nRSE\\np\\n1 + ∥x⊤X+∥2. (5.26)\\nThis prediction interval captures the uncertainty from an as-yet-unobserved response as\\nwell as the uncertainty in the parameters of the regression model itself.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 204, 'page_label': '187'}, page_content='Regression 187\\nExample 5.7 (Confidence Limits in Simple Linear Regression) The following pro-\\ngram draws n = 100 samples from a simple linear regression model with parameters\\nβ= [6,13]⊤ and σ= 2, where the x-coordinates are evenly spaced on the interval [0 ,1].\\nThe parameters are estimated in the third block of the code. Estimates for β and σ are\\n[6.03,13.09]⊤ and bσ= 1.60, respectively. The program then proceeds by calculating the\\n95% numeric confidence and prediction intervals for various values of the explanatory\\nvariable. Figure 5.4 shows the results.\\nconfpred.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom scipy.stats import t\\nfrom numpy.linalg import inv, lstsq, norm\\nnp.random.seed(123)\\nn = 100\\nx = np.linspace(0.01,1,100).reshape(n,1)\\n# parameters\\nbeta = np.array([6,13])\\nsigma = 2\\nXmat = np.hstack((np.ones((n,1)), x)) #design matrix\\ny = Xmat @ beta + sigma*np.random.randn(n)\\n# solve the normal equations\\nbetahat = lstsq(Xmat, y,rcond=None)[0]\\n# estimate for sigma'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 204, 'page_label': '187'}, page_content=\"# solve the normal equations\\nbetahat = lstsq(Xmat, y,rcond=None)[0]\\n# estimate for sigma\\nsqMSE = norm(y - Xmat @ betahat)/np.sqrt(n-2)\\ntquant = t.ppf(0.975,n-2) # 0.975 quantile\\nucl = np.zeros(n) #upper conf. limits\\nlcl = np.zeros(n) #lower conf. limits\\nupl = np.zeros(n)\\nlpl = np.zeros(n)\\nrl = np.zeros(n) # (true) regression line\\nu = 0\\nfor i in range (n):\\nu = u + 1/n;\\nxvec = np.array([1,u])\\nsqc = np.sqrt(xvec.T @ inv(Xmat.T @ Xmat) @ xvec)\\nsqp = np.sqrt(1 + xvec.T @ inv(Xmat.T @ Xmat) @ xvec)\\nrl[i] = xvec.T @ beta;\\nucl[i] = xvec.T @ betahat + tquant*sqMSE*sqc;\\nlcl[i] = xvec.T @ betahat - tquant*sqMSE*sqc;\\nupl[i] = xvec.T @ betahat + tquant*sqMSE*sqp;\\nlpl[i] = xvec.T @ betahat - tquant*sqMSE*sqp;\\nplt.plot(x,y, '.')\\nplt.plot(x,rl, 'b')\\nplt.plot(x,ucl, 'k:')\\nplt.plot(x,lcl, 'k:')\\nplt.plot(x,upl, 'r--')\\nplt.plot(x,lpl, 'r--')\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 205, 'page_label': '188'}, page_content='188 Nonlinear Regression Models\\n0.0\\n 0.2\\n 0.4\\n 0.6\\n 0.8\\n 1.0\\n5\\n10\\n15\\n20\\nFigure 5.4: The true regression line (blue, solid) and the upper and lower 95% prediction\\ncurves (red, dashed) and confidence curves (dotted).\\n5.5 Nonlinear Regression Models\\nSo far we have been mostly dealing with linear regression models, in which the predic-\\ntion function is of the form g(x |β) = x⊤β. In this section we discuss some strategies for\\nhandling general prediction functions g(x |β), where the functional form is known up to an\\nunknown parameter vector β. So the regression model becomes\\nYi = g(xi |β) + εi, i = 1,..., n, (5.27)\\nwhere ε1,...,ε n are independent with expectation 0 and unknown variance σ2. The model\\ncan be further specified by assuming that the error terms have a normal distribution.\\nTable 5.3 gives some common examples of nonlinear prediction functions for data tak-\\ning values in R.\\nTable 5.3: Common nonlinear prediction functions for one-dimensional data.\\nName g(x |β) β'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 205, 'page_label': '188'}, page_content='Table 5.3: Common nonlinear prediction functions for one-dimensional data.\\nName g(x |β) β\\nExponential a ebx a,b\\nPower law a xb a,b\\nLogistic (1 + ea+bx)−1 a,b\\nWeibull 1 −exp(−xb/a) a,b\\nPolynomial Pp−1\\nk=0 βk xk p, {βk}p−1\\nk=0\\nThe logistic and polynomial prediction functions in Table 5.3 can be readily gener-\\nalized to higher dimensions. For example, for x ∈R2 a general second-order polynomial\\nprediction function is of the form\\ng(x |β) = β0 + β1 x1 + β2 x2 + β11 x2\\n1 + β22 x2\\n2 + β12 x1 x2. (5.28)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 206, 'page_label': '189'}, page_content='Regression 189\\nThis function can be viewed as a second-order approximation to a general smooth predic-\\ntion function g(x1,x2); see also Exercise 4. Polynomial regression models are also called\\nresponse surface models. response\\nsurface model\\nThe generalization of the above logistic prediction to Rd is\\ng(x |β) = (1 + e−x⊤β)−1. (5.29)\\nThis function will make its appearance in Section 5.7 and later on in Chapters 7 and 9.\\nThe first strategy for performing regression with nonlinear prediction functions is to\\nextend the feature space to obtain a simpler (ideally linear) prediction function in the ex-\\ntended feature space. We already saw an application of this strategy in Example 2.1 for ☞ 26\\nthe polynomial regression model, where the original feature u was extended to the feature\\nvector x = [1,u,u2,..., up−1]⊤, yielding a linear prediction function. In a similar way, the\\nright-hand side of the polynomial prediction function in (5.28) can be viewed as a linear'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 206, 'page_label': '189'}, page_content='right-hand side of the polynomial prediction function in (5.28) can be viewed as a linear\\nfunction of the extended feature vector ϕ(x) = [1,x1,x2,x2\\n1,x2\\n2,x1 x2]⊤. The function ϕis\\ncalled a feature map feature map.\\nThe second strategy is to transform the response variable y and possibly also the ex-\\nplanatory variable x such that the transformed variablesey, ex are related in a simpler (ideally\\nlinear) way. For example, for the exponential prediction function y = a e−bx, we have\\nln y = ln a −bx, which is a linear relation between ln y and [1,x]⊤.\\nExample 5.8 (Chlorine) Table 5.4 lists the free chlorine concentration (in mg per liter)\\nin a swimming pool, recorded every 8 hours for 4 days. A simple chemistry-based model\\nfor the chlorine concentration y as a function of time t is y = a e−b t, where a is the initial\\nconcentration and b >0 is the reaction rate.\\nTable 5.4: Chlorine concentration (in mg/L) as a function of time (hours).\\nHours Concentration\\n0 1.0056\\n8 0.8497\\n16 0.6682'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 206, 'page_label': '189'}, page_content='Hours Concentration\\n0 1.0056\\n8 0.8497\\n16 0.6682\\n24 0.6056\\n32 0.4735\\n40 0.4745\\n48 0.3563\\nHours Concentration\\n56 0.3293\\n64 0.2617\\n72 0.2460\\n80 0.1839\\n88 0.1867\\n96 0.1688\\nThe exponential relationshipy = a e−bt suggests that a log transformation ofy will result\\nin a linear relationship between ln y and the feature vector [1,t]⊤. Thus, if for some given\\ndata (t1,y1),..., (tn,yn), we plot (t1,ln y1),..., (tn,ln yn), these points should approximately\\nlie on a straight line, and hence the simple linear regression model applies. The left panel of\\nFigure 5.5 illustrates that the transformed data indeed lie approximately on a straight line.\\nThe estimated regression line is also drawn here. The intercept and slope areβ0 = −0.0555\\nand β1 = −0.0190 here. The original (non-transformed) data is shown in the right panel\\nof Figure 5.5, along with the fitted curve y = ba e−bbt, where ba = exp(bβ0) = 0.9461 and\\nbb = −bβ1 = 0.0190.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 207, 'page_label': '190'}, page_content='190 Nonlinear Regression Models\\n0 50 100\\nt\\n-2\\n-1.5\\n-1\\n-0.5\\n0\\n0.5\\nlog y\\n0 50 100\\nt\\n0\\n0.5\\n1\\n1.5y\\nFigure 5.5: The chlorine concentration seems to have an exponential decay.\\nRecall that for a general regression problem the learner gτ(x) for a given training set τ\\nis obtained by minimizing the training (squared-error) loss\\nℓτ(g(·|β)) = 1\\nn\\nnX\\ni=1\\n(yi −g(xi |β))2. (5.30)\\nThe third strategy for regression with nonlinear prediction functions is to directly minimize\\n(5.30) by any means possible, as illustrated in the next example.\\nExample 5.9 (Hougen Function) In [7] the reaction rate y of a certain chemical reac-\\ntion is posited to depend on three input variables: quantities of hydrogen x1, n-pentane x2,\\nand isopentane x3. The functional relationship is given by the Hougen function:\\ny = β1 x2 −x3/β5\\n1 + β2 x1 + β3 x2 + β4 x3\\n,\\nwhere β1,...,β 5 are the unknown parameters. The objective is to estimate the model para-\\nmeters {βi}from the data, as given in Table 5.5.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 207, 'page_label': '190'}, page_content='meters {βi}from the data, as given in Table 5.5.\\nTable 5.5: Data for the Hougen function.\\nx1 x2 x3 y\\n470 300 10 8.55\\n285 80 10 3.79\\n470 300 120 4.82\\n470 80 120 0.02\\n470 80 10 2.75\\n100 190 10 14.39\\n100 80 65 2.54\\nx1 x2 x3 y\\n470 190 65 4.35\\n100 300 54 13.00\\n100 300 120 8.50\\n100 80 120 0.05\\n285 300 10 11.32\\n285 190 120 3.13\\nThe estimation is carried out via the least-squares method. The objective function to\\nminimize is thus\\nℓτ(g(·|β)) = 1\\n13\\n13X\\ni=1\\n \\nyi − β1 xi2 −xi3/β5\\n1 + β2 xi1 + β3 xi2 + β4 xi3\\n!2\\n, (5.31)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 208, 'page_label': '191'}, page_content='Regression 191\\nwhere the {yi}and {xi j}are given in Table 5.5.\\nThis is a highly nonlinear optimization problem, for which standard nonlinear least- ☞ 414\\nsquares methods do not work well. Instead, one can use global optimization methods such\\nas CE and SCO (see Sections 3.4.2 and 3.4.3). Using the CE method, we found the minimal ☞ 100\\nvalue 0.02299 for the objective function, which is attained at\\nbβ= [1.2526, 0.0628, 0.0400, 0.1124, 1.1914]⊤.\\n5.6 Linear Models in Python\\nIn this section we describe how to define and analyze linear models using Python and the\\ndata science module statsmodels. We encourage the reader to regularly refer back to\\nthe theory in the preceding sections of this chapter, so as to avoid using Python merely\\nas a black box without understanding the underlying principles. To run the code start by\\nimporting the following code snippet:\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport statsmodels.api as sm\\nfrom statsmodels.formula.api import ols\\n5.6.1 Modeling'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 208, 'page_label': '191'}, page_content='import statsmodels.api as sm\\nfrom statsmodels.formula.api import ols\\n5.6.1 Modeling\\nAlthough specifying a normal3 linear model in Python is relatively easy, it requires some\\nsubtlety. The main thing to realize is that Python treats quantitative and qualitative (that\\nis, categorical) explanatory variables differently. In statsmodels, ordinary least-squares\\nlinear models are specified via the function ols (short for ordinary least-squares). The\\nmain argument of this function is a formula of the form\\ny ∼x1 + x2 + ··· + xd, (5.32)\\nwhere y is the name of the response variable and x1, . . . ,xd are the names of the explan-\\natory variables. If all variables are quantitative, this describes the linear model\\nYi = β0 + β1 xi1 + β2 xi2 + ··· + βd xid + εi, i = 1,..., n, (5.33)\\nwhere xi j is the j-th explanatory variable for the i-th observation and the errors εi are\\nindependent normal random variables such that Eεi = 0 and Var εi = σ2. Or, in matrix\\nform: Y = Xβ+ ε, with\\nY =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nY1\\n...'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 208, 'page_label': '191'}, page_content='form: Y = Xβ+ ε, with\\nY =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nY1\\n...\\nYn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, X =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 x11 ··· x1d\\n1 x21 ··· x2d\\n... ... ... ...\\n1 xn1 ··· xnd\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, β=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nβ0\\n...\\nβd\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, and ε=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nε1\\n...\\nεn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\n3For the rest of this section, we assume all linear models to be normal.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 209, 'page_label': '192'}, page_content='192 Linear Models in Python\\nThus, the first column is always taken as an “intercept” parameter, unless otherwise spe-\\ncified. To remove the intercept term, add -1to the olsformula, as in ols(’y∼x-1’).\\nFor any linear model, the model matrix can be retrieved via the construction:\\nmodel_matrix = pd.DataFrame(model.exog,columns=model.exog_names)\\nLet us look at some examples of linear models. In the first model the variables x1 and x2\\nare both considered (by Python) to be quantitative.\\nmyData = pd.DataFrame({ \\'y\\' : [10,9,4,2,4,9],\\n\\'x1\\' : [7.4,1.2,3.1,4.8,2.8,6.5],\\n\\'x2\\' : [1,1,2,2,3,3]})\\nmod = ols(\"y~x1+x2\", data=myData)\\nmod_matrix = pd.DataFrame(mod.exog,columns=mod.exog_names)\\nprint (mod_matrix)\\nIntercept x1 x2\\n0 1.0 7.4 1.0\\n1 1.0 1.2 1.0\\n2 1.0 3.1 2.0\\n3 1.0 4.8 2.0\\n4 1.0 2.8 3.0\\n5 1.0 6.5 3.0\\nSuppose the second variable is actually qualitative; e.g., it represents a color, and the\\nlevels 1, 2, and 3 stand for red, blue, and green. We can account for such a categorical'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 209, 'page_label': '192'}, page_content='levels 1, 2, and 3 stand for red, blue, and green. We can account for such a categorical\\nvariable by using the astype method to redefine the data type (see Section 1.2).☞3\\nmyData[ \\'x2\\'] = myData[ \\'x2\\'].astype( \\'category \\')\\nAlternatively, a categorical variable can be specified in the model formula by wrapping\\nit with C(). Observe how this changes the model matrix.\\nmod2 = ols(\"y~x1+C(x2)\", data=myData)\\nmod2_matrix = pd.DataFrame(mod2.exog,columns=mod2.exog_names)\\nprint (mod2_matrix)\\nIntercept C(x2)[T.2] C(x2)[T.3] x1\\n0 1.0 0.0 0.0 7.4\\n1 1.0 0.0 0.0 1.2\\n2 1.0 1.0 0.0 3.1\\n3 1.0 1.0 0.0 4.8\\n4 1.0 0.0 1.0 2.8\\n5 1.0 0.0 1.0 6.5\\nThus, if a statsmodels formula of the form (5.32) contains factor (qualitative) variables,\\nthe model is no longer of the form (5.33), but contains indicator variables for each level of\\nthe factor variable, except the first level.\\nFor the case above, the corresponding linear model is\\nYi = β0 + β1 xi1 + α2 1{xi2 = 2}+ α3 1{xi2 = 3}+ εi, i = 1,..., 6, (5.34)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 210, 'page_label': '193'}, page_content='Regression 193\\nwhere we have used parameters α2 and α3 to correspond to the indicator features of the\\nqualitative variable. The parameter α2 describes how much the response is expected to\\nchange if the factor x2 switches from level 1 to 2. A similar interpretation holds for α3.\\nSuch parameters can thus be viewed as incremental effects.\\nIt is also possible to model interaction interactionbetween two variables. For two continuous\\nvariables, this simply adds the products of the original features to the model matrix. Adding\\ninteraction terms in Python is achieved by replacing “ +” in the formula with “ *”, as the\\nfollowing example illustrates.\\nmod3 = ols(\"y~x1*C(x2)\", data=myData)\\nmod3_matrix = pd.DataFrame(mod3.exog,columns=mod3.exog_names)\\nprint (mod3_matrix)\\nIntercept C(x2)[T.2] C(x2)[T.3] x1 x1:C(x2)[T.2] x1:C(x2)[T.3]\\n0 1.0 0.0 0.0 7.4 0.0 0.0\\n1 1.0 0.0 0.0 1.2 0.0 0.0\\n2 1.0 1.0 0.0 3.1 3.1 0.0\\n3 1.0 1.0 0.0 4.8 4.8 0.0\\n4 1.0 0.0 1.0 2.8 0.0 2.8\\n5 1.0 0.0 1.0 6.5 0.0 6.5'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 210, 'page_label': '193'}, page_content='3 1.0 1.0 0.0 4.8 4.8 0.0\\n4 1.0 0.0 1.0 2.8 0.0 2.8\\n5 1.0 0.0 1.0 6.5 0.0 6.5\\n5.6.2 Analysis\\nLet us consider some easy linear regression models by using the student survey data set\\nsurvey.csv from the book’s GitHub site, which contains measurements such as height,\\nweight, sex, etc., from a survey conducted amongn = 100 university students. Suppose we\\nwish to investigate the relation between the shoe size (explanatory variable) and the height\\n(response variable) of a person. First, we load the data and draw a scatterplot of the points\\n(height versus shoe size); see Figure 5.6 (without the fitted line).\\nsurvey = pd.read_csv( \\'survey.csv \\')\\nplt.scatter(survey.shoe, survey.height)\\nplt.xlabel(\"Shoe size\")\\nplt.ylabel(\"Height\")\\nWe observe a slight increase in the height as the shoe size increases, although this\\nrelationship is not very distinct. We analyze the data through the simple linear regression\\nmodel Yi = β0 + β1 xi + εi,i = 1,..., n. In statsmodels this is performed via the ols ☞ 169'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 210, 'page_label': '193'}, page_content='model Yi = β0 + β1 xi + εi,i = 1,..., n. In statsmodels this is performed via the ols ☞ 169\\nmethod as follows:\\nmodel = ols(\"height~shoe\", data=survey) # define the model\\nfit = model.fit() #fit the model defined above\\nb0, b1 = fit.params\\nprint (fit.params)\\nIntercept 145.777570\\nshoe 1.004803\\ndtype: float64'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 211, 'page_label': '194'}, page_content='194 Linear Models in Python\\n15\\n 20\\n 25\\n 30\\n 35\\nShoe size\\n150\\n160\\n170\\n180\\n190\\n200Height\\nFigure 5.6: Scatterplot of height (cm) against shoe size (cm), with the fitted line.\\nThe above output gives the least-squares estimates of β0 and β1. For this example, we\\nhave bβ0 = 145.778 and bβ1 = 1.005. Figure 5.6, which includes the regression line, was\\nobtained as follows:\\nplt.plot(survey.shoe, b0 + b1*survey.shoe)\\nplt.scatter(survey.shoe, survey.height)\\nplt.xlabel(\"Shoe size\")\\nplt.ylabel(\"Height\")\\nAlthough ols performs a complete analysis of the linear model, not all its calculations\\nneed to be presented. A summary of the results can be obtained with the methodsummary.\\nprint (fit.summary())\\nDep. Variable: height R-squared: 0.178\\nModel: OLS Adj. R-squared: 0.170\\nMethod: Least Squares F-statistic: 21.28\\nNo. Observations: 100 Prob (F-statistic): 1.20e-05\\nDf Residuals: 98 Log-Likelihood: -363.88\\nDf Model: 1 AIC: 731.8\\nCovariance Type: nonrobust BIC: 737.0'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 211, 'page_label': '194'}, page_content='Df Model: 1 AIC: 731.8\\nCovariance Type: nonrobust BIC: 737.0\\n=====================================================================\\ncoef std err t P>|t| [0.025 0.975]\\n--------------------------------------------------------------------\\nIntercept 145.7776 5.763 25.296 0.000 134.341 157.214\\nshoe 1.0048 0.218 4.613 0.000 0.573 1.437\\n=====================================================================\\nOmnibus: 1.958 Durbin-Watson: 1.772\\nProb(Omnibus): 0.376 Jarque-Bera (JB): 1.459\\nSkew: -0.072 Prob(JB): 0.482\\nKurtosis: 2.426 Cond. No. 164.\\nThe main output items are the following:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 212, 'page_label': '195'}, page_content='Regression 195\\n• coef:Estimates of the parameters of the regression line.\\n• std error: Standard deviations of the estimators of the regression line. These are\\nthe square roots of the variances of the {bβi}obtained in (5.25). ☞ 186\\n• t:Realization of Student’s test statistics associated with the hypotheses H0 : βi = 0\\nand H1 : βi , 0, i = 0,1. In particular, the outcome of T in (5.19). ☞ 183\\n• P>|t|: P-value of Student’s test (two-sided test).\\n• [0.025 0.975]: 95% confidence intervals for the parameters.\\n• R-Squared: Coefficient of determination R2 (percentage of variation explained by\\nthe regression), as defined in (5.18). ☞ 181\\n• Adj. R-Squared:adjusted R2 (explained in Section 5.3.7).\\n• F-statistic: Realization of the F test statistic (5.21) associated with testing the ☞ 183\\nfull model against the default model. The associated degrees of freedom (Df Model\\n= 1 and Df Residuals= n−2) are given, as is the P-value:Prob (F-statistic).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 212, 'page_label': '195'}, page_content='= 1 and Df Residuals= n−2) are given, as is the P-value:Prob (F-statistic).\\n• AIC:The AIC number in (5.15); that is, minus two times the log-likelihood plus two ☞ 177\\ntimes the number of model parameters (which is 3 here).\\nYou can access all the numerical values as they are attributes of the fit object. First\\ncheck which names are available, as in:\\ndir (fit)\\nThen access the values via the dot construction. For example, the following extracts the\\nP-value for the slope.\\nfit.pvalues[1]\\n1.1994e-05\\nThe results show strong evidence for a linear relationship between shoe size and height\\n(or, more accurately, strong evidence that the slope of the regression line is not zero), as\\nthe P-value for the corresponding test is very small (1 .2 ·10−5). The estimate of the slope\\nindicates that the di fference between the average height of students whose shoe size is\\ndifferent by one cm is 1.0048 cm.\\nOnly 17 .84% of the variability of student height is explained by the shoe size. We'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 212, 'page_label': '195'}, page_content='Only 17 .84% of the variability of student height is explained by the shoe size. We\\ntherefore need to add other explanatory variables to the model (multiple linear regression)\\nto increase the model’s predictive power.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 213, 'page_label': '196'}, page_content='196 Linear Models in Python\\n5.6.3 Analysis of Variance (ANOVA)\\nWe continue the student survey example of the previous section, but now add an extra\\nvariable, and also consider an analysis of variance of the model. Instead of “explaining”\\nthe student height via their shoe size, we include weight as an explanatory variable. The\\ncorresponding ols formula for this model is\\nheight∼shoe + weight,\\nmeaning that each random height, denoted by Height, satisfies\\nHeight = β0 + β1shoe + β2weight + ε,\\nwhere εis a normally distributed error term with mean 0 and varianceσ2. Thus, the model\\nhas 4 parameters. Before analyzing the model we present a scatterplot of all pairs of vari-\\nables, using scatter_matrix.\\nmodel = ols(\"height~shoe+weight\", data=survey)\\nfit = model.fit()\\naxes = pd.plotting.scatter_matrix(\\nsurvey[[ \\'height \\',\\'shoe \\',\\'weight \\']])\\nplt.show()\\n150\\n175\\nheight\\n20\\n30\\nshoe\\n150\\n175\\nheight\\n50\\n100\\nweight\\n20\\n30\\nshoe\\n50\\n100\\nweight'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 213, 'page_label': '196'}, page_content='plt.show()\\n150\\n175\\nheight\\n20\\n30\\nshoe\\n150\\n175\\nheight\\n50\\n100\\nweight\\n20\\n30\\nshoe\\n50\\n100\\nweight\\nFigure 5.7: Scatterplot of all pairs of variables: height (cm), shoe (cm), and weight (kg).\\nAs for the simple linear regression model in the previous section, we can analyze the\\nmodel using the summary method (below we have omitted some output):\\nfit.summary()'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 214, 'page_label': '197'}, page_content='Regression 197\\nDep. Variable: height R-squared: 0.430\\nModel: OLS Adj. R-squared: 0.418\\nMethod: Least Squares F-statistic: 36.61\\nNo. Observations: 100 Prob (F-statistic): 1.43e-12\\nDf Residuals: 97 Log-Likelihood: -345.58\\nDf Model: 2 AIC: 697.2\\nBIC: 705.0\\n======================================================================\\ncoef std err t P>|t| [0.025 0.975]\\n----------------------------------------------------------------------\\nIntercept 132.2677 5.247 25.207 0.000 121.853 142.682\\nshoe 0.5304 0.196 2.703 0.008 0.141 0.920\\nweight 0.3744 0.057 6.546 0.000 0.261 0.488\\nThe F-statistic is used to test whether the full model (here with two explanatory\\nvariables) is better at “explaining” the height than the default model. The corresponding\\nnull hypothesis is H0 : β1 = β2 = 0. The assertion of interest isH1: at least one of the coeffi-\\ncients βj ( j = 1,2) is significantly different from zero. Given the result of this test (P-value'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 214, 'page_label': '197'}, page_content='cients βj ( j = 1,2) is significantly different from zero. Given the result of this test (P-value\\n= 1.429·10−12), we can conclude that at least one of the explanatory variables is associated\\nwith height. The individual Student tests indicate that:\\n• shoe size is linearly associated with student height, after adjusting for weight, with\\nP-value 0.0081. At the same weight, an increase of one cm in shoe size corresponds\\nto an increase of 0.53 cm in average student height;\\n• weight is linearly associated with student height, after adjusting for shoe size (the\\nP-value is actually 2.82 ·10−09; the reported value of 0.000 should be read as “less\\nthan 0.001”). At the same shoe size, an increase of one kg in weight corresponds to\\nan increase of 0.3744 cm in average student height.\\nFurther understanding is extracted from the model by conducting an analysis of vari-\\nance. The standard statsmodels function is anova_lm. The input to this function is the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 214, 'page_label': '197'}, page_content='ance. The standard statsmodels function is anova_lm. The input to this function is the\\nfit object (obtained from model.fit()) and the output is a DataFrameobject.\\ntable = sm.stats.anova_lm(fit)\\nprint (table)\\ndf sum_sq mean_sq F PR(>F)\\nshoe 1.0 1840.467359 1840.467359 30.371310 2.938651e-07\\nweight 1.0 2596.275747 2596.275747 42.843626 2.816065e-09\\nResidual 97.0 5878.091294 60.598879 NaN NaN\\nThe meaning of the columns is as follows.\\n• df: The degrees of freedom of the variables, according to the sum of squares decom-\\nposition (5.17). As both shoe and weight are quantitative variables, their degrees ☞ 181\\nof freedom are both 1 (each corresponding to a single column in the overall model\\nmatrix). The degrees of freedom for the residuals is n −p = 100 −3 = 97.\\n• sum sq: The sum of squares according to (5.17). The total sum of squares is the\\nsum of all the entries in this column. The residual error in the model that cannot be\\nexplained by the variables is RSS ≈5878.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 215, 'page_label': '198'}, page_content='198 Linear Models in Python\\n• mean sq: The sum of squares divided by their degrees of freedom. Note that the\\nresidual square error RSE = RSS/(n −p) = 60.6 is an unbiased estimate of the\\nmodel variance σ2; see Section 5.4.☞182\\n• F: These are the outcomes of the test statistic (5.22).☞184\\n• PR(>F): These are the P-values corresponding to the test statistic in the preceding\\ncolumn and are computed using an F distribution whose degrees of freedom are\\ngiven in the dfcolumn.\\nThe ANOV A table indicates that the shoe variable explains a reasonable amount of the\\nvariation in the model, as evidenced by a sum of squares contribution of 1840 out of 1840+\\n2596+5878 = 10314 and a very small P-value. Aftershoeis included in the model, it turns\\nout that the weightvariable explains even more of the remaining variability, with an even\\nsmaller P-value. The remaining sum of squares (5878) is 57% of the total sum of squares,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 215, 'page_label': '198'}, page_content='smaller P-value. The remaining sum of squares (5878) is 57% of the total sum of squares,\\nyielding a 43% reduction, in accordance with the R2 value reported in the summary for the\\nols method. As mentioned in Section 5.4.1, the order in which the ANOV A is conducted\\nis important. To illustrate this, consider the output of the following commands.\\nmodel = ols(\"height~weight+shoe\", data=survey)\\nfit = model.fit()\\ntable = sm.stats.anova_lm(fit)\\nprint (table)\\ndf sum_sq mean_sq F PR(>F)\\nweight 1.0 3993.860167 3993.860167 65.906502 1.503553e-12\\nshoe 1.0 442.882938 442.882938 7.308434 8.104688e-03\\nResidual 97.0 5878.091294 60.598879 NaN NaN\\nWe see that weight as a single model variable explains much more of the variability\\nthan shoe did. If we now also include shoe, we only obtain a small (but according to the\\nP-value still significant) reduction in the model variability.\\n5.6.4 Confidence and Prediction Intervals'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 215, 'page_label': '198'}, page_content=\"5.6.4 Confidence and Prediction Intervals\\nIn statsmodels a method for computing confidence or prediction intervals from a dic-\\ntionary of explanatory variables is get_prediction. It simply executes formula (5.24) or\\n(5.26). A simpler version is predict, which only returns the predicted value.☞186\\nContinuing the student survey example, suppose we wish to predict the height of a\\nperson with shoe size 30 cm and weight 75 kg. Confidence and prediction intervals can\\nbe obtained as given in the code below. The new explanatory variable is entered as a dic-\\ntionary. Notice that the 95% prediction interval (for the corresponding random response) is\\nmuch wider than the 95% confidence interval (for the expectation of the random response).\\nx = { 'shoe ': [30.0], 'weight ': [75.0]} # new input (dictionary)\\npred = fit.get_prediction(x)\\npred.summary_frame(alpha=0.05).unstack()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 216, 'page_label': '199'}, page_content='Regression 199\\nmean 0 176.261722 # predicted value\\nmean_se 0 1.054015\\nmean_ci_lower 0 174.169795 # lower bound for CI\\nmean_ci_upper 0 178.353650 # upper bound for CI\\nobs_ci_lower 0 160.670610 # lower bound for PI\\nobs_ci_upper 0 191.852835 # upper bound for PI\\ndtype: float64\\n5.6.5 Model Validation\\nWe can perform an analysis of residuals to examine whether the underlying assumptions\\nof the (normal) linear regression model are verified. Various plots of the residuals can be\\nused to inspect whether the assumptions on the errors{εi}are satisfied. Figure 5.8 gives two\\nsuch plots. The first is a scatterplot of the residuals{ei}against the fitted valuesbyi. When the\\nmodel assumptions are valid, the residuals, as approximations of the model error, should\\nbehave approximately as iid normal random variables for each of the fitted values, with a\\nconstant variance. In this case we see no strong aberrant structure in this plot. The residuals'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 216, 'page_label': '199'}, page_content='constant variance. In this case we see no strong aberrant structure in this plot. The residuals\\nare fairly evenly spread and symmetrical about they = 0 line (not shown). The second plot\\nis a quantile–quantile (or qq) plot. This is a useful way to check for normality of the error\\nterms, by plotting the sample quantiles of the residuals against the theoretical quantiles\\nof the standard normal distribution. Under the model assumptions, the points should lie\\napproximately on a straight line. For the current case there does not seem to be an extreme\\ndeparture from normality. Drawing a histogram or density plot of the residuals will also\\nhelp to verify the normality assumption. The following code was used.\\nplt.plot(fit.fittedvalues,fit.resid, \\'.\\')\\nplt.xlabel(\"fitted values\")\\nplt.ylabel(\"residuals\")\\nsm.qqplot(fit.resid)\\n155\\n 160\\n 165\\n 170\\n 175\\n 180\\n 185\\n 190\\n 195\\nfitted values\\n25\\n20\\n15\\n10\\n5\\n0\\n5\\n10\\n15\\n20\\nresiduals\\n3\\n 2\\n 1\\n 0\\n 1\\n 2\\n 3\\nTheoretical Quantiles\\n25\\n20\\n15\\n10\\n5\\n0\\n5\\n10\\n15\\n20'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 216, 'page_label': '199'}, page_content='15\\n10\\n5\\n0\\n5\\n10\\n15\\n20\\nresiduals\\n3\\n 2\\n 1\\n 0\\n 1\\n 2\\n 3\\nTheoretical Quantiles\\n25\\n20\\n15\\n10\\n5\\n0\\n5\\n10\\n15\\n20\\nSample Quantiles\\nFigure 5.8: Left: residuals against fitted values. Right: a qq plot of the residuals. Neither\\nshows clear evidence against the model assumptions of constant variance and normality.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 217, 'page_label': '200'}, page_content='200 Linear Models in Python\\n5.6.6 Variable Selection\\nAmong the large number of possible explanatory variables, we wish to select those which\\nbest explain the observed responses. By eliminating redundant explanatory variables, we\\nreduce the statistical error without increasing the approximation error, and thus reduce the\\n(expected) generalization risk of the learner.\\nIn this section, we briefly present two methods for variable selection. They are illus-\\ntrated on a few variables from the data set birthwt discussed in Section 1.5.3.2. The data☞13\\nset contains information on the birth weights (masses) of babies, as well as various char-\\nacteristics of the mother, such as whether she smokes, her age, etc. We wish to explain\\nthe child’s weight at birth using various characteristics of the mother, her family history,\\nand her behavior during pregnancy. The response variable is weight at birth (quantitative\\nvariable bwt, expressed in grams); the explanatory variables are given below.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 217, 'page_label': '200'}, page_content='variable bwt, expressed in grams); the explanatory variables are given below.\\nThe data can be obtained as explained in Section 1.5.3.2, or from statsmodels in the\\nfollowing way:\\nbwt = sm.datasets.get_rdataset(\"birthwt\",\"MASS\").data\\nHere is some information about the explanatory variables that we will investigate.\\nage: mother \\'s age in years\\nlwt: mother \\'s weight in lbs\\nrace: mother \\'s race (1 = white, 2 = black, 3 = other)\\nsmoke: smoking status during pregnancy (0 = no, 1 = yes)\\nptl: no. of previous premature labors\\nht: history of hypertension (0 = no, 1 = yes)\\nui: presence of uterine irritability (0 = no, 1 = yes)\\nftv: no. of physician visits during first trimester\\nbwt: birth weight in grams\\nWe can see the structure of the variables via bwt.info(). Check yourself that all\\nvariables are defined as quantitative (int64). However, the variables race, smoke, ht,\\nand ui should really be interpreted as qualitative (factors). To fix this, we could redefine'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 217, 'page_label': '200'}, page_content='and ui should really be interpreted as qualitative (factors). To fix this, we could redefine\\nthem with the methodastype, similar to what we did in Chapter 1. Alternatively, we could\\nuse the C() construction in a statsmodels formula to let the program know that certain\\nvariables are factors. We will use the latter approach.\\nFor binary features it does not matter whether the variables are interpreted as\\nfactorial or numerical as the numerical and summary results are identical.\\nWe consider the explanatory variableslwt, age, ui, smoke, ht, and two recoded binary\\nvariables ftv1and ptl1. We define ftv1= 1 if there was at least one visit to a physician,\\nand ftv1= 0 otherwise. Similarly, we defineptl1= 1 if there is at least one preterm birth\\nin the family history, and ptl1= 0 otherwise.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 218, 'page_label': '201'}, page_content='Regression 201\\nftv1 = (bwt[ \\'ftv\\']>=1).astype( int )\\nptl1 = (bwt[ \\'ptl\\']>=1).astype( int )\\n5.6.6.1 Forward Selection and Backward Elimination\\nThe forward selection forward\\nselection\\nmethod is an iterative method for variable selection. In the first\\niteration we consider which feature f1 is the most significant in terms of its P-value in the\\nmodels bwt∼f1, with f1 ∈{lwt,age,... }. This feature is then selected into the model. In\\nthe second iteration, the featuref2 that has the smallest P-value in the modelsbwt∼f1+f2\\nis selected, where f2 , f1, and so on. Usually only features are selected that have a P-\\nvalue of at most 0.05. The following Python program automates this procedure. Instead of\\nselecting on the P-value one could select on the AIC or BIC value.\\nforwardselection.py\\nimport statsmodels.api as sm\\nfrom statsmodels.formula.api import ols\\nbwt = sm.datasets.get_rdataset(\"birthwt\",\"MASS\").data\\nftv1 = (bwt[ \\'ftv\\']>=1).astype( int )\\nptl1 = (bwt[ \\'ptl\\']>=1).astype( int )'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 218, 'page_label': '201'}, page_content=\"ftv1 = (bwt[ 'ftv']>=1).astype( int )\\nptl1 = (bwt[ 'ptl']>=1).astype( int )\\nremaining_features = { 'lwt', 'age', 'C(ui) ', 'smoke ',\\n'C(ht) ', 'ftv1 ', 'ptl1 '}\\nselected_features = []\\nwhile remaining_features:\\nPF = [] #list of (P value , feature)\\nfor f in remaining_features:\\ntemp = selected_features + [f] #temporary list of features\\nformula = 'bwt~ ' + '+'.join(temp)\\nfit = ols(formula,data=bwt).fit()\\npval= fit.pvalues[-1]\\nif pval < 0.05:\\nPF.append((pval,f))\\nif PF: #if not empty\\nPF.sort(reverse=True)\\n(best_pval, best_f) = PF.pop()\\nremaining_features.remove(best_f)\\nprint ('feature {} with P-value = {:.2E} '.\\nformat (best_f, best_pval))\\nselected_features.append(best_f)\\nelse :\\nbreak\\nfeature C(ui) with P-value = 7.52E-05\\nfeature C(ht) with P-value = 1.08E-02\\nfeature lwt with P-value = 6.01E-03\\nfeature smoke with P-value = 7.27E-03\\nIn backward elimination backward\\nelimination\\nwe start with the complete model (all features included) and\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 218, 'page_label': '201'}, page_content='elimination\\nwe start with the complete model (all features included) and\\nat each step, we remove the variable with the highest P-value, as long as it is not significant\\n(greater than 0.05). We leave it as an exercise to verify that the order in which the fea-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 219, 'page_label': '202'}, page_content='202 Linear Models in Python\\ntures are removed is: age, ftv1, and ptl1. In this case, forward selection and backward\\nelimination result in the same model, but this need not be the case in general.\\nThis way of model selection has the advantage of being easy to use and of treating the\\nquestion of variable selection in a systematic manner. The main drawback is that variables\\nare included or deleted based on purely statistical criteria, without taking into account the\\naim of the study. This usually leads to a model which may be satisfactory from a statistical\\npoint of view, but in which the variables are not necessarily the most relevant when it comes\\nto understanding and interpreting the data in the study.\\nOf course, we can choose to investigate any combination of features, not just the ones\\nsuggested by the above variable selection methods. For example, let us see if the mother’s\\nweight, her age, her race, and whether she smokes explain the baby’s birthweight.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 219, 'page_label': '202'}, page_content=\"weight, her age, her race, and whether she smokes explain the baby’s birthweight.\\nformula = 'bwt~lwt+age+C(race)+ smoke '\\nbwt_model = ols(formula, data=bwt).fit()\\nprint (bwt_model.summary())\\nOLS Regression Results\\n======================================================================\\nDep. Variable: bwt R-squared: 0.148\\nModel: OLS Adj. R-squared: 0.125\\nMethod: Least Squares F-statistic: 6.373\\nNo. Observations: 189 Prob (F-statistic): 1.76e-05\\nDf Residuals: 183 Log-Likelihood: -1498.4\\nDf Model: 5 AIC: 3009.\\nBIC: 3028.\\n=====================================================================\\ncoef std err t P>|t| [0.025 0.975]\\n----------------------------------------------------------------------\\nIntercept 2839.4334 321.435 8.834 0.000 2205.239 3473.628\\nC(race)[T.2] -510.5015 157.077 -3.250 0.001 -820.416 -200.587\\nC(race)[T.3] -398.6439 119.579 -3.334 0.001 -634.575 -162.713\\nsmoke -401.7205 109.241 -3.677 0.000 -617.254 -186.187\\nlwt 3.9999 1.738 2.301 0.022 0.571 7.429\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 219, 'page_label': '202'}, page_content='smoke -401.7205 109.241 -3.677 0.000 -617.254 -186.187\\nlwt 3.9999 1.738 2.301 0.022 0.571 7.429\\nage -1.9478 9.820 -0.198 0.843 -21.323 17.427\\n======================================================================\\nOmnibus: 3.916 Durbin-Watson: 0.458\\nProb(Omnibus): 0.141 Jarque-Bera (JB): 3.718\\nSkew: -0.343 Prob(JB): 0.156\\nKurtosis: 3.038 Cond. No. 899.\\nGiven the result of Fisher’s global test given byProb (F-Statistic)in the summary\\n(P-value = 1.76 ×10−5), we can conclude that at least one of the explanatory variables is\\nassociated with child weight at birth, after adjusting for the other variables. The individual\\nStudent tests indicate that:\\n• the mother’s weight is linearly associated with child weight, after adjusting for age,\\nrace, and smoking status (P-value = 0.022). At the same age, race, and smoking\\nstatus, an increase of one pound in the mother’s weight corresponds to an increase\\nof 4 g in the average child weight at birth;'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 219, 'page_label': '202'}, page_content='of 4 g in the average child weight at birth;\\n• the age of the mother is not significantly linearly associated with child weight at\\nbirth, when mother weight, race, and smoking status are already taken into account'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 220, 'page_label': '203'}, page_content='Regression 203\\n(P-value = 0.843);\\n• weight at birth is significantly lower for a child born to a mother who smokes, com-\\npared to children born to non-smoking mothers of the same age, race, and weight,\\nwith a P-value of 0.00031 (to see this, inspect bwt_model.pvalues). At the same\\nage, race, and mother weight, the child’s weight at birth is 401.720 g less for a\\nsmoking mother than for a non-smoking mother;\\n• regarding the interpretation of the variable race, we note that the first level of this\\ncategorical variable corresponds to white mothers. The estimate of −510.501 g for\\nC(race)[T.2] represents the di fference in the child’s birth weight between black\\nmothers and white mothers (reference group), and this result is significantly different\\nfrom zero (P-value = 0.001) in a model adjusted for the mother’s weight, age, and\\nsmoking status.\\n5.6.6.2 Interaction\\nWe can also include interaction terms in the model. Let us see whether there is any inter-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 220, 'page_label': '203'}, page_content=\"We can also include interaction terms in the model. Let us see whether there is any inter-\\naction effect between smokeand agevia the model\\nBwt = β0 + β1age + β2smoke + β3age ×smoke + ε.\\nIn Pythonthis can be done as follows (below we have removed some output):\\nformula = 'bwt~age*smoke '\\nbwt_model = ols(formula, data=bwt).fit()\\nprint (bwt_model.summary())\\nOLS Regression Results\\n======================================================================\\nDep. Variable: bwt R-squared: 0.069\\nModel: OLS Adj. R-squared: 0.054\\nMethod: Least Squares F-statistic: 4.577\\nNo. Observations: 189 Prob (F-statistic): 0.00407\\nDf Residuals: 183 Log-Likelihood: -1506.8\\nDf Model: 5 AIC: 3009.\\nBIC: 3028.\\n======================================================================\\ncoef std err t P>|t| [0.025 0.975]\\n----------------------------------------------------------------------\\nIntercept 2406.1 292.190 8.235 0.000 1829.6 2982.5\\nsmoke 798.2 484.342 1.648 0.101 -157.4 1753.7\\nage 27.7 12.149 2.283 0.024 3.8 51.7\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 220, 'page_label': '203'}, page_content='smoke 798.2 484.342 1.648 0.101 -157.4 1753.7\\nage 27.7 12.149 2.283 0.024 3.8 51.7\\nage:smoke -46.6 20.447 -2.278 0.024 -86.9 -6.2\\nWe observe that the estimate forβ3 (−46.6) is significantly different from zero (P-value\\n= 0.024). We therefore conclude that the e ffect of the mother’s age on the child’s weight\\ndepends on the smoking status of the mother. The results on association between mother\\nage and child weight must therefore be presented separately for the smoking and the non-\\nsmoking group. For non-smoking mothers ( smoke = 0), the mean child weight at birth\\nincreases on average by 27.7 grams for each year of the mother’s age. This is statistically'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 221, 'page_label': '204'}, page_content='204 Generalized Linear Models\\nsignificant, as can be seen from the 95% confidence intervals for the parameters (which\\ndoes not contain zero):\\nbwt_model.conf_int()\\n0 1\\nIntercept 1829.605754 2982.510194\\nage 3.762780 51.699977\\nsmoke -157.368023 1753.717779\\nage:smoke -86.911405 -6.232425\\nSimilarly, for smoking mothers, there seems to be a decrease in birthweight, bβ1 + bβ3 =\\n27.7 −46.6 = −18.9, but this is not statistically significant; see Exercise 6.\\n5.7 Generalized Linear Models\\nThe normal linear model in Section 2.8 deals with continuous response variables — such\\nas height and crop yield — and continuous or discrete explanatory variables. Given the\\nfeature vectors {xi}, the responses {Yi}are independent of each other, and each has a normal\\ndistribution with mean x⊤\\ni β, where x⊤\\ni is the i-th row of the model matrix X. Generalized\\nlinear models allow for arbitrary response distributions, including discrete ones.\\nDefinition 5.2: Generalized Linear Model'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 221, 'page_label': '204'}, page_content='Definition 5.2: Generalized Linear Model\\nIn a generalized linear modelgeneralized\\nlinear model\\n(GLM) the expected response for a given feature vec-\\ntor x = [x1,..., xp]⊤is of the form\\nE[Y |X = x] = h(x⊤β) (5.35)\\nfor some function h, which is called the activation functionactivation\\nfunction\\n. The distribution of\\nY (for a given x) may depend on additional dispersion parameters that model the\\nrandomness in the data that is not explained by x.\\nThe inverse of function h is called the link functionlink function . As for the linear model, (5.35) is\\na model for a single pair ( x,Y). Using the model simplification introduced at the end of\\nSection 5.1, the corresponding model for a whole training set T= {(xi,Yi)}is that the {xi}\\nare fixed and that the {Yi}are independent; each Yi satisfying (5.35) with x = xi. Writing\\nY = [Y1,..., Yn]⊤and defining h as the multivalued function with components h, we have\\nEXY = h(Xβ),\\nwhere X is the (model) matrix with rows x⊤\\n1 ,..., x⊤'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 221, 'page_label': '204'}, page_content='EXY = h(Xβ),\\nwhere X is the (model) matrix with rows x⊤\\n1 ,..., x⊤\\nn . A common assumption is that\\nY1,..., Yn come from the same family of distributions, e.g., normal, Bernoulli, or Pois-\\nson. The central focus is the parameter vector β, which summarizes how the matrix of\\nexplanatory variables X affects the response vector Y. The class of generalized linear mod-\\nels can encompass a wide variety of models. Obviously the normal linear model (2.34) is\\na generalized linear model, with E[Y |X = x] = x⊤β, so that h is the identity function. In\\nthis case, Y ∼N(x⊤β,σ2), i = 1,..., n, where σ2 is a dispersion parameter.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 222, 'page_label': '205'}, page_content='Regression 205\\nExample 5.10 (Logistic Regression) In a logistic regression logistic\\nregression\\nor logit model , we as-\\nsume that the response variables Y1,..., Yn are independent and distributed according to\\nYi ∼Ber(h(x⊤\\ni β)),where h here is defined as the cdf of the logistic distribution logistic\\ndistribution\\n:\\nh(x) = 1\\n1 + e−x .\\nLarge values of x⊤\\ni βthus lead to a high probability that Yi = 1, and small (negative) values\\nof x⊤\\ni βcause Yi to be 0 with high probability. Estimation of the parameter vector βfrom\\nthe observed data is not as straightforward as for the ordinary linear model, but can be\\naccomplished via the minimization of a suitable training loss, as explained below.\\nAs the {Yi}are independent, the pdf of Y = [Y1,..., Yn]⊤is\\ng(y |β,X) =\\nnY\\ni=1\\n[h(x⊤\\ni β)]yi [1 −h(x⊤\\ni β)]1−yi .\\nMaximizing the log-likelihood ln g(y |β,X) with respect to β gives the maximum likeli-\\nhood estimator of β. In a supervised learning framework, this is equivalent to minimizing:\\n−1'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 222, 'page_label': '205'}, page_content='hood estimator of β. In a supervised learning framework, this is equivalent to minimizing:\\n−1\\nn ln g(y |β,X) = −1\\nn\\nnX\\ni=1\\nln g(yi |β,xi)\\n= −1\\nn\\nnX\\ni=1\\n\\x02yi ln h(x⊤\\ni β) + (1 −yi) ln(1 −h(x⊤\\ni β))\\x03.\\n(5.36)\\nBy comparing (5.36) with (4.4), we see that we can interpret (5.36) as the cross-entropy ☞ 123\\ntraining loss associated with comparing a true conditional pdf f (y |x) with an approxima-\\ntion pdf g(y |β,x) via the loss function\\nLoss( f (y |x),g(y |β,x)) := −ln g(y |β,x) = −y ln h(x⊤β) −(1 −y) ln(1 −h(x⊤β)).\\nMinimizing (5.36) in terms of βactually constitutes a convex optimization problem. Since\\nln h(x⊤β) = −ln(1 + e−x⊤β) and ln(1 −h(x⊤β)) = −x⊤β−ln(1 + e−x⊤β), the cross-entropy\\ntraining loss (5.36) can be rewritten as\\nrτ(β) := 1\\nn\\nnX\\ni=1\\nh\\n(1 −yi)x⊤\\ni β+ ln\\n\\x10\\n1 + e−x⊤\\ni β\\x11i\\n.\\nWe leave it as Exercise 7 to show that the gradient ∇rτ(β) and Hessian H(β) of rτ(β) are\\ngiven by\\n∇rτ(β) = 1\\nn\\nnX\\ni=1\\n(µi −yi) xi (5.37)\\nand\\nH(β) = 1\\nn\\nnX\\ni=1\\nµi(1 −µi) xi x⊤\\ni , (5.38)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 222, 'page_label': '205'}, page_content='given by\\n∇rτ(β) = 1\\nn\\nnX\\ni=1\\n(µi −yi) xi (5.37)\\nand\\nH(β) = 1\\nn\\nnX\\ni=1\\nµi(1 −µi) xi x⊤\\ni , (5.38)\\nrespectively, where µi := h(x⊤\\ni β).\\nNotice that H(β) is a positive semidefinite matrix for all values of β, implying the ☞ 403'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 223, 'page_label': '206'}, page_content='206 Generalized Linear Models\\nconvexity of rτ(β). Consequently, we can find an optimal βefficiently; e.g., via Newton’s\\nmethod. Specifically, given an initial valueβ0, for t = 1,2,..., iteratively compute☞409\\nβt = βt−1 −H−1(βt−1) ∇rτ(βt−1), (5.39)\\nuntil the sequence β0,β1,β2,... is deemed to have converged, using some pre-fixed con-\\nvergence criterion.\\nFigure 5.9 shows the outcomes of 100 independent Bernoulli random variables, where\\neach success probability, (1+exp(−(β0 +β1 x)))−1, depends on x and β0 = −3, β1 = 10. The\\ntrue logistic curve is also shown (dashed line). The minimum training loss curve (red line)\\nis obtained via the Newton scheme (5.39), giving estimates bβ0 = −2.66 and bβ1 = 10.08.\\nThe Python code is given below.\\n-1 -0.5 0 0.5 1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure 5.9: Logistic regression data (blue dots), fitted curve (red), and true curve (black\\ndashed).\\nlogreg1d.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom numpy.linalg import lstsq\\nn = 100 # sample size'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 223, 'page_label': '206'}, page_content='import matplotlib.pyplot as plt\\nfrom numpy.linalg import lstsq\\nn = 100 # sample size\\nx = (2*np.random.rand(n)-1).reshape(n,1) # explanatory variables\\nbeta = np.array([-3, 10])\\nXmat = np.hstack((np.ones((n,1)), x))\\np = 1/(1 + np.exp(-Xmat @ beta))\\ny = np.random.binomial(1,p,n) # response variables\\n# initial guess\\nbetat = lstsq((Xmat.T @ Xmat),Xmat.T @ y, rcond=None)[0]\\ngrad = np.array([2,1]) # gradient\\nwhile (np. sum (np. abs (grad)) > 1e-5) : # stopping criteria\\nmu = 1/(1+np.exp(-Xmat @ betat))\\n# gradient\\ndelta = (mu - y).reshape(n,1)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 224, 'page_label': '207'}, page_content=\"Regression 207\\ngrad = np. sum (np.multiply( np.hstack((delta,delta)),Xmat), axis\\n=0).T\\n# Hessian\\nH = Xmat.T @ np.diag(np.multiply(mu,(1-mu))) @ Xmat\\nbetat = betat - lstsq(H,grad,rcond=None)[0]\\nprint (betat)\\nplt.plot(x,y, '.') # plot data\\nxx = np.linspace(-1,1,40).reshape(40,1)\\nXXmat = np.hstack( (np.ones(( len (xx),1)), xx))\\nyy = 1/(1 + np.exp(-XXmat @ beta))\\nplt.plot(xx,yy, 'r-') #true logistic curve\\nyy = 1/(1 + np.exp(-XXmat @ betat));\\nplt.plot(xx,yy, 'k--')\\nFurther Reading\\nAn excellent overview of regression is provided in [33] and an accessible mathematical\\ntreatment of linear regression models can be found in [108]. For extensions to nonlinear\\nregression we refer the reader to [7]. A practical introduction to multilevel /hierarchical\\nmodels is given in [47]. For further discussion on regression with discrete responses (clas-\\nsification) we refer to Chapter 7 and the further reading therein. On the important question ☞ 251\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 224, 'page_label': '207'}, page_content='sification) we refer to Chapter 7 and the further reading therein. On the important question ☞ 251\\nof how to handle missing data, the classic reference is [80] (see also [85]) and a modern\\napplied reference is [120].\\nExercises\\n1. Following his mentor Francis Galton, the mathematician /statistician Karl Pearson con-\\nducted comprehensive studies comparing hereditary traits between members of the same\\nfamily. Figure 5.10 depicts the measurements of the heights of 1078 fathers and their\\nadult sons (one son per father). The data is available from the book’s GitHub site as\\npearson.csv.\\n(a) Show that sons are on average 1 inch taller than the fathers.\\n(b) We could try to “explain” the height of the son by taking the height of his father and\\nadding 1 inch. The prediction line y = x + 1 (red dashed) is given Figure 5.10. The\\nblack solid line is the fitted regression line. This line has a slope less than 1, and'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 224, 'page_label': '207'}, page_content='black solid line is the fitted regression line. This line has a slope less than 1, and\\ndemonstrates Galton’s “regression” to the average. Find the intercept and slope of the\\nfitted regression line.\\n2. For the simple linear regression model, show that the values for bβ1 and bβ0 that solve the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 225, 'page_label': '208'}, page_content='208 Exercises\\n58 60 62 64 66 68 70 72 74 76\\nHeight Father (in)\\n60\\n65\\n70\\n75Height Son (in)\\nFigure 5.10: A scatterplot of heights from Pearson’s data.\\nequations (5.9) are:\\nbβ1 =\\nPn\\ni=1(xi −x)(yi −y)Pn\\ni=1(xi −x)2 (5.40)\\nbβ0 = y −bβ1 x, (5.41)\\nprovided that not all xi are the same.\\n3. Edwin Hubble discovered that the universe is expanding. If v is a galaxy’s recession ve-\\nlocity (relative to any other galaxy) and d is its distance (from that same galaxy), Hubble’s\\nlaw states that\\nv = Hd,\\nwhere H is known as Hubble’s constant. The following are distance (in millions of light-\\nyears) and velocity (thousands of miles per second) measurements made on five galactic\\nclusters.\\ndistance 68 137 315 405 700\\nvelocity 2.4 4.7 12.0 14.4 26.0\\nState the regression model and estimate H.\\n4. The multiple linear regression model (5.6) can be viewed as a first-order approximation\\nof the general model\\nY = g(x) + ε, (5.42)\\nwhere Eε= 0, Var ε= σ2, and g(x) is some known or unknown function of a d-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 225, 'page_label': '208'}, page_content='Y = g(x) + ε, (5.42)\\nwhere Eε= 0, Var ε= σ2, and g(x) is some known or unknown function of a d-\\ndimensional vector x of explanatory variables. To see this, replace g(x) with its first-order\\nTaylor approximation around some point x0 and write this as β0 + x⊤β. Express β0 and β\\nin terms of g and x0.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 226, 'page_label': '209'}, page_content='Regression 209\\n5. Table 5.6 shows data from an agricultural experiment where crop yield was measured\\nfor two levels of pesticide and three levels of fertilizer. There are three responses for each\\ncombination.\\nTable 5.6: Crop yields for pesticide and fertilizer combinations.\\nFertilizer\\nPesticide Low Medium High\\nNo 3.23, 3.20, 3.16 2.99, 2.85, 2.77 5.72, 5.77, 5.62\\nYes 6.78, 6.73, 6.79 9.07, 9.09, 8.86 8.12, 8.04, 8.31\\n(a) Organize the data in standard form, where each row corresponds to a single meas-\\nurement and the columns correspond to the response variable and the two factor vari-\\nables.\\n(b) Let Yi jk be the response for the k-th replication at level i for factor 1 and level j\\nfor factor 2. To assess which factors best explain the response variable, we use the\\nANOV A model\\nYi jk = µ+ αi + βj + γi j + εi jk, (5.43)\\nwhere P\\ni αi = P\\nj βj = P\\ni γi j = P\\nj γi j = 0. Define β = [µ,α1,α2,β1,β2, β3,γ11,γ12,\\nγ13,γ21,γ22,γ23]⊤. Give the corresponding 18 ×12 model matrix.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 226, 'page_label': '209'}, page_content='γ13,γ21,γ22,γ23]⊤. Give the corresponding 18 ×12 model matrix.\\n(c) Note that the parameters are linearly dependent in this case. For example, α2 = −α1\\nand γ13 = −(γ11 + γ12). To retain only 6 linearly independent variables consider the\\n6-dimensional parameter vector eβ= [µ,α1,β1,β2,γ11,γ12]⊤. Find the matrix M such\\nthat Meβ= β.\\n(d) Give the model matrix corresponding to eβ.\\n6. Show that for the birthweight data in Section 5.6.6.2 there is no significant decrease\\nin birthweight for smoking mothers. [Hint: create a new variable nonsmoke = 1−smoke,\\nwhich reverses the encoding for the smoking and non-smoking mothers. Then, the para-\\nmeter β1 + β3 in the original model is the same as the parameter β1 in the model\\nBwt = β0 + β1age + β2nonsmoke + β3age ×nonsmoke + ε.\\nNow find a 95% for β3 and see if it contains zero.]\\n7. Prove (5.37) and (5.38).\\n8. In the Tobit regression Tobit\\nregression\\nmodel with normally distributed errors, the response is modeled\\nas:\\nYi =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nZi, if ui <Zi'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 226, 'page_label': '209'}, page_content='model with normally distributed errors, the response is modeled\\nas:\\nYi =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nZi, if ui <Zi\\nui, if Zi ⩽ui\\n, Z ∼N(Xβ,σ2In),\\nwhere the model matrix X and the thresholds u1,..., un are given. Typically, ui = 0,i =\\n1,..., n. Suppose we wish to estimate θ:= (β,σ2) via the Expectation–Maximization\\nmethod, similar to the censored data Example 4.2. Let y = [y1,..., yn]⊤ be the vector ☞ 130\\nof observed data.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 227, 'page_label': '210'}, page_content='210 Exercises\\n(a) Show that the likelihood of y is:\\ng(y |θ) =\\nY\\ni:yi>ui\\nφσ2 (yi −x⊤\\ni β) ×\\nY\\ni:yi=ui\\nΦ((ui −x⊤\\ni β)/σ),\\nwhere Φ is the cdf of the N(0,1) distribution and φσ2 the pdf of the N(0,σ2) distribu-\\ntion.\\n(b) Let y and y be vectors that collect all yi >ui and yi = ui, respectively. Denote the\\ncorresponding matrix of predictors by X and X, respectively. For each observation\\nyi = ui introduce a latent variable zi and collect these into a vector z. For the same\\nindices i collect the corresponding ui into a vector c. Show that the complete-data\\nlikelihood is given by\\ng(y,z |θ) = 1\\n(2πσ2)n/2 exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−∥y −Xβ∥2\\n2σ2 −∥z −Xβ∥2\\n2σ2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f81{z ⩽c}.\\n(c) For the E-step, show that, for a fixed θ,\\ng(z |y,θ) =\\nY\\ni\\ng(zi |y,θ),\\nwhere each g(zi |y,θ) is the pdf of the N((Xβ)i,σ2) distribution, truncated to the in-\\nterval (−∞,ci].\\n(d) For the M-step, compute the expectation of the complete log-likelihood\\n−n\\n2 ln σ2 −n\\n2 ln(2π) −∥y −Xβ∥2\\n2σ2 −E∥Z −Xβ∥2\\n2σ2 .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 227, 'page_label': '210'}, page_content='−n\\n2 ln σ2 −n\\n2 ln(2π) −∥y −Xβ∥2\\n2σ2 −E∥Z −Xβ∥2\\n2σ2 .\\nThen, derive the formulas forβand σ2 that maximize the expectation of the complete\\nlog-likelihood.\\n9. Dowload data set WomenWage.csvfrom the book’s website. This data set is a tidied-up\\nversion of the women’s wages data set from [91]. The first column of the data ( hours) is\\nthe response variable Y. It shows the hours spent in the labor force by married women in\\nthe 1970s. We want to understand what factors determine the participation rate of women\\nin the labor force. The predictor variables are:\\nTable 5.7: Features for the women’s wage data set.\\nFeature Description\\nkidslt6 Number of children younger than 6 years.\\nkidsge6 Number of children older than 6 years.\\nage Age of the married woman.\\neduc Number of years of formal education.\\nexper Number of years of “work experience”.\\nnwifeinc Non-wife income, that is, the income of the husband.\\nexpersq The square of exper, to capture any nonlinear relationships.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 228, 'page_label': '211'}, page_content='Regression 211\\nWe observe that some of the responses areY = 0, that is, some women did not particip-\\nate in the labor force. For this reason, we model the data using the Tobit regression model,\\nin which the response Y is given as:\\nYi =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nZi, if Zi >0\\n0, if Zi ⩽0 , Z ∼N(Xβ,σ2In).\\nWith θ= (β,σ2), the likelihood of the data y = [y1,..., yn]⊤is:\\ng(y |θ) = Q\\ni:yi>0 φσ2 (yi −x⊤\\ni β) ×Q\\ni:yi=0 Φ((ui −x⊤\\ni β)/σ),\\nwhere Φ is the standard normal cdf. In Exercise 8, we derived the EM algorithm for max-\\nimizing the log-likelihood.\\n(a) Write down the EM algorithm in pseudo code as it applies to this Tobit regression.\\n(b) Implement the EM algorithm pseudo code in Python. Comment on which factor you\\nthink is important in determining the labor participation rate of women living in the\\nUSA in the 1970s.\\n10. Let P be a projection matrix. Show that the diagonal elements ofP all lie in the interval\\n[0,1]. In particular, for P = XX+ in Theorem 5.1, the leverage value pi := Pii satisfies'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 228, 'page_label': '211'}, page_content='[0,1]. In particular, for P = XX+ in Theorem 5.1, the leverage value pi := Pii satisfies\\n0 ⩽pi ⩽1 for all i.\\n11. Consider the linear model Y = Xβ+ εin (5.8), with X being the n ×p model matrix\\nand ε having expectation vector 0 and covariance matrix σ2In. Suppose that bβ−i is the\\nleast-squares estimate obtained by omitting the i-th observation, Yi; that is,\\nbβ−i = argmin\\nβ\\nX\\nj,i\\n(Yj −x⊤\\nj β)2,\\nwhere x⊤\\nj is the j-th row of X. Let bY−i = x⊤\\ni\\nbβ−i be the corresponding fitted value atxi. Also,\\ndefine Bi as the least-squares estimator of βbased on the response data\\nY(i) := [Y1,..., Yi−1,bY−i,Yi+1,..., Yn]⊤.\\n(a) Prove that bβ−i = Bi; that is, the linear model obtained from fitting all responses except\\nthe i-th is the same as the one obtained from fitting the data Y(i).\\n(b) Use the previous result to verify that\\nYi −bY−i = (Yi −bYi)/(1 −Pii),\\nwhere P = XX+ is the projection matrix onto the columns of X. Hence, deduce the\\nPRESS formula in Theorem 5.1. ☞ 174'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 228, 'page_label': '211'}, page_content='PRESS formula in Theorem 5.1. ☞ 174\\n12. Take the linear model Y = Xβ+ ε,where X is an n ×p model matrix, ε = 0, and\\nCov(ε) = σ2In. Let P = XX+ be the projection matrix onto the columns of X.\\n(a) Using the properties of the pseudo-inverse (see Definition A.2), show that PP⊤= P. ☞ 360'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 229, 'page_label': '212'}, page_content='212 Exercises\\n(b) Let E = Y −bY be the (random) vector of residuals, where bY = PY. Show that the i-th\\nresidual has a normal distribution with expectation 0 and varianceσ2(1 −Pii) (that is,\\nσ2 times 1 minus the i-th leverage).\\n(c) Show that σ2 can be unbiasedly estimated via\\nS 2 := 1\\nn −p∥Y −bY∥2 = 1\\nn −p∥Y −Xbβ∥2. (5.44)\\n[Hint: use the cyclic property of the trace as in Example 2.3.]\\n13. Consider a normal linear model Y = Xβ+ ε, where X is an n ×p model matrix and\\nε ∼N(0,σ2In). Exercise 12 shows that for any such model the i-th standardized residual\\nEi/(σ√1 −Pii) has a standard normal distribution. This motivates the use of the leverage\\nPii to assess whether the i-th observation is an outlier depending on the size of the i-th\\nresidual relative to √1 −Pii. A more robust approach is to include an estimate for σusing\\nall data except the i-th observation. This gives rise to the studentized residualstudentized\\nresidual\\nTi, defined\\nas\\nTi := Ei\\nS −i\\n√1 −Pii\\n,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 229, 'page_label': '212'}, page_content='residual\\nTi, defined\\nas\\nTi := Ei\\nS −i\\n√1 −Pii\\n,\\nwhere S −i is an estimate of σ obtained by fitting all the observations except the i-th and\\nEi = Yi −bYi is the i-th (random) residual. Exercise 12 shows that we can take, for example,\\nS 2\\n−i = 1\\nn −1 −p∥Y−i −X−ibβ−i∥2, (5.45)\\nwhere X−i is the model matrix X with the i-th row removed, is an unbiased estimator of\\nσ2. We wish to compute S 2\\n−i efficiently, using S 2 in (5.44), as the latter will typically be\\navailable once we have fitted the linear model. To this end, define ui as the i-th unit vector\\n[0,..., 0,1,0,..., 0]⊤, and let\\nY(i) := Y −(Yi −bY−i)ui = Y − Ei\\n1 −Pii\\nui,\\nwhere we have used the fact that Yi −bY−i = Ei/(1 −Pii), as derived in the proof of The-\\norem 5.1. Now apply Exercise 11 to prove that\\nS 2\\n−i = (n −p) S 2 −E2\\ni /(1 −Pii)\\nn −p −1 .\\n14. Using the notation from Exercises 11–13, Cook’s distanceCook’s distance for observation i is defined\\nas\\nDi := ∥bY −bY\\n(i)\\n∥2\\np S2 .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 229, 'page_label': '212'}, page_content='as\\nDi := ∥bY −bY\\n(i)\\n∥2\\np S2 .\\nIt measures the change in the fitted values when thei-th observation is removed, relative to\\nthe residual variance of the model (estimated via S 2).\\nBy using similar arguments as those in Exercise 13, show that\\nDi = Pii E2\\ni\\n(1 −Pii)2 p S2 .\\nIt follows that there is no need to “omit and refit” the linear model in order to compute\\nCook’s distance for the i-th response.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 230, 'page_label': '213'}, page_content='Regression 213\\n15. Prove that if we add an additional feature to the general linear model, then R2, the\\ncoefficient of determination, is necessarily non-decreasing in value and hence cannot be\\nused to compare models with different numbers of predictors.\\n16. Let X := [X1,..., Xn]⊤ and µ := [µ1,...,µ n]⊤. In the fundamental Theorem C.9, we\\nuse the fact that ifXi ∼N(µi,1), i = 1,..., n are independent, then ∥X∥2 has (per definition)\\na noncentral χ2\\nn distribution. Show that ∥X∥2 has moment generating function\\net∥µ∥2/(1−2t)\\n(1 −2t)n/2 , t <1/2,\\nand so the distribution of ∥X∥2 depends on µonly through the norm ∥µ∥.\\n17. Carry out a logistic regression analysis on a (partial) wine data set classification prob-\\nlem. The data can be loaded using the following code.\\nfrom sklearn import datasets\\nimport numpy as np\\ndata = datasets.load_wine()\\nX = data.data[:, [9,10]]\\ny = np.array(data.target==1,dtype=np.uint)\\nX = np.append(np.ones( len (X)).reshape(-1,1),X,axis=1)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 230, 'page_label': '213'}, page_content='y = np.array(data.target==1,dtype=np.uint)\\nX = np.append(np.ones( len (X)).reshape(-1,1),X,axis=1)\\nThe model matrix has three features, including the constant feature. Instead of using\\nNewton’s method (5.39) to estimate β, implement a simple gradient descent procedure\\nβt = βt−1 −α∇rτ(βt−1),\\nwith learning rate α= 0.0001, and run it for 106 steps. Your procedure should deliver three\\ncoefficients; one for the intercept and the rest for the explanatory variables. Solve the same\\nproblem using the Logit method of statsmodels.api and compare the results.\\n18. Consider again Example 5.10, where we train the learner via the Newton iteration\\n(5.39). If X⊤:= [x1,..., xn] defines the matrix of predictors and µt := h(Xβt), then the ☞ 206\\ngradient (5.37) and Hessian (5.38) for Newton’s method can be written as:\\n∇rτ(βt) = 1\\nnX⊤(µt −y) and H(βt) = 1\\nnX⊤DtX,\\nwhere Dt := diag(µt ⊙(1 −µt)) is a diagonal matrix. Show that the Newton iteration (5.39)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 230, 'page_label': '213'}, page_content='nX⊤DtX,\\nwhere Dt := diag(µt ⊙(1 −µt)) is a diagonal matrix. Show that the Newton iteration (5.39)\\ncan be written as the iterative reweighted least-squares iterative\\nreweighted\\nleast squares\\nmethod:\\nβt = argmin\\nβ\\n(eyt−1 −Xβ)⊤Dt−1(eyt−1 −Xβ),\\nwhere eyt−1 := Xβt−1 + D−1\\nt−1(y −µt−1) is the so-called adjusted response. [Hint: use the fact\\nthat (M⊤M)−1M⊤z is the minimizer of ∥Mβ−z∥2.]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 231, 'page_label': '214'}, page_content='214 Exercises\\n19. In multi-output linear regressionmulti-output\\nlinear\\nregression\\n, the response variable is a real-valued vector of di-\\nmension, say, m. Similar to (5.8), the model can be written in matrix notation:\\nY = XB +\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nε⊤\\n1\\n...\\nε⊤\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\nwhere:\\n• Y is an n ×m matrix of n independent responses (stored as row vectors of length m);\\n• X is the usual n ×p model matrix;\\n• B is an p ×m matrix of model parameters;\\n• ε1,..., εn ∈Rm are independent error terms with Eε= 0 and Eεε⊤= Σ.\\nWe wish to learn the matrix parameters B and Σ from the training set {Y,X}. To this end,\\nconsider minimizing the training loss:\\n1\\nntr\\n\\x10\\n(Y −XB) Σ−1 (Y −XB)⊤\\x11\\n,\\nwhere tr(·) is the trace of a matrix.☞357\\n(a) Show that the minimizer of the training loss, denoted bB, satisfies the normal equa-\\ntions:\\nX⊤X bB = X⊤Y.\\n(b) Noting that\\n(Y −XB)⊤(Y −XB) =\\nnX\\ni=1\\nεiε⊤\\ni ,\\nexplain why\\nbΣ := (Y −XbB)⊤(Y −XbB)\\nn\\nis a method-of-moments estimator of Σ, just like the one given in (5.10).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 232, 'page_label': '215'}, page_content='CHAPTER 6\\nREGULARIZATION AND KERNEL\\nMETHODS\\nThe purpose of this chapter is to familiarize the reader with two central concepts\\nin modern data science and machine learning: regularization and kernel methods. Reg-\\nularization provides a natural way to guard against overfitting and kernel methods of-\\nfer a broad generalization of linear models. Here, we discuss regularized regression\\n(ridge, lasso) as a bridge to the fundamentals of kernel methods. We introduce repro-\\nducing kernel Hilbert spaces and show that selecting the best prediction function in\\nsuch spaces is in fact a finite-dimensional optimization problem. Applications to spline\\nfitting, Gaussian process regression, and kernel PCA are given.\\n6.1 Introduction\\nIn this chapter we return to the supervised learning setting of Chapter 5 (regression) and ex-\\npand its scope. Given training data τ= {(x1,y1),..., (xn,yn)}, we wish to find a prediction\\nfunction (the learner) gτ that minimizes the (squared-error) training loss\\nℓτ(g) = 1\\nn'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 232, 'page_label': '215'}, page_content='function (the learner) gτ that minimizes the (squared-error) training loss\\nℓτ(g) = 1\\nn\\nnX\\ni=1\\n(yi −g(xi))2\\nwithin a class of functions G. As noted in Chapter 2, ifGis the set of all possible functions\\nthen choosing any function g with the property thatg(xi) = yi for all i will give zero training\\nloss, but will likely have poor generalization performance (that is, suffer from overfitting).\\nRecall from Theorem 2.1 that the best possible prediction function (over all g) for ☞ 21\\nthe squared-error risk E(Y −g(X))2 is given by g∗(x) = E[Y |X = x]. The class Gshould\\nbe simple enough to permit theoretical understanding and analysis but, at the same time,\\nrich enough to contain the optimal function g∗ (or a function close to g∗). This ideal can\\nbe realized by taking Gto be a Hilbert space Hilbert space(i.e., a complete inner product space) of\\nfunctions; see Appendix A.7. ☞ 384\\nMany of the classes of functions that we have encountered so far are in fact Hilbert'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 232, 'page_label': '215'}, page_content='Many of the classes of functions that we have encountered so far are in fact Hilbert\\nspaces. In particular, the set Gof linear functions on Rp is a Hilbert space. To see this,\\n215'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 233, 'page_label': '216'}, page_content='216 Regularization\\nidentify with each element β ∈Rp the linear function gβ : x 7→x⊤βand define the inner\\nproduct on Gas ⟨gβ,gγ⟩:= β⊤γ. In this way, Gbehaves in exactly the same way as (is\\nisomorphic to) the space Rp equipped with the Euclidean inner product (dot product). The☞360\\nlatter is a Hilbert space, because it is completecomplete\\nvector space\\nwith respect to the Euclidean norm. See\\nExercise 12 for a further discussion.\\nLet us now turn to our “running” polynomial regression Example 2.1, where the feature☞26\\nvector x = [1,u,u2,..., up−1]⊤ =: ϕ(u) is itself a vector-valued function of another feature\\nu. Then, the space of functions hβ : u 7→ϕ(u)⊤βis a Hilbert space, through the identifica-\\ntion hβ ≡β. In fact, this is true for any feature mapping ϕ: u 7→[ϕ1(u),...,ϕ p(u)]⊤.\\nThis can be further generalized by considering feature maps u 7→κu, where each κufeature maps\\nis a real-valued function v 7→κu(v) on the feature space. As we shall soon see (in Sec-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 233, 'page_label': '216'}, page_content='is a real-valued function v 7→κu(v) on the feature space. As we shall soon see (in Sec-\\ntion 6.3), functions of the form u 7→P∞\\ni=1 βiκvi (u) live in a Hilbert space of functions called\\na reproducing kernel Hilbert space (RKHS).RKHS In Section 6.3 we introduce the notion of a\\nRKHS formally, give specific examples, including the linear and Gaussian kernels, and de-\\nrive various useful properties, the most important of which is the representer Theorem 6.6.\\nApplications of such spaces include the smoothing splines (Section 6.6), Gaussian pro-☞235\\ncess regression (Section 6.7), kernel PCA (Section 6.8), and support vector machines for\\nclassification (Section 7.7).☞269\\nThe RKHS formalism also makes it easier to treat the important topic ofregularization.regularization\\nThe aim of regularization is to improve the predictive performance of the best learner in\\nsome class of functions Gby adding a penalty term to the training loss that penalizes'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 233, 'page_label': '216'}, page_content='some class of functions Gby adding a penalty term to the training loss that penalizes\\nlearners that tend to overfit the data. In the next section we introduce the main ideas behind\\nregularization, which then segues into a discussion of kernel methods in the subsequent\\nsections.\\n6.2 Regularization\\nLet Gbe the Hilbert space of functions over which we search for the minimizer, gτ, of the\\ntraining loss ℓτ(g). Often, the Hilbert space Gis rich enough so that we can find a learner\\ngτ within Gsuch that the training loss is zero or close to zero. Consequently, if the space of\\nfunctions Gis sufficiently rich, we run the risk of overfitting. One way to avoid overfitting\\nis to restrict attention to a subset of the space Gby introducing a non-negative functional\\nJ : G→ R+ which penalizes complex models (functions). In particular, we want to find\\nfunctions g ∈G such that J(g) <c for some “regularization” constant c >0. Thus we can\\nformulate the quintessential supervised learning problem as:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 233, 'page_label': '216'}, page_content='formulate the quintessential supervised learning problem as:\\nmin {ℓτ(g) : g ∈G,J(g) <c}, (6.1)\\nthe solution (argmin) of which is our learner. When this optimization problem is convex, it\\ncan be solved by first obtaining the Lagrangian dual function\\nL∗(λ) := min\\ng∈G\\n{ℓτ(g) + λ(J(g) −c)},\\nand then maximizing L∗(λ) with respect to λ⩾0; see Section B.2.3.☞407\\nIn order to introduce the overall ideas of kernel methods and regularization, we will\\nproceed by exploring (6.1) in the special case of ridge regressionridge\\nregression\\n, with the following run-\\nning example.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 234, 'page_label': '217'}, page_content='Regularization and Kernel Methods 217\\nExample 6.1 (Ridge Regression) Ridge regression is simply linear regression with a\\nsquared-norm penalty functional (also called a regularization function, or regularizer regularizer).\\nSuppose we have a training set τ= {(xi,yi),i = 1,..., n}, with each xi ∈Rp and we use a\\nsquared-norm penalty with regularization parameter regularization\\nparameter\\nγ> 0. Then, the problem is to solve\\nmin\\ng∈G\\n1\\nn\\nnX\\ni=1\\n(yi −g(xi))2 + γ∥g∥2, (6.2)\\nwhere Gis the Hilbert space of linear functions on Rp. As explained in Section 6.1, we\\ncan identify each g ∈G with a vector β∈Rp and, consequently, ∥g∥2 = ⟨β,β⟩= ∥β∥2. The\\nabove functional optimization problem is thus equivalent to the parametric optimization\\nproblem\\nmin\\nβ∈Rp\\n1\\nn\\nnX\\ni=1\\n\\x00yi −x⊤\\ni β\\x012 + γ∥β∥2, (6.3)\\nwhich, in the notation of Chapter 5, further simplifies to\\nmin\\nβ∈Rp\\n1\\nn ∥y −Xβ∥2 + γ∥β∥2. (6.4)\\nIn other words, the solution to (6.2) is of the form x 7→x⊤β∗, where β∗ solves (6.3) (or'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 234, 'page_label': '217'}, page_content='In other words, the solution to (6.2) is of the form x 7→x⊤β∗, where β∗ solves (6.3) (or\\nequivalently (6.4)). Observe that asγ→∞, the regularization term becomes dominant and\\nconsequently the optimal g becomes identically zero.\\nThe optimization problem in (6.4) is convex, and by multiplying by the constant n/2\\nand setting the gradient equal to zero, we obtain\\nX⊤(Xβ−y) + n γβ= 0. (6.5)\\nIf γ = 0 these are simply the normal equations, albeit written in a slightly di fferent form. ☞ 28\\nIf the matrix X⊤X + n γIp is invertible (which is the case for any γ >0; see Exercise 13),\\nthen the solution to these modified normal equations is\\nbβ= (X⊤X + n γIp)−1X⊤y.\\nWhen using regularization with respect to some Hilbert spaceG, it is sometimes useful\\nto decompose Ginto two orthogonal subspaces, Hand Csay, such that every g ∈G can\\nbe uniquely written as g = h + c, with h ∈H, c ∈C, and ⟨h,c⟩= 0. Such a Gis said to be'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 234, 'page_label': '217'}, page_content='be uniquely written as g = h + c, with h ∈H, c ∈C, and ⟨h,c⟩= 0. Such a Gis said to be\\nthe direct sum direct sumof Cand H, and we write G= H⊕C. Decompositions of this form become\\nuseful when functions in Hare penalized but functions in Care not. We illustrate this\\ndecomposition with the ridge regression example where one of the features is a constant\\nterm, which we do not wish to penalize.\\nExample 6.2 (Ridge Regression (cont.)) Suppose one of the features in Example 6.1\\nis the constant 1, which we do not wish to penalize. The reason for this is to ensure that\\nwhen γ → ∞, the optimal g becomes the “constant” model, g(x) = β0, rather than the\\n“zero” model, g(x) = 0. Let us alter the notation slightly by considering the feature vectors\\nto be of the formex = [1,x⊤]⊤, where x = [x1,..., xp]⊤. We thus have p + 1 features, rather'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 235, 'page_label': '218'}, page_content='218 Regularization\\nthan p. Let Gbe the space of linear functions of ex. Each linear function g of ex can be\\nwritten as g : ex 7→β0 + x⊤β, which is the sum of the constant function c : ex 7→β0 and\\nh : ex 7→x⊤β. Moreover, the two functions are orthogonal with respect to the inner product\\non G: ⟨c,h⟩= [β0,0⊤][0,β⊤]⊤= 0, where 0 is a column vector of zeros.\\nAs subspaces of G, both Cand Hare again Hilbert spaces, and their inner products and\\nnorms follow directly from the inner product on G. For example, each function h : ex 7→\\nx⊤βin Hhas norm ∥h∥H = ∥β∥, and the constant function c : ex 7→β0 in Chas norm |β0|.\\nThe modification of the regularized optimization problem (6.2) where the constant term\\nis not penalized can now be written as\\nmin\\ng∈H⊕C\\n1\\nn\\nnX\\ni=1\\n(yi −g(exi))2 + γ∥g∥2\\nH, (6.6)\\nwhich further simplifies to\\nmin\\nβ0,β\\n1\\nn ∥y −β01 −Xβ∥2 + γ∥β∥2, (6.7)\\nwhere 1 is the n×1 vector of 1s. Observe that, in this case, asγ→∞ the optimal g tends to'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 235, 'page_label': '218'}, page_content='where 1 is the n×1 vector of 1s. Observe that, in this case, asγ→∞ the optimal g tends to\\nthe sample mean y of the {yi}; that is, we obtain the “default” regression model, without ex-\\nplanatory variables. Again, this is a convex optimization problem, and the solution follows\\nfrom\\nX⊤(β01 + Xβ−y) + n γβ= 0, (6.8)\\nwith\\nn β0 = 1⊤(y −Xβ). (6.9)\\nThis results in solving for βfrom\\n(X⊤X −n−1X⊤11⊤X + n γIp)β= (X⊤−n−1X⊤11⊤)y, (6.10)\\nand determining β0 from (6.9).\\nAs a precursor to the kernel methods in the following sections, let us assume thatn ⩾p\\nand that X has full (column) rank p. Then any vector β∈Rp can be written as a linear\\ncombination of the feature vectors {xi}; that is, as linear combinations of the columns of\\nthe matrix X⊤. In particular, letβ= X⊤α, where α= [α1,...,α n]⊤∈Rn. In this case (6.10)\\nreduces to\\n(XX⊤−n−111⊤XX⊤+ n γIn)α= (In −n−111⊤)y.\\nAssuming invertibility of (XX⊤−n−111⊤XX⊤+ n γIn), we have the solution\\nbα= (XX⊤−n−111⊤XX⊤+ n γIn)−1(In −n−111⊤)y,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 235, 'page_label': '218'}, page_content='bα= (XX⊤−n−111⊤XX⊤+ n γIn)−1(In −n−111⊤)y,\\nwhich depends on the training feature vectors {xi}only through the n ×n matrix of inner\\nproducts: XX⊤ = [⟨xi,xj⟩]. This matrix is called the Gram matrixGram matrix of the {xi}. From (6.9),\\nthe solution for the constant term is bβ0 = n−11⊤(y −XX⊤bα). It follows that the learner is a\\nlinear combination of inner products {⟨xi,x⟩}plus a constant:\\ngτ(ex) = bβ0 + x⊤X⊤bα= bβ0 +\\nnX\\ni=1\\nbαi ⟨xi,x⟩,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 236, 'page_label': '219'}, page_content='Regularization and Kernel Methods 219\\nwhere the coefficients bβ0 and bαi only depend on the inner products {⟨xi,xj⟩}. We will see\\nshortly that the representer Theorem 6.6 generalizes this result to a broad class of regular- ☞ 231\\nized optimization problems.\\nWe illustrate in Figure 6.1 how the solutions of the ridge regression problems appearing\\nin Examples 6.1 and 6.2 are qualitatively a ffected by the regularization parameter γfor a\\nsimple linear regression model. The data was generated from the modelyi = −1.5 +0.5xi +\\nεi, i = 1,..., 100, where each xi is drawn independently and uniformly from the interval\\n[0,10] and each εi is drawn independently from the standard normal distribution.\\n. = 0:1\\n-2\\n-1\\n0\\n1\\n2\\n-1\\n. = 1 . = 10\\n-2 0 2\\n-0\\n-2\\n-1\\n0\\n1\\n2\\n-1\\n-2 0 2\\n-0\\n-2 0 2\\n-0\\nFigure 6.1: Ridge regression solutions for a simple linear regression problem. Each panel\\nshows contours of the loss function (log scale) and the effect of the regularization parameter'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 236, 'page_label': '219'}, page_content='shows contours of the loss function (log scale) and the effect of the regularization parameter\\nγ ∈{0.1,1,10}, appearing in (6.4) and (6.7). Top row: both terms are penalized. Bottom\\nrow: only the non-constant term is penalized. Penalized (plus) and unpenalized (diamond)\\nsolutions are shown in each case.\\nThe contours are those of the squared-error loss (actually the logarithm thereof), which\\nis minimized with respect to the model parameters β0 and β1. The diamonds all repres-\\nent the same minimizer of this loss. The plusses show each minimizer [ β∗\\n0,β∗\\n1]⊤ of the\\nregularized minimization problems (6.4) and (6.7) for three choices of the regularization\\nparameter γ. For the top three panels the regularization involves both β0 and β1, through\\nthe squared norm β2\\n0 + β2\\n1. The circles show the points that have the same squared norm as'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 237, 'page_label': '220'}, page_content='220 Regularization\\nthe optimal solution. For the bottom three panels only β1 is regularized; there, horizontal\\nlines indicate vectors [β0,β1]⊤for which |β1|= |β∗\\n1|.\\nThe problem of ridge regression discussed in Example 6.2 boils down to solving a\\nproblem of the form in (6.7), involving a squared 2-norm penalty ∥β∥2. A natural ques-\\ntion to ask is whether we can replace the squared 2-norm penalty by a di fferent penalty\\nterm. Replacing it with a 1-norm gives the lasso (least absolute shrinkage and selection☞408\\nlasso operator). The lasso equivalent of the ridge regression problem (6.7) is thus:\\nmin\\nβ0,β\\n1\\nn ∥y −β01 −Xβ∥2 + γ∥β∥1, (6.11)\\nwhere ∥β∥1 = Pp\\ni=1 |βi|.\\nThis is again a convex optimization problem. Unlike ridge regression, the lasso gener-\\nally does not have an explicit solution, and so numerical methods must be used to solve it.\\nNote that the problem (6.11) is of the form\\nmin\\nx,z\\nf (x) + g(z)\\nsubject to Ax + Bz = c,\\n(6.12)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 237, 'page_label': '220'}, page_content='Note that the problem (6.11) is of the form\\nmin\\nx,z\\nf (x) + g(z)\\nsubject to Ax + Bz = c,\\n(6.12)\\nwith x := [β0,β⊤]⊤, z := β, A := [0p,Ip], B := −Ip, and c := 0p (vector of zeros), and\\nconvex functions f (x) := 1\\nn ∥y −[1n,X] x ∥2 and g(z) := γ∥z∥1. There exist e fficient al-\\ngorithms for solving such problems, including the alternating direction method of mul-\\ntipliers (ADMM) [17]. We refer to Example B.11 for details on this algorithm.☞416\\nWe repeat the examples from Figure 6.1, but now using lasso regression and taking\\nthe square roots of the previous regularization parameters. The results are displayed in\\nFigure 6.2.\\n. =\\np\\n0:1\\n-2\\n-1\\n0\\n1\\n2\\n-1\\n. = 1 . =\\np\\n10\\n-2 0 2\\n-0\\n-2\\n-1\\n0\\n1\\n2\\n-1\\n-2 0 2\\n-0\\n-2 0 2\\n-0\\nFigure 6.2: Lasso regression solutions. Compare with Figure 6.1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 238, 'page_label': '221'}, page_content='Regularization and Kernel Methods 221\\nOne advantage of using the lasso regularization is that the resulting optimal parameter\\nvector often has several components that are exactly 0. For example, in the top middle\\nand right panels of Figure 6.2, the optimal solution lies exactly at a corner point of the\\nsquare {[β0,β1]⊤ : |β0|+ |β1|= |β∗\\n0|+ |β∗\\n1|}; in this case β∗\\n0 = 0. For statistical models with\\nmany parameters, the lasso can provide a methodology for model selection. Namely, as the\\nregularization parameter increases (or, equivalently, as theL1 norm of the optimal solution\\ndecreases), the solution vector will have fewer and fewer non-zero parameters. By plotting\\nthe values of the parameters for eachγor L1 one obtains the so-called regularization paths regularization\\npaths(also called homotopy paths or coefficient profiles) for the variables. Inspection of such\\npaths may help assess which of the model parameters are relevant to explain the variability\\nin the observed responses {yi}.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 238, 'page_label': '221'}, page_content='in the observed responses {yi}.\\nExample 6.3 (Regularization Paths) Figure 6.3 shows the regularization paths forp =\\n60 coefficients from a multiple linear regression model ☞ 169\\nYi =\\n60X\\nj=1\\nβj xi j + εi, i = 1,..., 150,\\nwhere βj = 1 for j = 1,..., 10 and βj = 0 for j = 11,..., 60. The error terms {εi}are inde-\\npendent and standard normal. The explanatory variables{xi j}were independently generated\\nfrom a standard normal distribution. As it is clear from the figure, the estimates of the 10\\nnon-zero coefficients are first selected, as the L1 norm of the solutions increases. By the\\ntime the L1 norm reaches around 4, all 10 variables for which βj = 1 have been correctly\\nidentified and the remaining 50 parameters are estimated as exactly 0. Only after the L1\\nnorm reaches around 8, will these “spurious” parameters be estimated to be non-zero. For\\nthis example, the regularization parameter γvaried from 10−4 to 10.\\n0 5 10 15\\nL1 norm\\n-0.5\\n0\\n0.5\\n1\\n1.5\\nb-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 238, 'page_label': '221'}, page_content='0 5 10 15\\nL1 norm\\n-0.5\\n0\\n0.5\\n1\\n1.5\\nb-\\nFigure 6.3: Regularization paths for lasso regression solutions as a function of theL1 norm\\nof the solutions.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 239, 'page_label': '222'}, page_content='222 Reproducing Kernel Hilbert Spaces\\n6.3 Reproducing Kernel Hilbert Spaces\\nIn this section, we formalize the idea outlined at the end of Section 6.1 of extending finite\\ndimensional feature maps to those that are functions by introducing a special type of Hil-\\nbert space of functions known as a reproducing kernel Hilbert space (RKHS). Although\\nthe theory extends naturally to Hilbert spaces of complex-valued functions, we restrict\\nattention to Hilbert spaces of real-valued functions here.\\nTo evaluate the loss of a learnerg in some class of functionsG, we do not need to expli-\\ncitly construct g — rather, it is only required that we can evaluateg at all the feature vectors\\nx1,..., xn of the training set. A defining property of an RKHS is that function evaluation\\nat a point x can be performed by simply taking the inner product of g with some feature\\nfunction κx associated with x. We will see that this property becomes particularly useful'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 239, 'page_label': '222'}, page_content='function κx associated with x. We will see that this property becomes particularly useful\\nin light of the representer theorem (see Section 6.5), which states that the learner g itself☞230\\ncan be represented as a linear combination of the set of feature functions{κxi ,i = 1,..., n}.\\nConsequently, we can evaluate a learner g at the feature vectors {xi}by taking linear com-\\nbinations of terms of the form κ(xi,xj) = ⟨κxi ,κxj ⟩G. Collecting these inner products into\\na matrix K = [κ(xi,xj),i, j = 1,..., n] (the Gram matrix of the {κxi }), we will see that the\\nfeature vectors {xi}only enter the loss minimization problem through K.\\nDefinition 6.1: Reproducing Kernel Hilbert Space\\nFor a non-empty set X, a Hilbert space Gof functions g : X→ Rwith inner product\\n⟨·,·⟩Gis called a reproducing kernel Hilbert spacereproducing\\nkernel Hilbert\\nspace\\n(RKHS) with reproducing kernel\\nκ: X×X→ Rif:\\n1. for every x ∈X, κx := κ(x,·) is in G,\\n2. κ(x,x) <∞for all x ∈X,\\n3. for every x ∈X and g ∈G, g(x) = ⟨g,κx⟩G.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 239, 'page_label': '222'}, page_content='2. κ(x,x) <∞for all x ∈X,\\n3. for every x ∈X and g ∈G, g(x) = ⟨g,κx⟩G.\\nThe reproducing kernel of a Hilbert space of functions, if it exists, is unique; see Exer-\\ncise 2. The main (third) condition in Definition 6.1 is known as the reproducing propertyreproducing\\nproperty\\n.\\nThis property allows us to evaluate any functiong ∈G at a point x ∈X by taking the inner\\nproduct of g and κx; as such, κx is called the representer of evaluation. Further, by taking\\ng = κx′ and applying the reproducing property, we have⟨κx′,κx⟩G= κ(x′,x), and so by sym-\\nmetry of the inner product it follows thatκ(x,x′) = κ(x′,x). As a consequence, reproducing\\nkernels are necessarily symmetric functions. Moreover, a reproducing kernelκis a positive\\nsemidefinitepositive\\nsemidefinite\\nfunction, meaning that for every n ⩾1 and every choice of α1,...,α n ∈Rand\\nx1,..., xn ∈X, it holds that\\nnX\\ni=1\\nnX\\nj=1\\nαi κ(xi,xj) αj ⩾0. (6.13)\\nIn other words, every Gram matrix K associated with κ is a positive semidefinite matrix;'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 239, 'page_label': '222'}, page_content='In other words, every Gram matrix K associated with κ is a positive semidefinite matrix;\\nthat is α⊤Kα⩾0 for all α. The proof is addressed in Exercise 1.\\nThe following theorem gives an alternative characterization of an RKHS. The proof\\nuses the Riesz representation Theorem A.17. Also note that in the theorem below we could☞390'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 240, 'page_label': '223'}, page_content='Regularization and Kernel Methods 223\\nhave replaced the word “bounded” with “continuous”, as the two are equivalent for linear\\nfunctionals; see Theorem A.16.\\nTheorem 6.1: Continuous Evaluation Functionals Characterize a RKHS\\nAn RKHS Gon a set Xis a Hilbert space in which every evaluation functional evaluation\\nfunctionalδx : g 7→g(x) is bounded. Conversely, a Hilbert space Gof functions X→ Rfor\\nwhich every evaluation functional is bounded is an RKHS.\\nProof: Note that, since evaluation functionals δx are linear operators, showing bounded-\\nness is equivalent to showing continuity. Given an RKHS with reproducing kernel κ, sup-\\npose that we have a sequence gn ∈G converging to g ∈G, that is ∥gn −g∥G→0. We apply\\nthe Cauchy–Schwarz inequality (Theorem A.15) and the reproducing property of κto find ☞ 389\\nthat for every x ∈X and any n:\\n|δxgn −δxg|= |gn(x) −g(x)|= |⟨gn −g,κx⟩G|⩽∥gn −g∥G∥κx∥G= ∥gn −g∥G\\np\\n⟨κx,κx⟩G\\n= ∥gn −g∥G\\np\\nκ(x,x).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 240, 'page_label': '223'}, page_content='|δxgn −δxg|= |gn(x) −g(x)|= |⟨gn −g,κx⟩G|⩽∥gn −g∥G∥κx∥G= ∥gn −g∥G\\np\\n⟨κx,κx⟩G\\n= ∥gn −g∥G\\np\\nκ(x,x).\\nNoting that √κ(x,x) <∞by definition for every x ∈X, and that ∥gn −g∥G→0 as n →∞,\\nwe have shown continuity of δx, that is |δxgn −δxg|→ 0 as n →∞ for every x ∈X.\\nConversely, suppose that evaluation functionals are bounded. Then from the Riesz\\nrepresentation Theorem A.17, there exists some gδx ∈G such that δxg = ⟨g,gδx ⟩G for all\\ng ∈G — the representer of evaluation. If we defineκ(x,x′) = gδx (x′) for all x,x′∈X, then\\nκx := κ(x,·) = gδx is an element of Gfor every x ∈X and ⟨g,κx⟩G= δxg = g(x), so that the\\nreproducing property in Definition 6.1 is verified. □\\nThe fact that an RKHS has continuous evaluation functionals means that if two func-\\ntions g,h ∈G are “close” with respect to ∥·∥ G, then their evaluations g(x),h(x) are close\\nfor every x ∈X. Formally, convergence in ∥·∥ G norm implies pointwise convergence for\\nall x ∈X.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 240, 'page_label': '223'}, page_content='for every x ∈X. Formally, convergence in ∥·∥ G norm implies pointwise convergence for\\nall x ∈X.\\nThe following theorem shows that any finite function κ : X×X→ Rcan serve as a\\nreproducing kernel as long as it is finite, symmetric, and positive semidefinite. The cor-\\nresponding (unique!) RKHS Gis the completion of the set of all functions of the formPn\\ni=1 αi κxi where αi ∈Rfor all i = 1,..., n.\\nTheorem 6.2: Moore–Aronszajn\\nGiven a non-empty set Xand any finite symmetric positive semidefinite function\\nκ : X×X→ R, there exists an RKHS Gof functions g : X→ Rwith reproducing\\nkernel κ. Moreover, Gis unique.\\nProof: (Sketch) As the proof of uniqueness is treated in Exercise 2, the objective is to\\nprove existence. The idea is to construct a pre-RKHSG0 from the given function κthat has\\nthe essential structure and then to extend G0 to an RKHS G.\\nIn particular, define G0 as the set of finite linear combinations of functions κx, x ∈X:\\nG0 :=\\n\\x1a\\ng =\\nnX\\ni=1\\nαi κxi\\n\\x0c\\x0c\\x0c\\x0c\\x0c x1,..., xn ∈X, αi ∈R, n ∈N\\n\\x1b\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 241, 'page_label': '224'}, page_content='224 Construction of Reproducing Kernels\\nDefine on G0 the following inner product:\\n⟨f,g⟩G0 :=\\n* nX\\ni=1\\nαi κxi ,\\nmX\\nj=1\\nβj κx′\\nj\\n+\\nG0\\n:=\\nnX\\ni=1\\nmX\\nj=1\\nαi βj κ(xi,x′\\nj).\\nThen G0 is an inner product space. In fact,G0 has the essential structure we require, namely\\nthat (i) evaluation functionals are bounded /continuous (Exercise 4) and (ii) Cauchy se-\\nquences in G0 that converge pointwise also converge in norm (see Exercise 5).\\nWe then enlarge G0 to the set Gof all functions g : X→ Rfor which there exists a\\nCauchy sequence in G0 converging pointwise to g and define an inner product on Gas the\\nlimit\\n⟨f,g⟩G:= lim\\nn→∞\\n⟨fn,gn⟩G0 , (6.14)\\nwhere fn →f and gn →g. To show thatGis an RKHS it remains to be shown that (1) this\\ninner product is well defined; (2) evaluation functionals remain bounded; and (3) the space\\nGis complete. A detailed proof is established in Exercises 6 and 7. □\\n6.4 Construction of Reproducing Kernels'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 241, 'page_label': '224'}, page_content='6.4 Construction of Reproducing Kernels\\nIn this section we describe various ways to construct a reproducing kernel κ : X×X→\\nR for some feature space X. Recall that κ needs to be a finite, symmetric, and positive\\nsemidefinite function (that is, it satisfies (6.13)). In view of Theorem 6.2, specifying the\\nspace Xand a reproducing kernel κ : X×X→ Rcorresponds to uniquely specifying an\\nRKHS.\\n6.4.1 Reproducing Kernels via Feature Mapping\\nPerhaps the most fundamental way to construct a reproducing kernel κ is via a feature\\nmap ϕ : X→ Rp. We define κ(x,x′) : = ⟨ϕ(x),ϕ(x′)⟩, where ⟨, ⟩denotes the Euclidean\\ninner product. The function is clearly finite and symmetric. To verify that κ is positive\\nsemidefinite, let Φ be the matrix with rows ϕ(x1)⊤,..., ϕ(xn)⊤and let α= [α1,...,α n]⊤ ∈\\nRn. Then,\\nnX\\ni=1\\nnX\\nj=1\\nαi κ(xi,xj) αj =\\nnX\\ni=1\\nnX\\nj=1\\nαi ϕ⊤(xi) ϕ(xj) αj = α⊤ΦΦ⊤α= ∥Φ⊤α∥2 ⩾0.\\nExample 6.4 (Linear Kernel) Taking the identity feature map ϕ(x) = x on X= Rp,\\ngives the linear kernellinear kernel'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 241, 'page_label': '224'}, page_content='gives the linear kernellinear kernel\\nκ(x,x′) = ⟨x,x′⟩= x⊤x′.\\nAs can be seen from the proof of Theorem 6.2, the RKHS of functions corresponding to\\nthe linear kernel is the space of linear functions on Rp. This space is isomorphic to Rp\\nitself, as discussed in the introduction (see also Exercise 12).\\nIt is natural to wonder whether a given kernel function corresponds uniquely to a feature\\nmap. The answer is no, as we shall see by way of example.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 242, 'page_label': '225'}, page_content='Regularization and Kernel Methods 225\\nExample 6.5 (Feature Maps and Kernel Functions) Let X= Rand consider feature\\nmaps ϕ1 : X→ Rand ϕ2 : X→ R2, with ϕ1(x) := x and ϕ2(x) := [x,x]⊤/\\n√\\n2. Then\\nκϕ1 (x,x′) = ⟨ϕ1(x),ϕ1(x′)⟩= xx′,\\nbut also\\nκϕ2 (x,x′) = ⟨ϕ2(x),ϕ2(x′)⟩= xx′.\\nThus, we arrive at the same kernel function defined for the same underlying set Xvia two\\ndifferent feature maps.\\n6.4.2 Kernels from Characteristic Functions\\nAnother way to construct reproducing kernels on X= Rp makes use of the properties of\\ncharacteristic functions. In particular, we have the following result. We leave its proof as ☞ 441\\nExercise 10.\\nTheorem 6.3: Reproducing Kernel from a Characteristic Function\\nLet X ∼µ be an Rp-valued random vector that is symmetric about the origin (that\\nis, X and −X are identically distributed), and let ψ be its characteristic function:\\nψ(t) = Eeit⊤X =\\nR\\neit⊤x µ(dx) for t ∈Rp. Then κ(x,x′) := ψ(x −x′) is a valid repro-\\nducing kernel on Rp.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 242, 'page_label': '225'}, page_content='R\\neit⊤x µ(dx) for t ∈Rp. Then κ(x,x′) := ψ(x −x′) is a valid repro-\\nducing kernel on Rp.\\nExample 6.6 (Gaussian Kernel) The multivariate normal distribution with mean vec-\\ntor 0 and covariance matrix b2 Ip is clearly symmetric around the origin. Its characteristic\\nfunction is\\nψ(t) = exp\\n \\n−1\\n2b2 ∥t∥2\\n!\\n, t ∈Rp.\\nTaking b2 = 1/σ2, this gives the popular Gaussian kernel Gaussian\\nkernel\\non Rp:\\nκ(x,x′) = exp\\n \\n−1\\n2\\n∥x −x′∥2\\nσ2\\n!\\n. (6.15)\\nThe parameter σ is sometimes called the bandwidth bandwidth. Note that in the machine learning\\nliterature, the Gaussian kernel is sometimes referred to as “the” radial basis function (rbf)\\nkernel radial basis\\nfunction (rbf)\\nkernel\\n.1\\nFrom the proof of Theorem 6.2, we see that the RKHS Gdetermined by the Gaussian\\nkernel κis the space of pointwise limits of functions of the form\\ng(x) =\\nnX\\ni=1\\nαi exp\\n \\n−1\\n2\\n∥x −xi∥2\\nσ2\\n!\\n.\\nWe can think of each pointxi having a feature κxi that is a scaled multivariate Gaussian pdf\\ncentered at xi.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 242, 'page_label': '225'}, page_content='centered at xi.\\n1The term radial basis function is sometimes used more generally to mean kernels of the formκ(x,x′) =\\nf (∥x −x′∥) for some function f : R→R.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 243, 'page_label': '226'}, page_content='226 Construction of Reproducing Kernels\\nExample 6.7 (Sinc Kernel) The characteristic function of a Uniform[−1,1] random\\nvariable (which is symmetric around 0) isψ(t) = sinc(t) := sin(t)/t, so κ(x,x′) = sinc(x−x′)\\nis a valid kernel.\\nInspired by kernel density estimation (Section 4.4), we may be tempted to use the pdf☞131\\nof a random variable that is symmetric about the origin to construct a reproducing kernel.\\nHowever, doing so will not work in general, as the next example illustrates.\\nExample 6.8 (Uniform pdf Does not Construct a Valid Reproducing Kernel) Take\\nthe function ψ(t) = 1\\n2 1{|t|⩽1}, which is the pdf of X ∼Uniform[−1,1]. Unfortunately, the\\nfunction κ(x,x′) = ψ(x −x′) is not positive semidefinite, as can be seen for example by\\nconstructing the matrix A = [κ(ti,tj),i, j = 1,2,3] for the points t1 = 0, t2 = 0.75, and\\nt3 = 1.5 as follows:\\nA =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nψ(0) ψ(−0.75) ψ(−1.5)\\nψ(0.75) ψ(0) ψ(−0.75)\\nψ(1.5) ψ(0.75) ψ(0)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n0.5 0 .5 0\\n0.5 0 .5 0 .5\\n0 0 .5 0 .5'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 243, 'page_label': '226'}, page_content='ψ(1.5) ψ(0.75) ψ(0)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n0.5 0 .5 0\\n0.5 0 .5 0 .5\\n0 0 .5 0 .5\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.\\nThe eigenvalues of A are {1/2 −√1/2,1/2,1/2 + √1/2}≈{− 0.2071,0.5,1.2071}and so\\nby Theorem A.9, A is not a positive semidefinite matrix, since it has a negative eigenvalue.☞367\\nConsequently, κis not a valid reproducing kernel.\\nOne of the reasons why the Gaussian kernel (6.15) is popular is that it enjoys the uni-\\nversal approximation propertyuniversal\\napproximation\\nproperty\\n[88]: the space of functions spanned by the Gaussian kernel\\nis dense in the space of continuous functions with support Z⊂ Rp. Naturally, this is a\\ndesirable property especially if there is little prior knowledge about the properties of g∗.\\nHowever, note that every function g in the RKHS Gassociated with a Gaussian kernel κis\\ninfinitely differentiable. Moreover, a Gaussian RKHS does not contain non-zero constant\\nfunctions. Indeed, if A ⊂Z is non-empty and open, then the only function of the form'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 243, 'page_label': '226'}, page_content='functions. Indeed, if A ⊂Z is non-empty and open, then the only function of the form\\ng(x) = c 1{x ∈A}contained in Gis the zero function (c = 0).\\nConsequently, if it is known that g is differentiable only to a certain order, one may\\nprefer the Matérn kernelMat´ern kernel with parameters ν,σ> 0:\\nκν(x,x′) = 21−ν\\nΓ(ν)\\n\\x10√\\n2ν∥x −x′∥/σ\\n\\x11ν\\nKν\\n\\x10√\\n2ν∥x −x′∥/σ\\n\\x11\\n, (6.16)\\nwhich gives functions that are (weakly) di fferentiable to order ⌊ν⌋(but not necessarily to\\norder ⌈ν⌉). Here, Kν denotes the modified Bessel function of the second kind; see (4.49).\\nThe particular form of the Matérn kernel appearing in (6.16) ensures that limν→∞κν(x,x′) =☞164\\nκ(x,x′), where κis the Gaussian kernel appearing in (6.15).\\nWe remark that Sobolev spaces are closely related to the Matérn kernel. Up to constants\\n(which scale the unit ball in the space), in dimension p and for a parameter s > p/2, these\\nspaces can be identified with ψ(t) = 21−s\\nΓ(s) ∥t∥s−p/2Kp/2−s(∥t∥), which in turn can be viewed as'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 243, 'page_label': '226'}, page_content='spaces can be identified with ψ(t) = 21−s\\nΓ(s) ∥t∥s−p/2Kp/2−s(∥t∥), which in turn can be viewed as\\nthe characteristic function corresponding to the (radially symmetric) multivariate Student’s\\nt distribution with s degrees of freedom: that is, with pdf f (x) ∝(1 + ∥x∥2)−s.☞162'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 244, 'page_label': '227'}, page_content='Regularization and Kernel Methods 227\\n6.4.3 Reproducing Kernels Using Orthonormal Features\\nWe have seen in Sections 6.4.1 and 6.4.2 how to construct reproducing kernels from feature\\nmaps and characteristic functions. Another way to construct kernels on a spaceXis to work\\ndirectly from the function class L2(X; µ); that is, the set of square-integrable 2 functions\\non Xwith respect to µ; see also Definition A.4. For simplicity, in what follows, we will ☞ 385\\nconsider µto be the Lebesgue measure, and will simply write L2(X) rather than L2(X; µ).\\nWe will also assume that X⊆ Rp.\\nLet {ξ1,ξ2,... }be an orthonormal basis of L2(X) and let c1,c2,... be a sequence of\\npositive numbers. As discussed in Section 6.4.1, the kernel corresponding to a feature map\\nϕ: X→ Rp is κ(x,x′) = ϕ(x)⊤ϕ(x′) = Pp\\ni=1 ϕi(x) ϕi(x′). Now consider a (possibly infinite)\\nsequence of feature functions ϕi = ci ξi,i = 1,2,... and define\\nκ(x,x′) :=\\nX\\ni⩾1\\nϕi(x) ϕi(x′) =\\nX\\ni⩾1\\nλi ξi(x) ξi(x′), (6.17)\\nwhere λi = c2'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 244, 'page_label': '227'}, page_content='κ(x,x′) :=\\nX\\ni⩾1\\nϕi(x) ϕi(x′) =\\nX\\ni⩾1\\nλi ξi(x) ξi(x′), (6.17)\\nwhere λi = c2\\ni ,i = 1,2,... . This is well-defined as long as P\\ni⩾1 λi < ∞, which we assume\\nfrom now on. Let Hbe the linear space of functions of the form f = P\\ni⩾1 αiξi, whereP\\ni⩾1 α2\\ni /λi <∞. As every function f ∈L2(X) can be represented as f = P\\ni⩾1⟨f,ξi⟩ξi, we\\nsee that His a linear subspace of L2(X). On Hdefine the inner product\\n⟨f,g⟩H :=\\nX\\ni⩾1\\n⟨f,ξi⟩⟨g,ξi⟩\\nλi\\n.\\nWith this inner product, the squared norm of f = P\\ni⩾1 αi ξi is ∥f ∥2\\nH = P\\ni⩾1 α2\\ni /λi < ∞.\\nWe show that His actually an RKHS with kernel κby verifying the conditions of Defini-\\ntion 6.1. First,\\nκx =\\nX\\ni⩾1\\nλi ξi(x) ξi ∈H,\\nas P\\ni λi < ∞by assumption, and so κ is finite. Second, the reproducing property holds.\\nNamely, let f = P\\ni⩾1 αi ξi. Then,\\n⟨κx, f ⟩H =\\nX\\ni⩾1\\n⟨κx,ξi⟩⟨f,ξi⟩\\nλi\\n=\\nX\\ni⩾1\\nλi ξi(x) αi\\nλi\\n=\\nX\\ni⩾1\\nαiξi(x) = f (x).\\nThe discussion above demonstrates that kernels can be constructed via (6.17). In fact,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 244, 'page_label': '227'}, page_content='The discussion above demonstrates that kernels can be constructed via (6.17). In fact,\\n(under mild conditions) any given reproducing kernel κcan be written in the form (6.17),\\nwhere this series representation enjoys desirable convergence properties. This result is\\nknown as Mercer’s theorem, and is given below. We leave the full proof including the\\nprecise conditions to, e.g., [40], but the main idea is that a reproducing kernel κ can be\\nthought of as a generalization of a positive semidefinite matrix K, and can also be writ-\\nten in spectral form (see also Section A.6.5). In particular, by Theorem A.9, we can write ☞ 367\\nK = VDV⊤, where V is a matrix of orthonormal eigenvectors [ vℓ] and D the diagonal\\nmatrix of the (positive) eigenvalues [λℓ]; that is,\\nK(i, j) =\\nX\\nℓ⩾1\\nλℓ vℓ(i) vℓ( j).\\n2A function f : X→ Ris said to be square-integrable if\\nR\\nf 2(x) µ(dx) <∞, where µis a measure on X.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 245, 'page_label': '228'}, page_content='228 Construction of Reproducing Kernels\\nIn (6.18) below, x,x′play the role of i, j, and ξℓ plays the role of vℓ.\\nTheorem 6.4: Mercer\\nLet κ : X×X → R be a reproducing kernel for a compact set X ⊂Rp. Then\\n(under mild conditions) there exists a countable sequence of non-negative numbers\\n{λℓ}decreasing to zero and functions {ξℓ}orthonormal in L2(X) such that\\nκ(x,x′) =\\nX\\nℓ⩾1\\nλℓ ξℓ(x) ξℓ(x′) , for all x,x′∈X, (6.18)\\nwhere (6.18) converges absolutely and uniformly on X×X.\\nFurther, if λℓ >0, then (λℓ,ξℓ) is an (eigenvalue, eigenfunction) pair for the integral\\noperator K : L2(X) →L2(X) defined by [K f](x) :=\\nR\\nXκ(x,y) f (y) dy for x ∈X.\\nTheorem 6.4 holds if (i) the kernel κis continuous on X×X , (ii) the function eκ(x) :=\\nκ(x,x) defined for x ∈X is integrable. Extensions of Theorem 6.4 to more general spaces\\nXand measures µhold; see, e.g., [115] or [40].\\nThe key importance of Theorem 6.4 lies in the fact that the series representation (6.18)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 245, 'page_label': '228'}, page_content='The key importance of Theorem 6.4 lies in the fact that the series representation (6.18)\\nconverges absolutely and uniformly onX×X. The uniform convergence is a much stronger\\ncondition than pointwise convergence, and means for instance that properties of the se-\\nquence of partial sums, such as continuity and integrability, are transferred to the limit.\\nExample 6.9 (Mercer) Suppose X= [−1,1] and the kernel is κ(x,x′) = 1 + xx′which\\ncorresponds to the RKHS Gof affine functions from X →R. To find the (eigenvalue,\\neigenfunction) pairs for the integral operator appearing in Theorem 6.4, we need to find\\nnumbers {λℓ}and orthonormal functions {ξℓ(x)}that solve\\nZ 1\\n−1\\n(1 + xx′) ξℓ(x′) dx′= λℓ ξℓ(x) , for all x ∈[−1,1].\\nConsider first a constant functionξ1(x) = c. Then, for allx ∈[−1,1], we have that 2c = λ1c,\\nand the normalization condition requires that\\nR 1\\n−1 c2 dx = 1. Together, these giveλ1 = 2 and\\nc = ±1/\\n√\\n2. Next, consider an affine function ξ2(x) = a + bx. Orthogonality requires that'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 245, 'page_label': '228'}, page_content='c = ±1/\\n√\\n2. Next, consider an affine function ξ2(x) = a + bx. Orthogonality requires that\\nZ 1\\n−1\\nc(a + bx) dx = 0,\\nwhich implies a = 0 (since c , 0). Moreover, the normalization condition then requires\\nZ 1\\n−1\\nb2 x2 dx = 1,\\nor, equivalently, 2b2/3 = 1, implying b = ±√3/2. Finally, the integral equation reads\\nZ 1\\n−1\\n(1 + xx′) bx′dx′= λ2 bx ⇐⇒ 2bx\\n3 = λ2bx,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 246, 'page_label': '229'}, page_content='Regularization and Kernel Methods 229\\nimplying that λ2 = 2/3. We take the positive solutions (i.e.,c >0 and b >0), and note that\\nλ1 ξ1(x) ξ1(x′) + λ2 ξ2(x) ξ2(x′) = 2 1√\\n2\\n1√\\n2\\n+ 2\\n3\\n√\\n3√\\n2\\nx\\n√\\n3√\\n2\\nx′= 1 + xx′= κ(x,x′),\\nand so we have found the decomposition appearing in (6.18). As an aside, observe that ξ1\\nand ξ2 are orthonormal versions of the first two Legendre polynomials. The corresponding ☞ 387\\nfeature map can be explicitly identified as ϕ1(x) =\\n√\\nλ1 ξ1(x) = 1 and ϕ2(x) = √λ2 ξ2(x) =\\nx.\\n6.4.4 Kernels from Kernels\\nThe following theorem lists some useful properties for constructing reproducing kernels\\nfrom existing reproducing kernels.\\nTheorem 6.5: Rules for Constructing Kernels from Other Kernels\\n1. If κ : Rp ×Rp →Ris a reproducing kernel and ϕ : X→ Rp is a function, then\\nκ(ϕ(x),ϕ(x′)) is a reproducing kernel from X×X→ R.\\n2. If κ : X×X→ R is a reproducing kernel and f : X→ R+ is a function, then\\nf (x)κ(x,x′) f (x′) is also a reproducing kernel from X×X→ R.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 246, 'page_label': '229'}, page_content='f (x)κ(x,x′) f (x′) is also a reproducing kernel from X×X→ R.\\n3. If κ1 and κ2 are reproducing kernels from X×X→ R, then so is their sum κ1 + κ2.\\n4. If κ1 and κ2 are reproducing kernels from X×X→ R, then so is their product\\nκ1κ2.\\n5. If κ1 and κ2 are reproducing kernels from X×X → R and Y×Y → R re-\\nspectively, then κ+((x,y),(x′,y′)) := κ1(x,x′) + κ2(y,y′) and κ×((x,y),(x′,y′)) :=\\nκ1(x,x′)κ2(y,y′) are reproducing kernels from (X×Y) ×(X×Y) →R.\\nProof: For Rules 1, 2, and 3 it is easy to verify that the resulting function is finite, sym-\\nmetric, and positive semidefinite, and so is a valid reproducing kernel by Theorem 6.2.\\nFor example, for Rule 1 we have Pn\\ni=1\\nPn\\nj=1 αi κ(yi,yj)αj ⩾0 for every choice of {αi}n\\ni=1\\nand {yi}n\\ni=1 ∈Rp, since κis a reproducing kernel. In particular, it holds true for yi = ϕ(xi),\\ni = 1,..., n. Rule 4 is easy to show for kernelsκ1,κ2 that admit a representation of the form\\n(6.17), since\\nκ1(x,x′) κ2(x,x′) =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nX\\ni⩾1\\nϕ(1)\\ni (x) ϕ(1)\\ni (x′)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nX'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 246, 'page_label': '229'}, page_content='(6.17), since\\nκ1(x,x′) κ2(x,x′) =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nX\\ni⩾1\\nϕ(1)\\ni (x) ϕ(1)\\ni (x′)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nX\\nj⩾1\\nϕ(2)\\nj (x) ϕ(2)\\nj (x′)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n=\\nX\\ni,j⩾1\\nϕ(1)\\ni (x) ϕ(2)\\nj (x) ϕ(1)\\ni (x′) ϕ(2)\\nj (x′)\\n=\\nX\\nk⩾1\\nϕk(x) ϕk(x′) =: κ(x,x′),\\nshowing that κ= κ1κ2 also admits a representation of the form (6.17), where the new (pos-\\nsibly infinite) sequence of features (ϕk) is identified in a one-to-one way with the sequence\\n(ϕ(1)\\ni ϕ(2)\\nj ). We leave the proof of rule 5 as an exercise (Exercise 8). □'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 247, 'page_label': '230'}, page_content='230 Representer Theorem\\nExample 6.10 (Polynomial Kernel) Consider x,x′∈R2 with\\nκ(x,x′) = (1 + ⟨x,x′⟩)2,\\nwhere ⟨x,x′⟩= x⊤x′. This is an example of a polynomial kernelpolynomial\\nkernel\\n. Combining the fact that\\nsums and products of kernels are again kernels (rules 3 and 4 of Theorem 6.5), we find that,\\nsince ⟨x,x′⟩and the constant function 1 are kernels, so are 1+ ⟨x,x′⟩and (1 + ⟨x,x′⟩)2. By\\nwriting\\nκ(x,x′) = (1 + x1 x′\\n1 + x2 x′\\n2)2\\n= 1 + 2x1 x′\\n1 + 2x2 x′\\n2 + 2x1 x2 x′\\n1 x′\\n2 + (x1 x′\\n1)2 + (x2 x′\\n2)2,\\nwe see that κ(x,x′) can be written as the inner product inR6 of the two feature vectorsϕ(x)\\nand ϕ(x′), where the feature map ϕ: R2 →R6 can be explicitly identified as\\nϕ(x) = [1,\\n√\\n2x1,\\n√\\n2x2,\\n√\\n2x1 x2,x2\\n1,x2\\n2]⊤.\\nThus, the RKHS determined by κ can be explicitly identified with the space of functions\\nx 7→ϕ(x)⊤βfor some β∈R6.\\nIn the above example we could explicitly identify the feature map. However, in general'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 247, 'page_label': '230'}, page_content='In the above example we could explicitly identify the feature map. However, in general\\na feature map need not be explicitly available. Using a particular reproducing kernel cor-\\nresponds to using an implicit (possibly infinite dimensional!) feature map that never needs\\nto be explicitly computed.\\n6.5 Representer Theorem\\nRecall the setting discussed at the beginning of this chapter: we are given training data\\nτ = {(xi,yi)}n\\ni=1 and a loss function that measures the fit to the data, and we wish to find\\na function g that minimizes the training loss, with the addition of a regularization term,\\nas described in Section 6.2. To do this, we assume first that the class Gof prediction\\nfunctions can be decomposed as the direct sum of an RKHSH, defined by a kernel function\\nκ: X×X→ R, and another linear space of real-valued functions H0 on X; that is,\\nG= H⊕H 0,\\nmeaning that any element g ∈G can be written as g = h + h0, with h ∈H and h0 ∈H0.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 247, 'page_label': '230'}, page_content='G= H⊕H 0,\\nmeaning that any element g ∈G can be written as g = h + h0, with h ∈H and h0 ∈H0.\\nIn minimizing the training loss we wish to penalize the h term of g but not the h0 term.\\nSpecifically, the aim is to solve the functional optimization problem\\nmin\\ng∈H⊕H0\\n1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) + γ∥g∥2\\nH. (6.19)\\nHere, we use a slight abuse of notation: ∥g∥H means ∥h∥H if g = h + h0, as above. In this\\nway, we can viewH0 as the null space of the functional g 7→∥g∥H. This null space may be\\nempty, but typically has a small dimensionm; for example it could be the one-dimensional\\nspace of constant functions, as in Example 6.2.☞217'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 248, 'page_label': '231'}, page_content='Regularization and Kernel Methods 231\\nExample 6.11 (Null Space) Consider again the setting of Example 6.2, for which we\\nhave feature vectorsex = [1,x⊤]⊤and Gconsists of functions of the formg : ex 7→β0 + x⊤β.\\nEach function g can be decomposed as g = h + h0, where h : ex 7→x⊤β, and h0 : ex 7→β0.\\nGiven g ∈G, we have∥g∥H = ∥β∥, and so the null spaceH0 of the functional g 7→∥g∥H\\n(that is, the set of all functions g ∈G for which ∥g∥H = 0) is the set of constant functions\\nhere, which has dimension m = 1.\\nRegularization favors elements in H0 and penalizes large elements in H. As the reg-\\nularization parameter γ varies between zero and infinity, solutions to (6.19) vary from\\n“complex” (g ∈H⊕H 0) to “simple” (g ∈H0).\\nA key reason why RKHSs are so useful is the following. By choosing Hto be an\\nRKHS in (6.19) this functional optimization problem e ffectively becomes a parametric\\noptimization problem. The reason is that any solution to (6.19) can be represented as a'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 248, 'page_label': '231'}, page_content='optimization problem. The reason is that any solution to (6.19) can be represented as a\\nfinite-dimensional linear combination of kernel functions, evaluated at the training sample.\\nThis is known as the kernel trick kernel trick.\\nTheorem 6.6: Representer Theorem\\nThe solution to the penalized optimization problem (6.19) is of the form\\ng(x) =\\nnX\\ni=1\\nαi κ(xi,x) +\\nmX\\nj=1\\nηj qj(x), (6.20)\\nwhere {q1,..., qm}is a basis of H0.\\nProof: Let F= Span \\x08κxi ,i = 1,..., n\\t. Clearly, F ⊆H. Then, the Hilbert space Hcan\\nbe represented as H= F⊕F ⊥, where F⊥ is the orthogonal complement of F. In other\\nwords, F⊥is the class of functions\\n{f ⊥∈H : ⟨f ⊥, f ⟩H = 0, f ∈F}≡{ f ⊥: ⟨f ⊥,κxi ⟩H = 0, ∀i}.\\nIt follows, by the reproducing kernel property, that for all f ⊥∈F⊥:\\nf ⊥(xi) = ⟨f ⊥,κxi ⟩H = 0, i = 1,..., n.\\nNow, take any g ∈H⊕H 0, and write it as g = f + f ⊥+ h0, with f ∈F, f ⊥∈F⊥, and\\nh0 ∈H0. By the definition of the null space H0, we have ∥g∥2\\nH = ∥f + f ⊥∥2\\nH. Moreover, by'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 248, 'page_label': '231'}, page_content='h0 ∈H0. By the definition of the null space H0, we have ∥g∥2\\nH = ∥f + f ⊥∥2\\nH. Moreover, by\\nPythagoras’ theorem, the latter is equal to ∥f ∥2\\nH + ∥f ⊥∥2\\nH. It follows that\\n1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) + γ∥g∥2\\nH = 1\\nn\\nnX\\ni=1\\nLoss(yi, f (xi) + h0(xi)) + γ\\n\\x10\\n∥f ∥2\\nH + ∥f ⊥∥2\\nH\\n\\x11\\n⩾1\\nn\\nnX\\ni=1\\nLoss(yi, f (xi) + h0(xi)) + γ∥f ∥2\\nH.\\nSince we can obtain equality by taking f ⊥= 0, this implies that the minimizer of the pen-\\nalized optimization problem (6.19) lies in the subspaceF⊕H 0 of G= H⊕H0, and hence\\nis of the form (6.20). □'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 249, 'page_label': '232'}, page_content='232 Representer Theorem\\nSubstituting the representation (6.20) of g into (6.19) gives the finite-dimensional op-\\ntimization problem:\\nmin\\nα∈Rn,η∈Rm\\n1\\nn\\nnX\\ni=1\\nLoss(yi,(Kα+ Qη)i) + γα⊤Kα, (6.21)\\nwhere\\n• K is the n ×n (Gram) matrix with entries [κ(xi,xj),i = 1,..., n, j = 1,..., n].\\n• Q is the n ×m matrix with entries [qj(xi),i = 1,..., n, j = 1,..., m].\\nIn particular, for the squared-error loss we have\\nmin\\nα∈Rn,η∈Rm\\n1\\nn\\n\\r\\r\\r y −(Kα+ Qη)\\n\\r\\r\\r\\n2\\n+ γα⊤Kα. (6.22)\\nThis is a convex optimization problem, and its solution is found by di fferentiating (6.22)\\nwith respect to αand ηand equating to zero, leading to the following system of ( n + m)\\nlinear equations: \"KK⊤+ n γK KQ\\nQ⊤K⊤ Q⊤Q\\n#\"α\\nη\\n#\\n=\\n\"K⊤\\nQ⊤\\n#\\ny. (6.23)\\nAs long as Q is of full column rank, the minimizing function is unique.\\nExample 6.12 (Ridge Regression (cont.)) We return to Example 6.2 and identify that\\nHis the RKHS with linear kernel functionκ(x,x′) = x⊤x′and C= H0 is the linear space of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 249, 'page_label': '232'}, page_content='His the RKHS with linear kernel functionκ(x,x′) = x⊤x′and C= H0 is the linear space of\\nconstant functions. In this case,H0 is spanned by the functionq1 ≡1. Moreover,K = XX⊤\\nand Q = 1.\\nIf we appeal to the representer theorem directly, then the problem in (6.6) becomes, as\\na result of (6.21):\\nmin\\nα,η0\\n1\\nn\\n\\r\\r\\r y −η0 1 −XX⊤α\\n\\r\\r\\r\\n2\\n+ γ∥X⊤α∥2.\\nThis is a convex optimization problem, and so the solution follows by taking derivatives\\nand setting them to zero. This gives the equations\\nXX⊤\\x00(XX⊤+ n γIn) α+ η0 1 −y\\x01 = 0,\\nand\\nn η0 = 1⊤(y −XX⊤α).\\nNote that these are equivalent to (6.8) and (6.9) (once again assuming thatn ⩾p and X has\\nfull rank p). Equivalently, the solution is found by solving (6.23):\\n\"XX⊤XX⊤+ n γXX⊤ XX⊤1\\n1⊤XX⊤ n\\n#\" α\\nη0\\n#\\n=\\n\"XX⊤\\n1⊤\\n#\\ny.\\nThis is a system of (n + 1) linear equations, and is typically of much larger dimension than\\nthe (p + 1) linear equations given by (6.8) and (6.9). As such, one may question the prac-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 249, 'page_label': '232'}, page_content='the (p + 1) linear equations given by (6.8) and (6.9). As such, one may question the prac-\\nticality of reformulating the problem in this way. However, the benefit of this formulation\\nis that the problem can be expressed entirely through the Gram matrix K, without having\\nto explicitly compute the feature vectors — in turn permitting the (implicit) use of infinite\\ndimensional feature spaces.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 250, 'page_label': '233'}, page_content='Regularization and Kernel Methods 233\\nExample 6.13 (Estimating the Peaks Function) Figure 6.4 shows the surface plot of\\nthe peaks function:\\nf (x1,x2) = 3(1 −x1)2e−x2\\n1−(x2+1)2\\n−10\\n\\x12x1\\n5 −x3\\n1 −x5\\n2\\n\\x13\\ne−x2\\n1−x2\\n2 −1\\n3e−(x1+1)2−x2\\n2 . (6.24)\\nThe goal is to learn the function y = f (x) based on a small set of training data (pairs of\\n(x,y) values). The red dots in the figure represent dataτ= {(xi,yi)}20\\ni=1, where yi = f (xi) and\\nthe {xi}have been chosen in a quasi-random quasi-randomway, using Hammersley points (with bases 2\\nand 3) on the square [−3,3]2. Quasi-random point sets have better space-filling properties\\nthan either a regular grid of points or a set of pseudo-random points. We refer to [71] for\\ndetails. Note that there is no observation noise in this particular problem.\\n-5\\n2-2\\n0\\n00\\n5\\n-22\\nFigure 6.4: Peaks function sampled at 20 Hammersley points.\\nThe purpose of this example is to illustrate how, using the small data set of sizen = 20,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 250, 'page_label': '233'}, page_content='The purpose of this example is to illustrate how, using the small data set of sizen = 20,\\nthe entire peaks function can be approximated well using kernel methods. In particular, we\\nuse the Gaussian kernel (6.15) on R2, and denote by Hthe unique RKHS corresponding\\nto this kernel. We omit the regularization term in (6.19), and thus our objective is to find\\nthe solution to\\nmin\\ng∈H\\n1\\nn\\nnX\\ni=1\\n(yi −g(xi))2.\\nBy the representer theorem, the optimal function is of the form\\ng(x) =\\nnX\\ni=1\\nαi exp\\n \\n−1\\n2\\n∥x −xi∥2\\nσ2\\n!\\n,\\nwhere α:= [α1,...,α n]⊤is, by (6.23), the solution to the set of linear equations KK⊤α=\\nKy.\\nNote that we are performing regression over the class of functions Hwith an implicit\\nfeature space. Due to the representer theorem, the solution to this problem coincides with\\nthe solution to the linear regression problem for which the i-th feature (for i = 1,..., n) is\\nchosen to be the vector [κ(x1,xi),...,κ (xn,xi)]⊤.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 251, 'page_label': '234'}, page_content=\"234 Representer Theorem\\nThe following code performs these calculations and gives the contour plots of g and\\nthe peaksfunctions, shown in Figure 6.5. We see that the two are quite close. Code for the\\ngeneration of Hammersley points is available from the book’s GitHub site asgenham.py.\\npeakskernel.py\\nfrom genham import hammersley\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nfrom matplotlib import cm\\nfrom numpy.linalg import norm\\nimport numpy as np\\ndef peaks(x,y):\\nz = (3*(1-x)**2 * np.exp(-(x**2) - (y+1)**2)\\n- 10*(x/5 - x**3 - y**5) * np.exp(-x**2 - y**2)\\n- 1/3 * np.exp(-(x+1)**2 - y**2))\\nreturn (z)\\nn = 20\\nx = -3 + 6*hammersley([2,3],n)\\nz = peaks(x[:,0],x[:,1])\\nxx, yy = np.mgrid[-3:3:150j,-3:3:150j]\\nzz = peaks(xx,yy)\\nplt.contour(xx,yy,zz,levels=50)\\nfig=plt.figure()\\nax = fig.add_subplot(111,projection= '3d')\\nax.plot_surface(xx,yy,zz,rstride=1,cstride=1,color= 'c',alpha=0.3,\\nlinewidth=0)\\nax.scatter(x[:,0],x[:,1],z,color= 'k',s=20)\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 251, 'page_label': '234'}, page_content=\"linewidth=0)\\nax.scatter(x[:,0],x[:,1],z,color= 'k',s=20)\\nplt.show()\\nsig2 = 0.3 # kernel parameter\\ndef k(x,u):\\nreturn (np.exp(-0.5*norm(x- u)**2/sig2))\\nK = np.zeros((n,n))\\nfor i in range (n):\\nfor j in range (n):\\nK[i,j] = k(x[i,:],x[j])\\nalpha = np.linalg.solve(K@K.T, K@z)\\nN, = xx.flatten().shape\\nKx = np.zeros((n,N))\\nfor i in range (n):\\nfor j in range (N):\\nKx[i,j] = k(x[i,:],np.array([xx.flatten()[j],yy.flatten()[j\\n]]))\\ng = Kx.T @ alpha\\ndim = np.sqrt(N).astype( int )\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 252, 'page_label': '235'}, page_content='Regularization and Kernel Methods 235\\nyhat = g.reshape(dim,dim)\\nplt.contour(xx,yy,yhat,levels=50)\\n-2 0 2\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\n-2 0 2\\nFigure 6.5: Contour plots for the prediction function g (left) and the peaks function given\\nin (6.24) (right).\\n6.6 Smoothing Cubic Splines\\nA striking application of kernel methods is to fitting “well-behaved” functions to data.\\nKey examples of “well-behaved” functions are those that do not have large second-\\norder derivatives. Consider functionsg : [0,1] →Rthat are twice differentiable and define\\n∥g′′∥2 :=\\nR 1\\n0 (g′′(x))2 dx as a measure of the size of the second derivative.\\nExample 6.14 (Behavior of ∥g′′∥2) Intuitively, the larger ∥g′′∥2 is, the more “wiggly”\\nthe function g will be. As an explicit example, considerg(x) = sin(ωx) for x ∈[0,1], where\\nωis a free parameter. We can explicitly computeg′′(x) = −ω2 sin(ωx), and consequently\\n∥g′′∥2 =\\nZ 1\\n0\\nω4 sin2(ωx) dx = ω4\\n2 (1 −sinc(2ω)) .\\nAs |ω|→∞ , the frequency of g increases and we have ∥g′′∥2 →∞.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 252, 'page_label': '235'}, page_content='2 (1 −sinc(2ω)) .\\nAs |ω|→∞ , the frequency of g increases and we have ∥g′′∥2 →∞.\\nNow, in the context of data fitting, consider the following penalized least-squares op-\\ntimization problem on [0,1]:\\nmin\\ng∈G\\n1\\nn\\nnX\\ni=1\\n(yi −g(xi))2 + γ∥g′′∥2, (6.25)\\nwhere we will specify Gin what follows. In order to apply the kernel machinery, we want\\nto write this in the form (6.19), for some RKHSHand null space H0. Clearly, the norm on\\nHshould be of the form ∥g∥H = ∥g′′∥and should be well-defined (i.e., finite and ensuring\\ng and g′are absolutely continuous). This suggests that we take\\nH= {g ∈L2[0,1] : ∥g′′∥<∞,g,g′absolutely continuous, g(0) = g′(0) = 0},'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 253, 'page_label': '236'}, page_content='236 Smoothing Cubic Splines\\nwith inner product\\n⟨f,g⟩H :=\\nZ 1\\n0\\nf ′′(x) g′′(x) dx.\\nOne rationale for imposing the boundary conditions g(0) = g′(0) = 0 is as follows: when\\nexpanding g about the point x = 0, Taylor’s theorem (with integral remainder term) states\\nthat\\ng(x) = g(0) + g′(0) x +\\nZ x\\n0\\ng′′(s) (x −s) ds.\\nImposing the condition that g(0) = g′(0) = 0 for functions in Hwill ensure that G=\\nH⊕H 0 where the null space H0 contains only linear functions, as we will see.\\nTo see that this His in fact an RKHS, we derive its reproducing kernel. Using integra-\\ntion by parts (or directly from the Taylor expansion above), write\\ng(x) =\\nZ x\\n0\\ng′(s) ds =\\nZ x\\n0\\ng′′(s) (x −s) ds =\\nZ 1\\n0\\ng′′(s) (x −s)+ ds.\\nIf κis a kernel, then by the reproducing property it must hold that\\ng(x) = ⟨g,κx⟩H =\\nZ 1\\n0\\ng′′(s) κ′′\\nx (s) ds,\\nso that κ must satisfy ∂2\\n∂s2 κ(x,s) = (x −s)+, where y+ := max{y,0}. Therefore, noting that\\nκ(x,u) = ⟨κx,κu⟩H, we have (see Exercise 15)\\nκ(x,u) =\\nZ 1\\n0\\n∂2κ(x,s)\\n∂s2\\n∂2κ(u,s)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 253, 'page_label': '236'}, page_content='κ(x,u) = ⟨κx,κu⟩H, we have (see Exercise 15)\\nκ(x,u) =\\nZ 1\\n0\\n∂2κ(x,s)\\n∂s2\\n∂2κ(u,s)\\n∂s2 ds = max{x,u}min{x,u}2\\n2 −min{x,u}3\\n6 .\\nThe last expression is a cubic function with quadratic and cubic terms that misses the\\nconstant and linear monomials. This is not surprising considering the Taylor’s theorem\\ninterpretation of a function g ∈H . If we now take H0 as the space of functions of the\\nfollowing form (having zero second derivative):\\nh0 = η1 + η2 x, x ∈[0,1],\\nthen (6.25) is exactly of the form (6.19).\\nAs a consequence of the representer Theorem 6.6, the optimal solution to (6.25) is a\\nlinear combination of piecewise cubic functions:\\ng(x) = η1 + η2 x +\\nnX\\ni=1\\nαi κ(xi,x). (6.26)\\nSuch a function is called a cubic splinecubic spline with n knots (with one knot at each data point xi)\\n— so called, because the piecewise cubic function between knots is required to be “tied\\ntogether” at the knots. The parameters α,η are determined from (6.21) for instance by'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 253, 'page_label': '236'}, page_content='together” at the knots. The parameters α,η are determined from (6.21) for instance by\\nsolving (6.23) with matrices K = [κ(xi,xj)]n\\ni,j=1 and Q with i-th row of the form [1,xi] for\\ni = 1,..., n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 254, 'page_label': '237'}, page_content='Regularization and Kernel Methods 237\\nExample 6.15 (Smoothing Spline) Figure 6.6 shows various cubic smoothing splines\\nfor the data (0 .05,0.4),(0.2,0.2),(0.5,0.6),(0.75,0.7),(1,1). In the figure, we use the re-\\nparameterization r = 1/(1 + n γ) for the smoothing parameter. Thusr ∈[0,1], where r = 0\\nmeans an infinite penalty for curvature (leading to the ordinary linear regression solution)\\nand r = 1 does not penalize curvature at all and leads to a perfect fit via the so-called nat-\\nural spline. Of course the latter will generally lead to overfitting. Forr from 0 up to 0.8 the\\nsolutions will be close to the simple linear regression line, while only for r very close to 1,\\nthe shape of the curve changes significantly.\\n0 0.2 0.4 0.6 0.8 1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure 6.6: Various cubic smoothing splines for smoothing parameter r = 1/(1 + n γ) ∈\\n{0.8,0.99,0.999,0.999999}. For r = 1, the natural spline through the data points is ob-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 254, 'page_label': '237'}, page_content='{0.8,0.99,0.999,0.999999}. For r = 1, the natural spline through the data points is ob-\\ntained; for r = 0, the simple linear regression line is found.\\nThe following code first computes the matrices K and Q, and then solves the linear\\nsystem (6.23). Finally, the smoothing curve is determined via (6.26), for selected points,\\nand then plotted. Note that the code plots only a single curve corresponding to the specified\\nvalue of p.\\nsmoothspline.py\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nx = np.array([[0.05, 0.2, 0.5, 0.75, 1.]]).T\\ny = np.array([[0.4, 0.2, 0.6, 0.7, 1.]]).T\\nn = x.shape[0]\\nr = 0.999\\nngamma = (1-r)/r\\nk = lambda x1, x2 : (1/2)* np. max ((x1,x2)) * np. min ((x1,x2)) ** 2 \\\\\\n- ((1/6)* np. min ((x1,x2))**3)\\nK = np.zeros((n,n))\\nfor i in range (n):\\nfor j in range (n):'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 255, 'page_label': '238'}, page_content=\"238 Gaussian Process Regression\\nK[i,j] = k(x[i], x[j])\\nQ = np.hstack((np.ones((n,1)), x))\\nm1 = np.hstack((K @ K.T + (ngamma * K), K @ Q))\\nm2 = np.hstack((Q.T @ K.T, Q.T @ Q))\\nM = np.vstack((m1,m2))\\nc = np.vstack((K, Q.T)) @ y\\nad = np.linalg.solve(M,c)\\n# plot the curve\\nxx = np.arange(0,1+0.01,0.01).reshape(-1,1)\\ng = np.zeros_like(xx)\\nQx = np.hstack((np.ones_like(xx), xx))\\ng = np.zeros_like(xx)\\nN = np.shape(xx)[0]\\nKx = np.zeros((n,N))\\nfor i in range (n):\\nfor j in range (N):\\nKx[i,j] = k(x[i], xx[j])\\ng = g + np.hstack((Kx.T, Qx)) @ ad\\nplt.ylim((0,1.15))\\nplt.plot(xx, g, label = 'r = {} '.format (r), linewidth = 2)\\nplt.plot(x,y, 'b.', markersize=15)\\nplt.xlabel( '$x$')\\nplt.ylabel( '$y$')\\nplt.legend()\\n6.7 Gaussian Process Regression\\nAnother application of the kernel machinery is to Gaussian process regression. AGaussian\\nprocessGaussian\\nprocess\\n(GP) on a space Xis a stochastic process {Zx,x ∈X} where, for any choice of\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 255, 'page_label': '238'}, page_content='processGaussian\\nprocess\\n(GP) on a space Xis a stochastic process {Zx,x ∈X} where, for any choice of\\nindices x1,..., xn, the vector [ Zx1 ,... Zxn ]⊤ has a multivariate Gaussian distribution. As\\nsuch, the distribution of a GP is completely specified by its mean and covariance functions\\nµ: X→ Rand κ : X×X→ R, respectively. The covariance function is a finite positive\\nsemidefinite function, and hence, in view of Theorem 6.2, can be viewed as a reproducing\\nkernel on X.\\nAs for ordinary regression, the objective of GP regression is to learn a regression func-☞168\\ntion g that predicts a responsey = g(x) for each feature vectorx. This is done in a Bayesian\\nfashion, by establishing (1) a prior pdf for g and (2) the likelihood of the data, for a given\\ng. From these two we then derive, via Bayes’ formula, the posterior distribution ofg given\\nthe data. We refer to Section 2.9 for the general Bayesian framework.☞48'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 255, 'page_label': '238'}, page_content='the data. We refer to Section 2.9 for the general Bayesian framework.☞48\\nA simple Bayesian model for GP regression is as follows. First, the prior distribution of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 256, 'page_label': '239'}, page_content='Regularization and Kernel Methods 239\\ng is taken to be the distribution of a GP with some known mean function µand covariance\\nfunction (that is, kernel) κ. Most often µ is taken to be a constant, and for simplicity of\\nexposition, we take it to be 0. The Gaussian kernel (6.15) is often used for the covariance\\nfunction. For radial basis function kernels (including the Gaussian kernel), points that are\\ncloser will be more highly correlated or “similar” [97], independent of translations in space.\\nSecond, similar to standard regression, we view the observed feature vectorsx1,..., xn\\nas fixed and the responses y1,..., yn as outcomes of random variables Y1,..., Yn. Specific-\\nally, given g, we model the {Yi}as\\nYi = g(xi) + εi , i = 1,..., n, (6.27)\\nwhere {εi}\\niid\\n∼N(0,σ2). To simplify the analysis, let us assume thatσ2 is known, so no prior\\nneeds to be specified for σ2. Let g = [g(x1),..., g(xn)]⊤ be the (unknown) vector of re-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 256, 'page_label': '239'}, page_content='needs to be specified for σ2. Let g = [g(x1),..., g(xn)]⊤ be the (unknown) vector of re-\\ngression values. Placing a GP prior on the functiong is equivalent to placing a multivariate\\nGaussian prior on the vector g:\\ng ∼N(0,K), (6.28)\\nwhere the covariance matrix K of g is a Gram matrix (implicitly associated with a feature\\nmap through the kernel κ), given by:\\nK =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nκ(x1,x1) κ(x1,x2) ... κ (x1,xn)\\nκ(x2,x1) κ(x2,x2) ... κ (x2,xn)\\n... ... ... ...\\nκ(xn,x1) κ(xn,x2) ... κ (xn,xn)\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (6.29)\\nThe likelihood of our data given g, denoted p(y |g), is obtained directly from the model\\n(6.27):\\n(Y |g) ∼N(g,σ2In). (6.30)\\nSolving this Bayesian problem involves deriving the posterior distribution of ( g |Y). To\\ndo so, we first note that since Y has covariance matrix K + σ2In (which can be seen from\\n(6.27)), the joint distribution of Y and g is again normal, with mean 0 and covariance\\nmatrix:\\nKy,g =\\n\"K + σ2In K\\nK K\\n#\\n. (6.31)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 256, 'page_label': '239'}, page_content='matrix:\\nKy,g =\\n\"K + σ2In K\\nK K\\n#\\n. (6.31)\\nThe posterior can then be found by conditioning on Y = y, via Theorem C.8, giving ☞ 436\\n(g |y) ∼N\\n\\x10\\nK⊤(K + σ2In)−1 y, K −K⊤(K + σ2In)−1K\\n\\x11\\n.\\nThis only gives information aboutg at the observed points x1,..., xn. It is more interesting\\nto consider the posterior predictive distribution ofeg := g(ex) for a new input ex. We can find\\nthe corresponding posterior predictive pdf p(eg |y) by integrating out the joint posterior pdf\\np(eg,g |y), which is equivalent to taking the expectation of p(eg |g) when g is distributed\\naccording to the posterior pdf p(g |y); that is,\\np(eg |y) =\\nZ\\np(eg |g) p(g |y) dg.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 257, 'page_label': '240'}, page_content='240 Gaussian Process Regression\\nTo do so more easily than direct evaluation via the above integral representation ofp(eg |y),\\nwe can begin with the joint distribution of [y⊤,eg]⊤, which is multivariate normal with mean\\n0 and covariance matrix\\neK =\\n\"K + σ2In κ\\nκ⊤ κ(ex,ex)\\n#\\n, (6.32)\\nwhere κ= [κ(ex,x1),...,κ (ex,xn)]⊤. It now follows, again by using Theorem C.8, that (eg |y)\\nhas a normal distribution with mean and variance given respectively by\\nµ(ex) = κ⊤(K + σ2In)−1 y (6.33)\\nand\\nσ2(ex) = κ(ex,ex) −κ⊤(K + σ2In)−1κ. (6.34)\\nThese are sometimes called the predictivepredictive mean and variance. It is important to note that\\nwe are predicting the expected response EeY = g(ex) here, and not the actual response eY.\\nExample 6.16 (GP Regression) Suppose the regression function is\\ng(x) = 2 sin(2πx), x ∈[0,1].\\nWe use GP regression to estimateg, using a Gaussian kernel of the form (6.15) with band-\\nwidth parameter 0 .2. The explanatory variables x1,..., x30 were drawn uniformly on the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 257, 'page_label': '240'}, page_content='width parameter 0 .2. The explanatory variables x1,..., x30 were drawn uniformly on the\\ninterval [0,1], and the responses were obtained from (6.27), with noise level σ= 0.5. Fig-\\nure 6.7 shows 10 samples from the prior distribution for g as well as the data points and\\nthe true sinusoidal regression function g.\\n0 0.2 0.4 0.6 0.8 1\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\n0 0.2 0.4 0.6 0.8 1\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\nFigure 6.7: Left: samples drawn from the GP prior distribution. Right: the true regression\\nfunction with the data points.\\nAgain assuming that the variance σ2, is known, the predictive distribution as determ-\\nined by (6.33) and (6.34) is shown in Figure 6.8 for bandwidth 0 .2 (left) and 0.02 (right).\\nClearly, decreasing the bandwidth leads to the covariance between pointsx and x′decreas-\\ning at a faster rate with respect to the squared distance ∥x −x′∥2, leading to a predictive\\nmean that is less smooth.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 258, 'page_label': '241'}, page_content='Regularization and Kernel Methods 241\\n0 0.2 0.4 0.6 0.8 1\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\ng(x)\\nPredictive Mean\\n0 0.2 0.4 0.6 0.8 1\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\ng(x)\\nPredictive Mean\\nFigure 6.8: GP regression of synthetic data set with bandwidth 0 .2 (left) and 0.02 (right).\\nThe black dots represent the data and the blue curve is the latent functiong(x) = 2 sin(2πx).\\nThe red curve is the mean of the GP predictive distribution given by (6.33), and the shaded\\nregion is the 95% confidence band, corresponding to the predictive variance given in (6.34).\\nIn the above exposition, we have taken the mean function for the prior distribution\\nof g to be identically zero. If instead we have a general mean function m and write\\nm = [m(x1),..., m(xn)]⊤ then the predictive variance (6.34) remains unchanged, and the\\npredictive mean (6.33) is modified to read\\nµ(ex) = m(ex) + κ⊤(K + σ2In)−1 (y −m) . (6.35)\\nTypically, the variance σ2 appearing in (6.27) is not known, and the kernel κ itself'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 258, 'page_label': '241'}, page_content='Typically, the variance σ2 appearing in (6.27) is not known, and the kernel κ itself\\ndepends on several parameters — for instance a Gaussian kernel (6.15) with an unknown\\nbandwidth parameter. In the Bayesian framework, one typically specifies a hierarchical\\nmodel by introducing a prior p(θ) for the vector θ of such hyperparameters hyperparamet-\\ners\\n. Now, the\\nGP prior ( g |θ) (equivalently, specifying p(g |θ)) and the model for the likelihood of the\\ndata given Y|g,θ, namely p(y |g,θ), are both dependent on θ. The posterior distribution of\\n(g |y,θ) is as before.\\nOne approach to setting the hyperparameter θis to determine its posterior p(θ|y) and\\nobtain a point estimate, for instance via its maximum a posteriori estimate. However, this\\ncan be a computationally demanding exercise. What is frequently done in practice is to\\nconsider instead the marginal likelihood p(y |θ) and maximize this with respect to θ. This\\nprocedure is called empirical Bayes empirical Bayes.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 258, 'page_label': '241'}, page_content='procedure is called empirical Bayes empirical Bayes.\\nConsidering again the mean function m to be identically zero, from (6.31), we have\\nthat ( Y |θ) is multivariate normal with mean 0 and covariance matrix Ky = K + σ2In,\\nimmediately giving an expression for the marginal log-likelihood:\\nln p(y |θ) = −n\\n2 ln(2π) −1\\n2 ln |det(Ky)|− 1\\n2 y⊤K−1\\ny y. (6.36)\\nWe notice that only the second and third terms in (6.36) depend onθ. Considering a partial'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 259, 'page_label': '242'}, page_content='242 Kernel PCA\\nderivative of (6.36) with respect to a single elementθof the hyperparameter vectorθyields\\n∂\\n∂θln p(y |θ) = −1\\n2tr\\n \\nK−1\\ny\\n\" ∂\\n∂θKy\\n#!\\n+ 1\\n2 y⊤K−1\\ny\\n\" ∂\\n∂θKy\\n#\\nK−1\\ny y, (6.37)\\nwhere\\nh\\n∂\\n∂θKy\\ni\\nis the element-wise derivative of matrix Ky with respect to θ. If these partial\\nderivatives can be computed for each hyperparameterθ, gradient information could be used\\nwhen maximizing (6.36).\\nExample 6.17 (GP Regression (cont.)) Continuing Example 6.16, we plot in Fig-\\nure 6.9 the marginal log-likelihood as a function of the noise level σand bandwidth para-\\nmeter.\\n10-2 10-1 10010-1\\n100\\nFigure 6.9: Contours of the marginal log-likelihood for the GP regression example. The\\nmaximum is denoted by a cross.\\nThe maximum is attained for a bandwidth parameter around 0.20 and σ≈0.44, which\\nis very close to the left panel of Figure 6.8 for the case where σwas assumed to be known\\n(and equal to 0.5). We note here that the marginal log-likelihood is extremely flat, perhaps'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 259, 'page_label': '242'}, page_content='(and equal to 0.5). We note here that the marginal log-likelihood is extremely flat, perhaps\\nowing to the small number of points.\\n6.8 Kernel PCA\\nIn its basic form, kernel PCA (principal component analysis) can be thought of as PCA in\\nfeature space. The main motivation for PCA introduced in Section 4.8 was as a dimension-☞153\\nality reduction technique. There, the analysis rested on an SVD of the matrix bΣ = 1\\nn X⊤X,\\nwhere the data in X was first centered via x′\\ni,j = xi,j −xj where xi = 1\\nn\\nPn\\ni=1 xi,j.\\nWhat we shall do is to first re-cast the problem in terms of the Gram matrixK = XX⊤=\\n[⟨xi,xj⟩] (note the different order of X and X⊤), and subsequently replace the inner product\\n⟨x,x′⟩with κ(x,x′) for a general reproducing kernel κ. To make the link, let us start with\\nan SVD of X⊤:\\nX⊤= UDV⊤. (6.38)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 260, 'page_label': '243'}, page_content='Regularization and Kernel Methods 243\\nThe dimensions of X⊤, U, D, and V are d ×n, d ×d, d ×n, and n ×n, respectively. Then an\\nSVD of X⊤X is\\nX⊤X = (UDV⊤)(UDV⊤)⊤= U(DD⊤)U⊤\\nand an SVD of K is\\nK = (UDV⊤)⊤(UDV⊤) = V(D⊤D)V⊤.\\nLet λ1 ⩾··· ⩾λr >0 denote the non-zero eigenvalues of X⊤X (or, equivalently, ofK) and\\ndenote the corresponding r ×r diagonal matrix by Λ. Without loss of generality we can\\nassume that the eigenvector of X⊤X corresponding to λk is the k-th column of U and that\\nthe k-th column of V is an eigenvector of K. Similar to Section 4.8, let Uk and Vk contain ☞ 153\\nthe first k columns of U and V, respectively, and letΛk be the correspondingk×k submatrix\\nof Λ, k = 1,..., r.\\nBy the SVD (6.38), we haveX⊤Vk = UDV⊤Vk = UkΛ1/2\\nk . Next, consider the projection\\nof a point x onto the k-dimensional linear space spanned by the columns of Uk — the first\\nk principal components. We saw in Section 4.8 that this projection simply is the linear\\nmapping x 7→U⊤'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 260, 'page_label': '243'}, page_content='mapping x 7→U⊤\\nk x. Using the fact that Uk = X⊤VkΛ−1/2, we find that x is projected to a\\npoint z given by\\nz = Λ−1/2\\nk V⊤\\nk Xx = Λ−1/2\\nk V⊤\\nk κx,\\nwhere we have (suggestively) defined κx := [⟨x1,x⟩,..., ⟨xn,x⟩]⊤. The important point\\nis that z is completely determined by the vector of inner products κx and the k principal\\neigenvalues and (right) eigenvectors of the Gram matrix K. Note that each component zm\\nof z is of the form\\nzm =\\nnX\\ni=1\\nαm,i κ(xi,x), m = 1,..., k. (6.39)\\nThe preceding discussion assumed centering of the columns of X. Consider now an\\nuncentered data matrix eX. Then the centered data can be written as X = eX −1\\nn EneX, where\\nEn is the n ×n matrix of ones. Consequently,\\nXX⊤= eXeX\\n⊤\\n−1\\nnEneXeX\\n⊤\\n−1\\nn\\neXeX\\n⊤\\nEn + 1\\nn2 EneXeX\\n⊤\\nEn,\\nor, more compactly,XX⊤= H eXeX\\n⊤\\nH, where H = In −1\\nn 1n1⊤\\nn , In is the n×n identity matrix,\\nand 1n is the n ×1 vector of ones.\\nTo generalize to the kernel setting, we replace eXeX\\n⊤\\nby K = [κ(xi,xj),i, j = 1,..., n]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 260, 'page_label': '243'}, page_content='To generalize to the kernel setting, we replace eXeX\\n⊤\\nby K = [κ(xi,xj),i, j = 1,..., n]\\nand set κx = [κ(x1,x),...,κ (xn,x)]⊤, so thatΛk is the diagonal matrix of thek largest eigen-\\nvalues of HKH and Vk is the corresponding matrix of eigenvectors. Note that the “usual”\\nPCA is recovered when we use the linear kernelκ(x,y) = x⊤y. However, instead of having\\nonly kernels that are explicitly inner products of feature vectors, we are now permitted to\\nimplicitly use infinite feature maps (functions) by using kernels.\\nExample 6.18 (Kernel PCA) We simulated 200 points, x1,..., x200, from the uniform\\ndistribution on the set B1 ∪(B4 ∩Bc\\n3), where Br := {(x,y) ∈R2 : x2 + y2 ⩽r2}(disk with\\nradius r). We apply kernel PCA with Gaussian kernel κ(x,x′) = exp\\n\\x10\\n−∥x −x′∥2\\n\\x11\\nand\\ncompute the functions zm(x),m = 1,..., 9 in (6.39). Their density plots are shown in Fig-\\nure 6.10. The data points are superimposed in each plot. From this we see that the principal'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 260, 'page_label': '243'}, page_content='ure 6.10. The data points are superimposed in each plot. From this we see that the principal\\ncomponents identify the radial structure present in the data. Finally, Figure 6.11 shows'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 261, 'page_label': '244'}, page_content='244 Kernel PCA\\nthe projections [z1(xi),z2(xi)]⊤,i = 1,..., 200 of the original data points onto the first two\\nprincipal components. We see that the projected points can be separated by a straight line,\\nwhereas this is not possible for the original data; see also, Example 7.6 for a related prob-☞272\\nlem.\\nFigure 6.10: First nine eigenfunctions using a Gaussian kernel for the two-dimensional\\ndata set formed by the red and cyan points.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 262, 'page_label': '245'}, page_content='Regularization and Kernel Methods 245\\n-0.4 -0.2 0 0.2 0.4 0.6 0.8\\n-0.8\\n-0.6\\n-0.4\\n-0.2\\n0\\n0.2\\n0.4\\n0.6\\nFigure 6.11: Projection of the data onto the first two principal components. Observe that\\nalready the projections of the inner and outer points are well separated.\\nFurther Reading\\nFor a good overview of the ridge regression and the lasso, we refer the reader to [36, 56].\\nFor overviews of the theory of RKHS we refer to [3, 115, 126], and for in-depth background\\non splines and their connection to RKHSs we refer to [123]. For further details on GP\\nregression we refer to [97] and for kernel PCA in particular we refer to [12, 92]. Finally,\\nmany facts about kernels and their corresponding RKHSs can be found in [115].\\nExercises\\n1. Let Gbe an RKHS with reproducing kernel κ. Show that κis a positive semidefinite\\nfunction.\\n2. Show that a reproducing kernel, if it exists, is unique.\\n3. Let Gbe a Hilbert space of functions g : X→ R. Recall that the evaluation func-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 262, 'page_label': '245'}, page_content='3. Let Gbe a Hilbert space of functions g : X→ R. Recall that the evaluation func-\\ntional is the map δx : g 7→g(x) for a given x ∈X. Show that evaluation functionals\\nare linear operators.\\n4. Let G0 be the pre-RKHS G0 constructed in the proof of Theorem 6.2. Thus, g ∈G0\\nis of the form g = Pn\\ni=1 αi κxi and\\n⟨g,κx⟩G0 =\\nnX\\ni=1\\nαi ⟨κxi ,κx⟩G0 =\\nnX\\ni=1\\nαi κ(xi,x) = g(x).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 263, 'page_label': '246'}, page_content='246 Exercises\\nTherefore, we may write the evaluation functional of g ∈G0 at x as δxg := ⟨g,κx⟩G0 .\\nShow that δx is bounded on G0 for every x; that is, |δx f |<γ ∥f ∥G0 , for some γ< ∞.\\n5. Continuing Exercise 4, let ( fn) be a Cauchy sequence in G0 such that |fn(x)|→ 0 for\\nall x. Show that ∥fn∥G0 →0.\\n6. Continuing Exercises 5 and 4, to show that the inner product (6.14) is well defined,\\na number of facts have to be checked.\\n(a) Verify that the limit converges.\\n(b) Verify that the limit is independent of the Cauchy sequences used.\\n(c) Verify that the properties of an inner product are satisfied. The only non-trivial\\nproperty to verify is that ⟨f, f ⟩G= 0 if and only if f = 0.\\n7. Exercises 4–6 show that Gdefined in the proof of Theorem 6.2 is an inner product\\nspace. It remains to prove thatGis an RKHS. This requires us to prove that the inner\\nproduct space Gis complete (and thus Hilbert), and that its evaluation functionals'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 263, 'page_label': '246'}, page_content='product space Gis complete (and thus Hilbert), and that its evaluation functionals\\nare bounded and hence continuous (see Theorem A.16). This is done in a number of☞389\\nsteps.\\n(a) Show that G0 is dense in Gin the sense that every f ∈G is a limit point (with\\nrespect to the norm on G) of a Cauchy sequence ( fn) in G0.\\n(b) Show that every evaluation functional δx on Gis continuous at the 0 function.\\nThat is,\\n∀ε> 0 : ∃δ> 0 : ∀f ∈G : ∥f ∥G<δ ⇒|f (x)|<ε. (6.40)\\nContinuity of δx at all functionsg ∈Gthen follows automatically from linearity.\\n(c) Show that Gis complete; that is, every Cauchy sequence (fn) ∈G converges in\\nthe norm ||·||G.\\n8. If κ1 and κ2 are kernels on Xand Y, then κ+((x,y),(x′,y′)) : = κ1(x,x′) + κ2(y,y′)\\nand κ×((x,y),(x′,y′) := κ1(x,x′)κ2(y,y′) are kernels on the Cartesian productX×Y.\\nProve this.\\n9. An RKHS enjoys the following desirable smoothness property: if ( gn) is a sequence\\nbelonging to RKHS Gon X, and ∥gn −g∥G→0, then g(x) = limn gn(x) for all x ∈X.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 263, 'page_label': '246'}, page_content='belonging to RKHS Gon X, and ∥gn −g∥G→0, then g(x) = limn gn(x) for all x ∈X.\\nProve this, using Cauchy–Schwarz.\\n10. Let X be an Rd-valued random variable that is symmetric about the origin (that is,\\nX and (−X) are identically distributed). Denote by µ is its distribution and ψ(t) =\\nEeit⊤X =\\nR\\neit⊤x µ(dx) for t ∈Rd is its characteristic function. Verify that κ(x,x′) =\\nψ(x −x′) is a real-valued positive semidefinite function.\\n11. Suppose an RKHS Gof functions from X→ R(with kernel κ) is invariant under a\\ngroup Tof transformations T : X→X ; that is, for all f,g ∈G and T ∈T, we have\\n(i) f ◦T ∈G and (ii) ⟨f ◦T,g ◦T⟩G = ⟨f,g⟩G. Show that κ(T x,T x′) = κ(x,x′) for\\nall x,x′∈X and T ∈T.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 264, 'page_label': '247'}, page_content='Regularization and Kernel Methods 247\\n12. Given two Hilbert spaces Hand G, we call a mapping A : H→G a Hilbert space\\nisomorphism Hilbert space\\nisomorphism\\nif it is\\n(i) a linear map; that is, A(a f +bg) = aA( f ) +bA(g) for any f,g ∈H and a,b ∈R.\\n(ii) a surjective map; and\\n(iii) an isometry; that is, for all f,g ∈H, it holds that ⟨f,g⟩H = ⟨A f,Ag⟩G.\\nLet H= Rp (equipped with the usual Euclidean inner product) and construct its\\n(continuous) dual space G, consisting of all continuous linear functions from Rp to\\nR, as follows: (a) For each β∈Rp, define gβ : Rp →Rvia gβ(x) = ⟨β,x⟩= β⊤x, for\\nall x ∈Rp. (b) Equip Gwith the inner product ⟨gβ,gγ⟩G:= β⊤γ.\\nShow that A : H→G defined by A(β) = gβ for β∈Rp is a Hilbert space isomorph-\\nism.\\n13. Let X be an n ×p model matrix. Show that X⊤X + n γIp for γ> 0 is invertible.\\n14. As Example 6.8 clearly illustrates, the pdf of a random variable that is symmetric\\nabout the origin is not in general a valid reproducing kernel. Take two such iid ran-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 264, 'page_label': '247'}, page_content='about the origin is not in general a valid reproducing kernel. Take two such iid ran-\\ndom variables X and X′ with common pdf f , and define Z = X + X′. Denote by ψZ\\nand fZ the characteristic function and pdf of Z, respectively.\\nShow that if ψZ is in L1(R), fZ is a positive semidefinite function. Use this to show\\nthat κ(x,x′) = fZ(x −x′) = 1{|x −x′|⩽2}(1 −|x −x′|/2) is a valid reproducing kernel.\\n15. For the smoothing cubic spline of Section 6.6, show that κ(x,u) = max{x,u}min{x,u}2\\n2 −\\nmin{x,u}3\\n6 .\\n16. Let X be an n ×p model matrix and let u ∈Rp be the unit-length vector with k-th\\nentry equal to one (uk = ∥u∥= 1). Suppose that the k-th column of X is v and that it\\nis replaced with a new predictor w, so that we obtain the new model matrix:\\neX = X + (w −v)u⊤.\\n(a) Denoting\\nδ:= X⊤(w −v) + ∥w −v∥2\\n2 u,\\nshow that\\neX\\n⊤eX = X⊤X + uδ⊤+ δu⊤= X⊤X + (u + δ)(u + δ)⊤\\n2 −(u −δ)(u −δ)⊤\\n2 .\\nIn other words, eX\\n⊤eX differs from X⊤X by a symmetric matrix of rank two.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 264, 'page_label': '247'}, page_content='2 −(u −δ)(u −δ)⊤\\n2 .\\nIn other words, eX\\n⊤eX differs from X⊤X by a symmetric matrix of rank two.\\n(b) Suppose that B := (X⊤X + n γIp)−1 is already computed. Explain how the\\nSherman–Morrison formulas in Theorem A.10 can be applied twice to com- ☞ 371\\npute the inverse and log-determinant of the matrix eX\\n⊤eX + n γIp in O((n + p)p)\\ncomputing time, rather than the usual O((n + p2)p) computing time.3\\n3This Sherman–Morrison updating is not always numerically stable. A more numerically stable method\\nwill perform two consecutive rank-one updates of the Cholesky decomposition of X⊤X + n γIp.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 265, 'page_label': '248'}, page_content='248 Exercises\\n(c) Write a Python program for updating a matrix B = (X⊤X + n γIp)−1 when we\\nchange the k-th column of X, as shown in the following pseudo-code.\\nAlgorithm 6.8.1: Updating via Sherman–Morrison Formula\\ninput: Matrices X and B, index k, and replacement w for the k-th column of X.\\noutput: Updated matrices X and B.\\n1 Set v ∈Rn to be the k-th column of X.\\n2 Set u ∈Rp to be the unit-length vector such that uk = ∥u∥= 1.\\n3 B ←B − Buδ⊤B\\n1 + δ⊤Bu\\n4 B ←B − Bδu⊤B\\n1 + u⊤Bδ\\n5 Update the k-th column of X with w.\\n6 return X,B\\n17. Use Algorithm 6.8.1 from Exercise 16 to write Python code that computes the ridge\\nregression coefficient βin (6.5) and use it to replicate the results on Figure 6.1. The☞217\\nfollowing pseudo-code (with running cost ofO((n+ p)p2)) may help with the writing\\nof the Python code.\\nAlgorithm 6.8.2: Ridge Regression Coefficients via Sherman–Morrison Formula\\ninput: Training set {X,y}and regularization parameter γ> 0.\\noutput: Solution bβ= (n γIp + X⊤X)−1X⊤y.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 265, 'page_label': '248'}, page_content='output: Solution bβ= (n γIp + X⊤X)−1X⊤y.\\n1 Set A to be an n ×p matrix of zeros and B ←(n γIp)−1.\\n2 for j = 1,..., p do\\n3 Set w to be the j-th column of X.\\n4 Update {A,B}via Algorithm 6.8.1 with inputs {A,B, j,w}.\\n5 bβ←B(X⊤y)\\n6 return bβ\\n18. Consider Example 2.10 with D = diag(λ1,...,λ p) for some nonnegative vector λ ∈☞56\\nRp, so that twice the negative logarithm of the model evidence can be written as\\n−2 lng(y) = l(λ) := n ln[y⊤(I −XΣX⊤)y] + ln |D|−ln |Σ|+ c,\\nwhere c is a constant that depends only on n.\\n(a) Use the Woodbury identities (A.15) and (A.16) to show that☞371\\nI −XΣX⊤= (I + XDX⊤)−1\\nln |D|−ln |Σ|= ln |I + XDX⊤|.\\nDeduce that l(λ) = n ln[y⊤Cy] −ln |C|+ c, where C := (I + XDX⊤)−1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 266, 'page_label': '249'}, page_content='Regularization and Kernel Methods 249\\n(b) Let [ v1,..., vp] := X denote the p columns/predictors of X. Show that\\nC−1 = I +\\npX\\nk=1\\nλkvkv⊤\\nk .\\nExplain why setting λk = 0 has the effect of excluding the k-th predictor from\\nthe regression model. How can this observation be used for model selection?\\n(c) Prove the following formulas for the gradient and Hessian elements of l(λ):\\n∂l\\n∂λi\\n= v⊤\\ni Cvi −n(v⊤\\ni Cy)2\\ny⊤Cy\\n∂2l\\n∂λi ∂λj\\n= (n −1)(v⊤\\ni Cvj)2 −n\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0v⊤\\ni Cvj −\\n(v⊤\\ni Cy)(v⊤\\nj Cy)\\ny⊤Cy\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n2\\n.\\n(6.41)\\n(d) One method to determine which predictors in X are important is to compute\\nλ∗:= argmin\\nλ⩾0\\nl(λ)\\nusing, for example, the interior-point minimization Algorithm B.4.1 with gradi- ☞ 419\\nent and Hessian computed from (6.41). Write Python code to compute λ∗ and\\nuse it to select the best polynomial model in Example 2.10.\\n19. (Exercise 18 continued.) Consider again Example 2.10 with D = diag(λ1,...,λ p) for ☞ 56\\nsome nonnegative model-selection parameter λ∈Rp. A Bayesian choice for λis the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 266, 'page_label': '249'}, page_content='some nonnegative model-selection parameter λ∈Rp. A Bayesian choice for λis the\\nmaximizer of the marginal likelihood g(y |λ); that is,\\nλ∗= argmax\\nλ⩾0\\n\"\\ng(β,σ2,y |λ) dβdσ2,\\nwhere\\nln g(β,σ2,y |λ) = −∥y −Xβ∥2 + β⊤D−1β\\n2σ2 −1\\n2 ln |D|− n + p\\n2 ln(2πσ2) −ln σ2.\\nTo maximize g(y |λ), one can use the EM algorithm with βand σ2 acting as latent ☞ 128\\nvariables in the complete-data log-likelihood ln g(β,σ2,y |λ). Define\\nΣ := (D−1 + X⊤X)−1\\nβ:= ΣX⊤y\\nbσ2 :=\\n\\x10\\n∥y∥2 −y⊤Xβ\\n\\x11\\x0en.\\n(6.42)\\n(a) Show that the conditional density of the latent variables βand σ2 is such that\\n\\x10\\nσ−2 \\x0c\\x0c\\x0cλ,y\\n\\x11\\n∼Gamma\\n\\x12n\\n2, n\\n2 bσ2\\n\\x13\\n\\x10\\nβ\\n\\x0c\\x0c\\x0cλ,σ2,y\\n\\x11\\n∼N\\n\\x10\\nβ, σ2Σ\\n\\x11\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 267, 'page_label': '250'}, page_content='250 Exercises\\n(b) Use Theorem C.2 to show that the expected complete-data log-likelihood is☞430\\n−β\\n⊤\\nD−1β\\n2bσ2 −tr(D−1Σ) + ln |D|\\n2 + c1,\\nwhere c1 is a constant that does not depend on λ.\\n(c) Use Theorem A.2 to simplify the expected complete-data log-likelihood and to☞359\\nshow that it is maximized at λi = Σii + (βi/bσ)2 for i = 1,..., p.Hence, deduce\\nthe following E and M steps in the EM algorithm:\\nE-step. Given λ, update (Σ,β,bσ2) via the formulas (6.42).\\nM-step. Given (Σ,β,bσ2), update λvia λi = Σii + (βi/bσ)2, i = 1,..., p.\\n(d) Write Python code to compute λ∗ via the EM algorithm, and use it to select\\nthe best polynomial model in Example 2.10. A possible stopping criterion is to\\nterminate the EM iterations when\\nln g(y |λt+1) −ln g(y |λt) <ε\\nfor some small ε> 0, where the marginal log-likelihood is\\nln g(y |λ) = −n\\n2 ln(nπbσ2) −1\\n2 ln |D|+ 1\\n2 ln |Σ|+ ln Γ(n/2).\\n20. In this exercise we explore how the early stopping of the gradient descent iterations\\n(see Example B.10),☞412'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 267, 'page_label': '250'}, page_content='(see Example B.10),☞412\\nxt+1 = xt −α∇f (xt), t = 0,1,...,\\nis (approximately) equivalent to the global minimization of f (x) + 1\\n2 γ∥x∥2 for certain\\nvalues of the ridge regularization parameter γ >0 (see Example 6.1). We illustrate\\nthe early stoppingearly stopping idea on the quadratic function f (x) = 1\\n2 (x −µ)⊤H(x −µ), where\\nH ∈Rn×n is a symmetric positive-definite (Hessian) matrix with eigenvalues {λk}n\\nk=1.\\n(a) Verify that for a symmetric matrix A ∈Rn such that I −A is invertible, we have\\nI + A + ··· + At−1 = (I −At)(I −A)−1.\\n(b) Let H = QΛQ⊤ be the diagonalization of H as per Theorem A.8. If x0 = 0,☞366\\nshow that the formula for xt is\\nxt = µ−Q(I −αΛ)tQ⊤µ.\\nHence, deduce that a necessary condition for xt to converge is α< 2/maxk λk.\\n(c) Show that the minimizer of f (x) + 1\\n2 γ∥x∥2 can be written as\\nx∗= µ−Q(I + γ−1Λ)−1Q⊤µ.\\n(d) For a fixed value of t, let the learning rate α↓0. Using part (b) and (c), show\\nthat if γ≃1/(t α) as α↓0, then xt ≃x∗. In other words, xt is approximately'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 267, 'page_label': '250'}, page_content='that if γ≃1/(t α) as α↓0, then xt ≃x∗. In other words, xt is approximately\\nequal to x∗for small α, provided that γis inversely proportional to t α.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 268, 'page_label': '251'}, page_content='CHAPTER 7\\nCLASSIFICATION\\nThe purpose of this chapter is to explain the mathematical ideas behind well-known\\nclassification techniques such as the naïve Bayes method, linear and quadratic discrim-\\ninant analysis, logistic /softmax classification, the K-nearest neighbors method, and\\nsupport vector machines.\\n7.1 Introduction\\nClassification methods are supervised learning methods in which a categorical response\\nvariable Y takes one of c possible values (for example whether a person is sick or healthy),\\nwhich is to be predicted from a vector X of explanatory variables (for example, the blood\\npressure, age, and smoking status of the person), using a prediction function g . In this\\nsense, g classifies the input X into one of the classes, say in the set {0,..., c −1}. For this\\nreason, we will call g a classification function or simply classifier classifier. As with any supervised\\nlearning technique (see Section 2.3), the goal is to minimize the expected loss or risk\\nℓ(g) = ELoss(Y,g(X)) (7.1)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 268, 'page_label': '251'}, page_content='ℓ(g) = ELoss(Y,g(X)) (7.1)\\nfor some loss function, Loss(y,by), that quantifies the impact of classifying a response y via\\nby = g(x). The natural loss function is the zero–one (also written 0–1) or indicator loss indicator loss:\\nLoss(y,by) : = 1{y , by}; that is, there is no loss for a correct classification ( y = by) and a\\nunit loss for a misclassification (y , by). In this case the optimal classifier g∗is given in the\\nfollowing theorem.\\nTheorem 7.1: Optimal classifier\\nFor the loss function Loss(y,by) = 1{y , by}, an optimal classification function is\\ng∗(x) = argmax\\ny∈{0,...,c−1}\\nP[Y = y |X = x]. (7.2)\\nProof: The goal is to minimize ℓ(g) = E1{Y , g(X)}over all functions g taking values in\\n{0,..., c −1}. Conditioning on X gives, by the tower property,ℓ(g) = E(P[Y , g(X) |X] ), ☞ 431\\nand so minimizing ℓ(g) with respect to g can be accomplished by maximizing P[Y =\\n251'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 269, 'page_label': '252'}, page_content='252 Introduction\\ng(x) |X = x] with respect to g(x), for every fixed x. In other words, take g(x) to be equal\\nto the class label y for which P[Y = y |X = x] is maximal. □\\nThe formulation (7.2) allows for “ties”, when there is an equal probability between\\noptimal classes for a feature vector x. Assigning one of these tied classes arbitrarily (or\\nrandomly) to x does not affect the loss function and so we assume for simplicity that g∗(x)\\nis always a scalar value.\\nNote that, as was the case for the regression (see, e.g., Theorem 2.1), the optimal pre-☞21\\ndiction function depends on the conditional pdf f (y |x) = P[Y = y |X = x]. However, since\\nwe assign x to class y if f (y |x) ⩾f (z |x) for all z, we do not need to learn the entire sur-\\nface of the function f (y |x); we only need to estimate it well enough near the decision\\nboundary {x : f (y |x) = f (z |x)}for any choice of classes y and z. This is because the as-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 269, 'page_label': '252'}, page_content='boundary {x : f (y |x) = f (z |x)}for any choice of classes y and z. This is because the as-\\nsignment (7.2) divides the feature space into c regions, Ry = {x : f (y |x) = maxz f (z |x)},\\ny = 0,..., c −1.\\nRecall that for any supervised learning problem the smallest possible expected loss\\n(that is, the irreducible risk) is given by ℓ∗ = ℓ(g∗). For the indicator loss, the irreducible\\nrisk is equal to P[Y , g∗(X)]. This smallest possible probability of misclassification is\\noften called the Bayes error rateBayes error\\nrate\\n.\\nFor a given training set τ, a classifier is often derived from apre-classifier gτ, which\\nis a prediction function (learner) that can take any real value, rather than only values\\nin the set of class labels. A typical situation is the case of binary classification with\\nlabels −1 and 1, where the prediction function gτ is a function taking values in the\\ninterval [−1,1] and the actual classifier is given by sign( gτ). It will be clear from'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 269, 'page_label': '252'}, page_content='interval [−1,1] and the actual classifier is given by sign( gτ). It will be clear from\\nthe context whether a prediction function gτ should be interpreted as a classifier or\\npre-classifier.\\nThe indicator loss function may not always be the most appropriate choice of loss\\nfunction for a given classification problem. For example, when diagnosing an illness, the\\nmistake in misclassifying a person as being sick when in fact the person is healthy may\\nbe less serious than classifying the person as healthy when in fact the person is sick. In\\nSection 7.2 we consider various classification metrics.\\nThere are many ways to fit a classifier to a training set τ= {(x1,y1),..., (xn,yn)}. The\\napproach taken in Section 7.3 is to use a Bayesian framework for classification. Here the\\nconditional pdf f (y |x) is viewed as a posterior pdf f (y |x) ∝f (x |y) f (y) for a given class\\nprior f (y) and likelihood f (x |y). Section 7.4 discusses linear and quadratic discriminant'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 269, 'page_label': '252'}, page_content='prior f (y) and likelihood f (x |y). Section 7.4 discusses linear and quadratic discriminant\\nanalysis for classification, which assumes that the class of approximating functions for the\\nconditional pdf f (x |y) is a parametric class Gof Gaussian densities. As a result of this\\nchoice of G, the marginal f (x) is approximated via a Gaussian mixture density.\\nIn contrast, in the logistic or soft-max classification in Section 7.5, the conditional\\npdf f (y |x) is approximated using a more flexible class of approximating functions. As a\\nresult of this, the approximation to the marginal density f (x) does not belong to a simple\\nparametric class (such as a Gaussian mixture). As in unsupervised learning, the cross-\\nentropy loss is the most common choice for training the learner.\\nThe K-nearest neighbors method, discussed in Section 7.6, is yet another approach to\\nclassification that makes minimal assumptions on the class G. Here the aim is to directly'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 270, 'page_label': '253'}, page_content='Classification 253\\nestimate the conditional pdf f (y |x) from the training data, using only feature vectors in\\nthe neighborhood of x. In Section 7.7 we explain the support vector methodology for clas-\\nsification; this is based on the same Reproducing Kernel Hilbert Space ideas that proved\\nsuccessful for regression analysis in Section 6.3. Finally, a versatile way to do both clas- ☞ 222\\nsification and regression is to use classification and regression trees. This is the topic of\\nChapter 8. Neural networks (Chapter 9) provide yet another way to perform classification. ☞ 287\\n☞ 323\\n7.2 Classification Metrics\\nThe effectiveness of a classifierg is, theoretically, measured in terms of the risk (7.1), which\\ndepends on the loss function used. Fitting a classifier to iid training data τ= {(xi,yi)}n\\ni=1 is\\nestablished by minimizing the training loss\\nℓτ(g) = 1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) (7.3)\\nover some class of functions G. As the training loss is often a poor estimator of the risk,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 270, 'page_label': '253'}, page_content='over some class of functions G. As the training loss is often a poor estimator of the risk,\\nthe risk is usually estimated as in (7.3), using instead a test set τ′ = {(x′\\ni ,y′\\ni )}n′\\ni=1}that is\\nindependent of the training set, as explained in Section 2.3. To measure the performance ☞ 23\\nof a classifier on a training or test set, it is convenient to introduce the notion of a loss\\nmatrix loss matrix. Consider a classification problem with classifier g, loss function Loss, and classes\\n0,..., c −1. If an input feature vector x is classified as by = g(x) when the observed class\\nis y, the loss incurred is, by definition, Loss( y,by). Consequently, we may identify the loss\\nfunction with a matrix L = [Loss( j,k), j,k ∈{0,..., c −1}]. For the indicator loss function,\\nthe matrix L has 0s on the diagonal and 1s everywhere else. Another useful matrix is the\\nconfusion matrix confusion\\nmatrix\\n, denoted by M, where the ( j,k)-th element of M counts the number of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 270, 'page_label': '253'}, page_content='matrix\\n, denoted by M, where the ( j,k)-th element of M counts the number of\\ntimes that, for the training or test data, the actual (observed) class isj whereas the predicted\\nclass is k. Table 7.1 shows the confusion matrix of some Dog/Cat/Possum classifier.\\nTable 7.1: Confusion matrix for three classes.\\nPredicted\\nActual Dog Cat Possum\\nDog 30 2 6\\nCat 8 22 15\\nPossum 7 4 41\\nWe can now express the classifier performance (7.3) in terms of L and M as\\n1\\nn\\nX\\nj,k\\n[L ⊙M]jk, (7.4)\\nwhere L ⊙M is the elementwise product of L and M. Note that for the indicator loss, (7.4)\\nis simply 1−tr(M)/n, and is called the misclassification error. The expression (7.4) makes misclassification\\nerrorit clear that both the counts and the loss are important in determining the performance of a\\nclassifier.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 271, 'page_label': '254'}, page_content='254 Classification Metrics\\nIn the spirit of Table C.4 for hypothesis testing, it is sometimes useful to divide the☞459\\nelements of a confusion matrix into four groups. The diagonal elements are thetrue positivetrue positive\\ncounts; that is, the numbers of correct classifications for each class. The true positive counts\\nfor the Dog, Cat, and Possum classes in Table 7.1 are 30,22, and 41, respectively. Similarly,\\nthe true negativetrue negative count for a class is the sum of all matrix elements that do not belong to the\\nrow or the column of this particular class. For the Dog class it is 22+15 +4 +41 = 82. The\\nfalse positivefalse positive count for a class is the sum of the corresponding column elements without\\nthe diagonal element. For the Dog class it is 8 + 7 = 15. Finally, the false negativefalse negative count\\nfor a specific class, can be calculated by summing over the corresponding row elements\\n(again, without counting the diagonal element). For the Dog class it is 2 + 6 = 8.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 271, 'page_label': '254'}, page_content='(again, without counting the diagonal element). For the Dog class it is 2 + 6 = 8.\\nIn terms of the elements of the confusion matrix, we have the following counts for class\\nj = 0,..., c −1:\\nTrue positive tp j = Mj j,\\nFalse positive fp j =\\nX\\nk,j\\nMk j, (column sum)\\nFalse negative fn j =\\nX\\nk,j\\nMjk, (row sum)\\nTrue negative tn j = n −fnj −fpj −tpj.\\nNote that in the binary classification case (c = 2), and using the indicator loss function,\\nthe misclassification error (7.4) can be written as\\nerrorj =\\nfpj + fnj\\nn . (7.5)\\nThis does not depend on which of the two classes is considered, as fp 0 + fn0 = fp1 + fn1.\\nSimilarly, the accuracyaccuracy measures the fraction of correctly classified objects:\\naccuracyj = 1 −errorj =\\ntpj + tnj\\nn . (7.6)\\nIn some cases, classification error (or accuracy) alone is not su fficient to adequately\\ndescribe the effectiveness of a classifier. As an example, consider the following two classi-\\nfication problems based on a fingerprint detection system:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 271, 'page_label': '254'}, page_content='fication problems based on a fingerprint detection system:\\n1. Identification of authorized personnel in a top-secret military facility.\\n2. Identification to get an online discount for some retail chain.\\nBoth problems are binary classification problems. However, a false positive in the first\\nproblem is extremely dangerous, while a false positive in the second problem will make\\na customer happy. Let us examine a classifier in the top-secret facility. The corresponding\\nconfusion matrix is given in Table 7.2.\\nTable 7.2: Confusion matrix for authorized personnel classification.\\nPredicted\\nActual authorized non-authorized\\nauthorized 100 400\\nnon-authorized 50 100,000'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 272, 'page_label': '255'}, page_content='Classification 255\\nFrom (7.6), we conclude that the accuracy of classification is equal to\\naccuracy = tp + tn\\ntp + tn + fp + fn = 100 + 100,000\\n100 + 100,000 + 50 + 400 ≈99.55%.\\nHowever, we can see that in this particular case, accuracy is a problematic metric, since the\\nalgorithm allowed 50 non-authorized personnel to enter the facility. One way to deal with\\nthis issue is to modify the loss function to give a much higher loss to non-authorized access.\\nThus, instead of an (indicator) loss matrix, we could for example take the loss matrix\\nL =\\n 0 1\\n1000 0\\n!\\n.\\nAn alternative approach is to keep the indicator loss function and consider additional clas-\\nsification metrics. Below we give a list of commonly used metrics. For simplicity we call\\nan object whose actual class is j a “ j-object”.\\n• The precision precision(also called positive predictive value ) is the fraction of all objects\\nclassified as j that are actually j-objects. Specifically,\\nprecisionj =\\ntpj\\ntpj + fpj\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 272, 'page_label': '255'}, page_content='classified as j that are actually j-objects. Specifically,\\nprecisionj =\\ntpj\\ntpj + fpj\\n.\\n• The recall recall(also called sensitivity) is the fraction of all j-objects that are correctly\\nclassified as such. That is,\\nrecallj =\\ntpj\\ntpj + fnj\\n.\\n• The specificity specificitymeasures the fraction of all non-j-objects that are correctly classified\\nas such. Specifically,\\nspecificityj = tnj\\nfpj + tnj\\n.\\n• The Fβ score Fβ scoreis a combination of the precision and the recall and is used as a single\\nmeasurement for a classifier’s performance. The Fβ score is given by\\nFβ,j =\\n(β2 + 1) tpj\\n(β2 + 1) tpj + β2 fnj + fpj\\n.\\nFor β= 0 we obtain the precision and for β→∞ we obtain the recall.\\nThe particular choice of metric is clearly application dependent. For example, in the\\nclassification of authorized personnel in a top-secret military facility, suppose we have\\ntwo classifiers. The first (Classifier 1) has a confusion matrix given in Table 7.2, and the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 272, 'page_label': '255'}, page_content='two classifiers. The first (Classifier 1) has a confusion matrix given in Table 7.2, and the\\nsecond (Classifier 2) has a confusion matrix given in Table 7.3. Various metrics for these\\ntwo classifiers are show in Table 7.4. In this case we prefer Classifier 1, which has a much\\nhigher precision.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 273, 'page_label': '256'}, page_content='256 Classification Metrics\\nTable 7.3: Confusion matrix for authorized personnel classification, using a different clas-\\nsifier (Classifier 2).\\nPredicted\\nActual Authorized Non-Authorized\\nauthorized 50 10\\nnon-authorized 450 100,040\\nTable 7.4: Comparing the metrics for the confusion matrices in Tables 7.2 and 7.3.\\nMetric Classifier 1 Classifier 2\\naccuracy 9 .955 ×10−1 9.954 ×10−1\\nprecision 6 .667 ×10−1 1.000 ×10−1\\nrecall 2 .000 ×10−1 8.333 ×10−1\\nspecificity 9 .995 ×10−1 9.955 ×10−1\\nF1 3.077 ×10−1 1.786 ×10−1\\nRemark 7.1 (Multilabel and Hierarchical Classification) In standard classification\\nthe classes are assumed to be mutually exclusive. For example a satellite image could\\nbe classified as “cloudy”, “clear”, or “foggy”. Inmultilabel classificationmultilabel\\nclassification\\nthe classes (often\\ncalled labels) do not have to be mutually exclusive. In this case the response is a subset\\nYof some collection of labels {0,..., c −1}. Equivalently, the response can be viewed as'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 273, 'page_label': '256'}, page_content='Yof some collection of labels {0,..., c −1}. Equivalently, the response can be viewed as\\na binary vector of length c, where the y-th element is 1 if the response belongs to label y\\nand 0 otherwise. Again, consider the satellite image example and add two labels, such as\\n“road” and “river” to the previous three labels. Clearly, an image can contain both a road\\nand a river. In addition, the image can be clear, cloudy, or foggy.\\nIn hierarchical classificationhierarchical\\nclassification\\na hierarchical relation between classes/labels is taken into\\naccount during the classification process. Usually, the relations are modeled via a tree or a\\ndirected acyclic graph. A visual comparison between the hierarchical and non-hierarchical\\n(flat) classification tasks for satellite image data is presented in Figure 7.1.\\nroot\\nrural\\nfarm barn\\nurban\\nskyscraper\\nroot\\nrural barn farm urban skyscraper\\nFigure 7.1: Hierarchical (left) and non-hierarchical (right) classification schemes. Barns'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 273, 'page_label': '256'}, page_content='Figure 7.1: Hierarchical (left) and non-hierarchical (right) classification schemes. Barns\\nand farms are common in rural areas, while skyscrapers are generally located in cities.\\nWhile this relation can be clearly observed in the hierarchical model scheme, the connec-\\ntion is missing in the non-hierarchical design.\\nIn multilabel classification, both the prediction bY:= g(x) and the true response Yare\\nsubsets of the label set{0,..., c−1}. A reasonable metric is the so-calledexact match ratioexact match\\nratio\\n,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 274, 'page_label': '257'}, page_content='Classification 257\\ndefined as\\nexact match ratio =\\nPn\\ni=1 1{bYi = Yi}\\nn .\\nThe exact match ratio is rather stringent, as it requires a full match. In order to consider\\npartial correctness, the following metrics could be used instead.\\n• The accuracy is defined as the ratio of correctly predicted labels and the total number\\nof predicted and actual labels. The formula is given by\\naccuracy =\\nPn\\ni=1 |Yi ∩bYi|\\nPn\\ni=1 |Yi ∪bYi|\\n.\\n• The precision is defined as the ratio of correctly predicted labels and the total number\\nof predicted labels. Specifically,\\nprecision =\\nPn\\ni=1 |Yi ∩bYi|\\nPn\\ni=1 |bYi|\\n. (7.7)\\n• The recall is defined as the ratio of correctly predicted labels and the total number of\\nactual labels. Specifically,\\nrecall =\\nPn\\ni=1 |Yi ∩bYi|Pn\\ni=1 |Yi| . (7.8)\\n• The Hamming loss counts the average number of incorrect predictions for all classes,\\ncalculated as\\nHamming = 1\\nn c\\nnX\\ni=1\\nc−1X\\ny=0\\n1{y ∈bYi}1{y < Yi}+ 1{y < bYi}1{y ∈Yi}.\\n7.3 Classification via Bayes’ Rule'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 274, 'page_label': '257'}, page_content='n c\\nnX\\ni=1\\nc−1X\\ny=0\\n1{y ∈bYi}1{y < Yi}+ 1{y < bYi}1{y ∈Yi}.\\n7.3 Classification via Bayes’ Rule\\nWe saw from Theorem 7.1 that the optimal classifier for classes 0 ,..., c −1 divides the\\nfeature space into c regions, depending on f (y |x): the conditional pdf of the response Y\\ngiven the feature vector X = x. In particular, if f (y |x) > f (z |x) for all z , y, the feature\\nvector x is classified as y. Classifying feature vectors on the basis of their conditional class\\nprobabilities is a natural thing to do, especially in a Bayesian learning context; see Sec-\\ntion 2.9 for an overview of Bayesian terminology and usage. Specifically, the conditional ☞ 48\\nprobability f (y |x) is interpreted as a posterior probability, of the form\\nf (y |x) ∝f (x |y) f (y), (7.9)\\nwhere f (x |y) is the likelihood of obtaining feature vector x from class y and f (y) is the\\nprior probability1 of class y. By making various modeling assumptions about the prior'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 274, 'page_label': '257'}, page_content='prior probability1 of class y. By making various modeling assumptions about the prior\\n1Here we have used the Bayesian notation convention of “overloading” the notation f .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 275, 'page_label': '258'}, page_content='258 Classification via Bayes’ Rule\\n(e.g., all classes are a priori equally likely) and the likelihood function, one obtains the\\nposterior pdf via Bayes’ formula (7.9). A class by is then assigned to a feature vector x\\naccording to the highest posterior probability; that is, we classify according to the Bayes\\noptimal decision ruleBayes optimal\\ndecision rule\\n:\\nby = argmax\\ny\\nf (y |x), (7.10)\\nwhich is exactly (7.2). Since the discrete density f (y |x), y = 0,..., c −1 is usually not\\nknown, the aim is to approximate it well with a function g(y |x) from some class of func-\\ntions G. Note that in this context, g(·|x) refers to a discrete density (a probability mass\\nfunction) for a given x.\\nSuppose a feature vector x = [x1,..., xp]⊤of p features has to be classified into one of\\nthe classes 0,..., c −1. For example, the classes could be different people and the features\\ncould be various facial measurements, such as the width of the eyes divided by the distance'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 275, 'page_label': '258'}, page_content='could be various facial measurements, such as the width of the eyes divided by the distance\\nbetween the eyes, or the ratio of the nose height and mouth width. In the naïve Bayesna¨ive Bayes\\nmethod, the class of approximating functions Gis chosen such that g(x |y) = g(x1 |y) ···\\ng(xp |y), that is, conditional on the label, all features are independent. Assuming a uniform\\nprior for y, the posterior pdf can thus be written as\\ng(y |x) ∝\\npY\\nj=1\\ng(xj |y),\\nwhere the marginal pdfs g(xj |y), j = 1,..., p belong to a given class of approximating\\nfunctions G. To classify x, simply take the y that maximizes the unnormalized posterior\\npdf.\\nFor instance, suppose that the approximating class Gis such that (Xj |y) ∼N(µy j,σ2),\\ny = 0,..., c −1, j = 1,..., p. The corresponding posterior pdf is then\\ng(y |θ,x) ∝exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−1\\n2\\npX\\nj=1\\n(xj −µy j)2\\nσ2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 = exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−1\\n2\\n∥x −µy∥2\\nσ2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,\\nwhere µy := [µy1,...,µ yp ]⊤ and θ := {µ0,..., µc−1,σ2}collects all model parameters. The'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 275, 'page_label': '258'}, page_content='\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,\\nwhere µy := [µy1,...,µ yp ]⊤ and θ := {µ0,..., µc−1,σ2}collects all model parameters. The\\nprobability g(y |θ,x) is maximal when ∥x −µy∥is minimal. Thus by = argminy ∥x −µy∥is\\nthe classifier that maximizes the posterior probability. That is, classify x as y when µy is\\nclosest to x in Euclidean distance. Of course, the parameters (here, the {µy}and σ2) are\\nunknown and have to be estimated from the training data.\\nWe can extend the above idea to the case where also the variance σ2 depends on the\\nclass y and feature j, as in the next example.\\nExample 7.1 (Naïve Bayes Classification) Table 7.5 lists the meansµand standard de-\\nviations σof p = 3 normally distributed features, for c = 4 different classes. How should\\na feature vector x = [1.67,2.00,4.23]⊤be classified? The posterior pdf is\\ng(y |θ,x) ∝(σy1σy2σy3)−1 exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−1\\n2\\n3X\\nj=1\\n(xj −µy j)2\\nσ2\\ny j\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,\\nwhere θ:= {σj,µj}c−1\\nj=0 again collects all model parameters. The (unscaled) values for'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 275, 'page_label': '258'}, page_content='\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,\\nwhere θ:= {σj,µj}c−1\\nj=0 again collects all model parameters. The (unscaled) values for\\ng(y |θ,x), y = 0,1,2,3 are 53.5, 0.24, 8.37, and 3.5 ×10−6, respectively. Hence, the feature\\nvector should be classified as 0. The code follows.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 276, 'page_label': '259'}, page_content=\"Classification 259\\nTable 7.5: Feature parameters.\\nFeature 1 Feature 2 Feature 3\\nClass µ σ µ σ µ σ\\n0 1.6 0.1 2.4 0.5 4.3 0.2\\n1 1.5 0.2 2.9 0.6 6.1 0.9\\n2 1.8 0.3 2.5 0.3 4.2 0.3\\n3 1.1 0.2 3.1 0.7 5.6 0.3\\nnaiveBayes.py\\nimport numpy as np\\nx = np.array([1.67,2,4.23]).reshape(1,3)\\nmu = np.array([1.6, 2.4, 4.3,\\n1.5, 2.9, 6.1,\\n1.8, 2.5, 4.2,\\n1.1, 3.1, 5.6]).reshape(4,3)\\nsig = np.array([0.1, 0.5, 0.2,\\n0.2, 0.6, 0.9,\\n0.3, 0.3, 0.3,\\n0.2, 0.7, 0.3]).reshape(4,3)\\ng = lambda y: 1/np.prod(sig[y,:]) * np.exp(\\n-0.5*np. sum ((x-mu[y,:])**2/sig[y,:]**2));\\nfor y in range (0,4):\\nprint ('{:3.2e} '.format (g(y)))\\n5.35e+01\\n2.42e-01\\n8.37e+00\\n3.53e-06\\n7.4 Linear and Quadratic Discriminant Analysis\\nThe Bayesian viewpoint for classification of the previous section (not limited to naïve\\nBayes) leads in a natural way to the well-established technique of discriminant analysis discriminant\\nanalysis\\n.\\nWe discuss the binary classification case first, with classes 0 and 1.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 276, 'page_label': '259'}, page_content='analysis\\n.\\nWe discuss the binary classification case first, with classes 0 and 1.\\nWe consider a class of approximating functions Gsuch that, conditional on the class\\ny ∈{0,1}, the feature vector X = [X1,..., Xp]⊤has a N(µy,Σy) distribution (see (2.33)): ☞ 46\\ng(x |θ,y) = 1p(2π)p |Σy|\\ne−1\\n2 (x−µy)⊤Σ−1\\ny (x−µy), x ∈Rp, y ∈{0,1}, (7.11)\\nwhere θ = {αj,µj,Σj}c−1\\nj=0 collects all model parameters, including the probability vector α\\n(that is, P\\ni αi = 1 and αi ⩾0) which helps define the prior density: g(y |θ) = αy, y ∈{0,1}.\\nThen, the posterior density is\\ng(y |θ,x) ∝αy ×g(x |θ,y),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 277, 'page_label': '260'}, page_content='260 Linear and Quadratic Discriminant Analysis\\nand, according to the Bayes optimal decision rule (7.10), we classify x to come from class\\n0 if α0g(x |θ,0) >α1g(x |θ,1) or, equivalently (by taking logarithms) if,\\nln α0 −1\\n2 ln |Σ0|− 1\\n2(x −µ0)⊤Σ−1\\n0 (x −µ0) >ln α1 −1\\n2 ln |Σ1|− 1\\n2(x −µ1)⊤Σ−1\\n1 (x −µ1).\\nThe function\\nδy(x) = ln αy −1\\n2 ln |Σy|− 1\\n2(x −µy)⊤Σ−1\\ny (x −µy), x ∈Rp (7.12)\\nis called the quadratic discriminant functionquadratic\\ndiscriminant\\nfunction\\nfor class y = 0,1. A point x is classified to\\nclass y for which δy(x) is largest. The function is quadratic in x and so the decision bound-\\nary {x ∈Rp : δ0(x) = δ1(x)}is quadratic as well. An important simplification arises for the\\ncase where the assumption is made that Σ0 = Σ1 = Σ. Now, the decision boundary is the\\nset of x for which\\nln α0 −1\\n2(x −µ0)⊤Σ−1(x −µ0) = ln α1 −1\\n2(x −µ1)⊤Σ−1(x −µ1).\\nExpanding the above expression shows that the quadratic term in x is eliminated, giving a\\nlinear decision boundary in x:\\nln α0 −1\\n2µ⊤'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 277, 'page_label': '260'}, page_content='linear decision boundary in x:\\nln α0 −1\\n2µ⊤\\n0 Σ−1µ0 + x⊤Σ−1µ0 = ln α1 −1\\n2µ⊤\\n1 Σ−1µ1 + x⊤Σ−1µ1.\\nThe corresponding linear discriminant functionlinear\\ndiscriminant\\nfunction\\nfor class y is\\nδy(x) = ln αy −1\\n2µ⊤\\ny Σ−1µy + x⊤Σ−1µy, x ∈Rp. (7.13)\\nExample 7.2 (Linear Discriminant Analysis) Consider the case whereα0 = α1 = 1/2\\nand\\nΣ =\\n\" 2 0 .7\\n0.7 2\\n#\\n, µ0 =\\n\"0\\n0\\n#\\n, µ1 =\\n\"2\\n4\\n#\\n.\\nThe distribution of X is a mixture of two bivariate normal distributions. Its pdf,☞135\\n1\\n2g(x |θ,y = 0) + 1\\n2g(x |θ,y = 1),\\nis depicted in Figure 7.2.\\nFigure 7.2: A Gaussian mixture density where the two mixture components have the same\\ncovariance matrix.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 278, 'page_label': '261'}, page_content=\"Classification 261\\nWe used the following Python code to make this figure.\\nLDAmixture.py\\nimport numpy as np, matplotlib.pyplot as plt\\nfrom scipy.stats import multivariate_normal\\nfrom mpl_toolkits.mplot3d import Axes3D\\nfrom matplotlib.colors import LightSource\\nmu0, mu1 = np.array([0,0]), np.array([2,4])\\nSigma = np.array([[2,0.7],[0.7, 2]])\\nx, y = np.mgrid[-4:6:150j,-5:8:150j]\\nmvn0 = multivariate_normal( mu0, Sigma )\\nmvn1 = multivariate_normal( mu1, Sigma )\\nxy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))\\nz = 0.5*mvn0.pdf(xy).reshape(x.shape) + 0.5*mvn1.pdf(xy).reshape(x.\\nshape)\\nfig = plt.figure()\\nax = fig.gca(projection= '3d')\\nls = LightSource(azdeg=180, altdeg=65)\\ncols = ls.shade(z, plt.cm.winter)\\nsurf = ax.plot_surface(x, y, z, rstride=1, cstride=1, linewidth=0,\\nantialiased=False, facecolors=cols)\\nplt.show()\\nThe following Python code, which imports the previous code, draws a contour plot of\\nthe mixture density, simulates 1000 data points from the mixture density, and draws the\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 278, 'page_label': '261'}, page_content=\"the mixture density, simulates 1000 data points from the mixture density, and draws the\\ndecision boundary. To compute and display the linear decision boundary, let [ a1,a2]⊤ =\\n2Σ−1(µ1 −µ0) and b = µ⊤\\n0 Σ−1µ0 −µ⊤\\n1 Σ−1µ1. Then, the decision boundary can be written\\nas a1 x1 + a2 x2 + b = 0 or, equivalently, x2 = −(a1 x1 + b)/a2. We see in Figure 7.3 that the\\ndecision boundary nicely separates the two modes of the mixture density.\\nLDA.py\\nfrom LDAmixture import *\\nfrom numpy.random import rand\\nfrom numpy.linalg import inv\\nfig = plt.figure()\\nplt.contourf(x, y,z, cmap=plt.cm.Blues, alpha= 0.9,extend= 'both ')\\nplt.ylim(-5.0,8.0)\\nplt.xlim(-4.0,6.0)\\nM = 1000\\nr = (rand(M,1) < 0.5)\\nfor i in range (0,M):\\nif r[i]:\\nu = np.random.multivariate_normal(mu0,Sigma,1)\\nplt.plot(u[0][0],u[0][1], '.r',alpha = 0.4)\\nelse :\\nu = np.random.multivariate_normal(mu1,Sigma,1)\\nplt.plot(u[0][0],u[0][1], '+k',alpha = 0.6)\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 279, 'page_label': '262'}, page_content=\"262 Linear and Quadratic Discriminant Analysis\\na = 2*inv(Sigma) @ (mu1-mu0);\\nb = ( mu0.reshape(1,2) @ inv(Sigma) @ mu0.reshape(2,1)\\n- mu1.reshape(1,2) @ inv(Sigma) @mu1.reshape(2,1) )\\nxx = np.linspace(-4,6,100)\\nyy = (-(a[0]*xx +b)/a[1])[0]\\nplt.plot(xx,yy, 'm')\\nplt.show()\\n4\\n 2\\n 0\\n 2\\n 4\\n 6\\n4\\n2\\n0\\n2\\n4\\n6\\n8\\nFigure 7.3: The linear discriminant boundary lies between the two modes of the mixture\\ndensity and is linear.\\nTo illustrate the difference between the linear and quadratic case, we specify di fferent\\ncovariance matrices for the mixture components in the next example.\\nExample 7.3 (Quadratic Discriminant Analysis) As in Example 7.2 we consider a\\nmixture of two Gaussians, but now with di fferent covariance matrices. Figure 7.4 shows\\nthe quadratic decision boundary. The Python code follows.\\n2\\n 1\\n 0\\n 1\\n 2\\n 3\\n 4\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\nFigure 7.4: A quadratic decision boundary.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 280, 'page_label': '263'}, page_content=\"Classification 263\\nQDA.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom scipy.stats import multivariate_normal\\nmu1 = np.array([0,0])\\nmu2 = np.array([2,2])\\nSigma1 = np.array([[1,0.3],[0.3, 1]])\\nSigma2 = np.array([[0.3,0.3],[0.3, 1]])\\nx, y = np.mgrid[-2:4:150j,-3:5:150j]\\nmvn1 = multivariate_normal( mu1, Sigma1 )\\nmvn2 = multivariate_normal( mu2, Sigma2 )\\nxy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))\\nz = ( 0.5*mvn1.pdf(xy).reshape(x.shape) +\\n0.5*mvn2.pdf(xy).reshape(x.shape) )\\nplt.contour(x,y,z)\\nz1 = ( 0.5*mvn1.pdf(xy).reshape(x.shape) -\\n0.5*mvn2.pdf(xy).reshape(x.shape))\\nplt.contour(x,y,z1, levels=[0],linestyles = 'dashed ',\\nlinewidths = 2, colors = 'm')\\nplt.show()\\nOf course, in practice the true parameter θ = {αj,Σj,µj}c\\nj=1 is not known and must be\\nestimated from the training data — for example, by minimizing the cross-entropy training\\nloss (4.4) with respect to θ: ☞ 123\\n1\\nn\\nnX\\ni=1\\nLoss( f (xi,yi),g(xi,yi |θ)) = −1\\nn\\nnX\\ni=1\\nln g(xi,yi |θ),\\nwhere\\nln g(x,y |θ) = ln αy −1\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 280, 'page_label': '263'}, page_content='1\\nn\\nnX\\ni=1\\nLoss( f (xi,yi),g(xi,yi |θ)) = −1\\nn\\nnX\\ni=1\\nln g(xi,yi |θ),\\nwhere\\nln g(x,y |θ) = ln αy −1\\n2 ln |Σy|− 1\\n2 (x −µy)⊤Σ−1\\ny (x −µy) −p\\n2 ln(2π).\\nThe corresponding estimates of the model parameters (see Exercise 2) are:\\nbαy = ny\\nn\\nbµy = 1\\nny\\nX\\ni:yi=y\\nxi\\nbΣy = 1\\nny\\nX\\ni:yi=y\\n(xi −bµy)(xi −bµy)⊤\\n(7.14)\\nfor y = 0,..., c −1, where ny := Pn\\ni=1 1{yi = y}. For the case where Σy = Σ for all y, we\\nhave bΣ = P\\ny bαy bΣy.\\nWhen c >2 classes are involved, the classification procedure carries through in exactly\\nthe same way, leading to quadratic and linear discriminant functions (7.12) and (7.13) for\\neach class. The space Rp now is partitioned into c regions, determined by the linear or\\nquadratic boundaries determined by each pair of Gaussians.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 281, 'page_label': '264'}, page_content='264 Linear and Quadratic Discriminant Analysis\\nFor the linear discriminant case (that is, when Σy = Σ for all y), it is convenient to first\\n“whiten” or sphere the datasphere the data as follows. Let B be an invertible matrix such that Σ = BB⊤,\\nobtained, for example, via the Cholesky method. We linearly transform each data point x☞373\\nto x′ := B−1 x and each mean µy to µ′\\ny := B−1µy, y = 0,..., c −1. Let the random vector X\\nbe distributed according to the mixture pdf\\ngX(x |θ) :=\\nX\\ny\\nαy\\n1p(2π)p |Σy|\\ne−1\\n2 (x−µy)⊤Σ−1\\ny (x−µy).\\nThen, by the transformation Theorem C.4, the vector X′= B−1 X has density☞433\\ngX′(x′|θ) = gX(x |θ)\\n|B−1| =\\nc−1X\\ny=0\\nαy\\n√(2π)p e−1\\n2 (x−µy)⊤(BB⊤)−1(x−µy)\\n=\\nc−1X\\ny=0\\nαy\\n√(2π)p e−1\\n2 (x′−µ′\\ny)⊤(x′−µ′\\ny) =\\nc−1X\\ny=0\\nαy\\n√(2π)p e−1\\n2 ∥x′−µ′\\ny∥2\\n.\\nThis is the pdf of a mixture of standard p-dimensional normal distributions. The name\\n“sphering” derives from the fact that the contours of each mixture component are perfect'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 281, 'page_label': '264'}, page_content='“sphering” derives from the fact that the contours of each mixture component are perfect\\nspheres. Classification of the transformed data is now particularly easy: classify x as by :=\\nargminy{∥x′−µ′\\ny∥2 −2 lnαy}. Note that this rule only depends on the prior probabilities and\\nthe distance from x′to the transformed means {µ′\\ny}. This procedure can lead to a significant\\ndimensionality reduction of the data. Namely, the data can be projected onto the space\\nspanned by the di fferences between the mean vectors {µ′\\ny}. When there are c classes, this\\nis a (c −1)-dimensional space, as opposed to the p-dimensional space of the original data.\\nWe explain the precise ideas via an example.\\nExample 7.4 (Classification after Data Reduction) Consider an equal mixture of\\nthree 3-dimensional Gaussian distributions with identical covariance matrices. After spher-\\ning the data, the covariance matrices are all equal to the identity matrix. Suppose the mean'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 281, 'page_label': '264'}, page_content='ing the data, the covariance matrices are all equal to the identity matrix. Suppose the mean\\nvectors of the sphered data are µ1 = [2,1,−3]⊤, µ2 = [1,−4,0]⊤, and µ3 = [2,4,6]⊤. The\\nleft panel of Figure 7.5 shows the 3-dimensional (sphered) data from each of the three\\nclasses.\\n4 2 0 2 4 6\\n4\\n2\\n0\\n2\\n4\\n5\\n4\\n3\\n2\\n1\\n0\\n1\\n2\\n6\\n 4\\n 2\\n 0\\n 2\\n 4\\n 6\\n4\\n2\\n0\\n2\\n4\\n6\\nFigure 7.5: Left: original data. Right: projected data.\\nThe data are stored in three 1000×3 matrices X1, X2, and X3. Here is how the data was\\ngenerated and plotted.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 282, 'page_label': '265'}, page_content=\"Classification 265\\ndatared.py\\nimport numpy as np\\nfrom numpy.random import randn\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nn=1000\\nmu1 = np.array([2,1,-3])\\nmu2 = np.array([1,-4,0])\\nmu3 = np.array([2,4,0])\\nX1 = randn(n,3) + mu1\\nX2 = randn(n,3) + mu2\\nX3 = randn(n,3) + mu3\\nfig = plt.figure()\\nax = fig.gca(projection= '3d',)\\nax.plot(X1[:,0],X1[:,1],X1[:,2], 'r.',alpha=0.5,markersize=2)\\nax.plot(X2[:,0],X2[:,1],X2[:,2], 'b.',alpha=0.5,markersize=2)\\nax.plot(X3[:,0],X3[:,1],X3[:,2], 'g.',alpha=0.5,markersize=2)\\nax.set_xlim3d(-4,6)\\nax.set_ylim3d(-5,5)\\nax.set_zlim3d(-5,2)\\nplt.show()\\nSince we have equal mixtures, we classify each data point x according to the closest\\ndistance to µ1, µ2, or µ3. We can achieve a reduction in the dimensionality of the data by\\nprojecting the data onto the two-dimensional a ffine space spanned by the {µi}; that is, all\\nvectors are of the form\\nµ1 + β1(µ2 −µ1) + β2(µ3 −µ1), β 1,β2 ∈R.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 282, 'page_label': '265'}, page_content='vectors are of the form\\nµ1 + β1(µ2 −µ1) + β2(µ3 −µ1), β 1,β2 ∈R.\\nIn fact, one may just as well project the data onto the subspace spanned by the vectors\\nµ21 = µ2 −µ1 and µ31 = µ3 −µ1. Let W = [µ21,µ31] be the 3 ×2 matrix whose columns\\nare µ21 and µ31. The orthogonal projection matrix onto the subspace Wspanned by the\\ncolumns of W is (see Theorem A.4): ☞ 362\\nP = WW+ = W(W⊤W)−1W⊤.\\nLet UDV⊤be the singular value decomposition of W. Then P can also be written as\\nP = UD(D⊤D)−1D⊤U⊤.\\nNote that D has dimension 3 ×2, so is not square. The first two columns of U, say u1\\nand u2, form an orthonormal basis of the subspace W. What we want to do is rotate this\\nsubspace to the x−y plane, mapping u1 and u2 to [1,0,0]⊤and [0,1,0]⊤, respectively. This\\nis achieved via the rotation matrix U−1 = U⊤, giving the skewed projection matrix\\nR = U⊤P = D(D⊤D)−1D⊤U⊤,\\nwhose 3rd row only contains zeros. Applying R to all the data points, and ignoring the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 282, 'page_label': '265'}, page_content='whose 3rd row only contains zeros. Applying R to all the data points, and ignoring the\\n3rd component of the projected points (which is 0), gives the right panel of Figure 7.5.\\nWe see that the projected points are much better separated than the original ones. We have\\nachieved dimensionality reduction of the data while retaining all the necessary information\\nrequired for classification. Here is the rest of the Python code.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 283, 'page_label': '266'}, page_content=\"266 Logistic Regression and Softmax Classification\\ndataproj.py\\nfrom datared import *\\nfrom numpy.linalg import svd, pinv\\nmu21 = (mu2 - mu1).reshape(3,1)\\nmu31 = (mu3 - mu1).reshape(3,1)\\nW = np.hstack((mu21, mu31))\\nU,_,_ = svd(W) # we only need U\\nP = W @ pinv(W)\\nR = U.T @ P\\nRX1 = (R @ X1.T).T\\nRX2 = (R @ X2.T).T\\nRX3 = (R @ X3.T).T\\nplt.plot(RX1[:,0],RX1[:,1], 'b.',alpha=0.5,markersize=2)\\nplt.plot(RX2[:,0],RX2[:,1], 'g.',alpha=0.5,markersize=2)\\nplt.plot(RX3[:,0],RX3[:,1], 'r.',alpha=0.5,markersize=2)\\nplt.show()\\n7.5 Logistic Regression and Softmax Classification\\nIn Example 5.10 we introduced the logistic (logit) regression model as a generalized linear☞205\\nmodel where, conditional on a p-dimensonal feature vector x, the random response Y has\\na Ber(h(x⊤β)) distribution with h(u) = 1/(1 + e−u). The parameter βwas then learned from\\nthe training data by maximizing the likelihood of the training responses or, equivalently,\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 283, 'page_label': '266'}, page_content='the training data by maximizing the likelihood of the training responses or, equivalently,\\nby minimizing the supervised version of the cross-entropy training loss (4.4):☞123\\n−1\\nn\\nnX\\ni=1\\nln g(yi |β,xi),\\nwhere g(y = 1 |β,x) = 1/(1 + e−x⊤β) and g(y = 0 |β,x) = e−x⊤β/(1 + e−x⊤β). In particular,\\nwe have\\nln g(y = 1 |β,x)\\ng(y = 0 |β,x) = x⊤β. (7.15)\\nIn other words, the log-odds ratiolog-odds ratio is a linear function of the feature vector. As a con-\\nsequence, the decision boundary {x : g(y = 0 |β,x) = g(y = 1 |β,x)} is the hyperplane\\nx⊤β= 0. Note that x typically includes the constant feature. If the constant feature is con-\\nsidered separately, that is x = [1,ex⊤]⊤, then the boundary is an affine hyperplane in ex.\\nSuppose that training on τ= {(xi,yi)}yields the estimate bβ with the corresponding\\nlearner gτ(y = 1 |x) = 1/(1 + e−x⊤bβ). The learner can be used as a pre-classifier from which\\nwe obtain the classifier 1{gτ(y = 1 |x) >1/2}or, equivalently,\\nby := argmax\\nj∈{0,1}\\ngτ(y = j |x),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 283, 'page_label': '266'}, page_content='we obtain the classifier 1{gτ(y = 1 |x) >1/2}or, equivalently,\\nby := argmax\\nj∈{0,1}\\ngτ(y = j |x),\\nin accordance with the fundamental classification rule (7.2).\\nThe above classification methodology for the logit model can be generalized to the\\nmulti-logitmulti-logit model where the response takes values in the set {0,..., c −1}. The key idea is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 284, 'page_label': '267'}, page_content='Classification 267\\nto replace (7.15) with\\nln g(y = j |W,b,x)\\ng(y = 0 |W,b,x) = x⊤βj, j = 1,..., c −1, (7.16)\\nwhere the matrix W ∈R(c−1)×(p−1) and vector b ∈Rc−1 reparameterize all βj ∈Rp such that\\n(recall x = [1,ex⊤]⊤):\\nWex + b = [β1,..., βc−1]⊤x.\\nObserve that the random response Y is assumed to have a conditional probability distri-\\nbution for which the log-odds ratio with respect to class j and a “reference” class (in this\\ncase 0) is linear. The separating boundaries between two pairs of classes are again a ffine\\nhyperplanes.\\nThe model (7.16) completely specifies the distribution of Y, namely:\\ng(y |W,b,x) = exp(zy+1)Pc\\nk=1 exp(zk), y = 0,..., c −1,\\nwhere z1 is an arbitrary constant, say 0, corresponding to the “reference” class y = 0, and\\n[z2,..., zc]⊤:= Wex + b.\\nNote that g(y |W,b,x) is the (y + 1)-st component of a = softmax(z), where\\nsoftmax : z 7→ exp(z)P\\nk exp(zk)\\nis the softmax softmaxfunction and z = [z1,..., zc]⊤. Finally, we can write the classifier as\\nby = argmax'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 284, 'page_label': '267'}, page_content='by = argmax\\nj∈{0,...,c−1}\\naj+1.\\nIn summary, we have the sequence of mappings transforming the inputx into the outputby:\\nx →Wex + b →softmax(z) → argmax\\nj ∈{0,...,c−1}\\naj+1 →by.\\nIn Example 9.4 we will revisit the multi-logit model and reinterpret this sequence of map- ☞ 334\\npings as a neural network. In the context of neural networks, W is called a weight matrix\\nand b is called a bias vector.\\nThe parameters W and b have to be learned from the training data, which involves\\nminimization of the supervised version of the cross-entropy training loss (4.4): ☞ 123\\n1\\nn\\nnX\\ni=1\\nLoss( f (yi |xi),g(yi |W,b,xi)) = −1\\nn\\nnX\\ni=1\\nln g(yi |W,b,xi).\\nUsing the softmax function, the cross-entropy loss can be simplified to:\\nLoss( f (y |x),g(y |W,b,x)) = −zy+1 + ln\\ncX\\nk=1\\nexp(zk). (7.17)\\nThe discussion on training is postponed until Chapter 9, where we reinterpret the multi-\\nlogit model as a neural net, which can be trained using the limited-memory BFGS method'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 284, 'page_label': '267'}, page_content='logit model as a neural net, which can be trained using the limited-memory BFGS method\\n(Exercise 11). Note that in the binary case ( c = 2), where there is only one vector β to ☞ 353\\nbe estimated, Example 5.10 already established that minimization of the cross-entropy\\ntraining loss is equivalent to likelihood maximization.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 285, 'page_label': '268'}, page_content='268 K-Nearest Neighbors Classification\\n7.6 K-Nearest Neighbors Classification\\nLet τ = {(xi,yi)}n\\ni=1 be the training set, with yi ∈{0,..., c −1}, and let x be a new feature\\nvector. Define x(1),x(2),..., x(n) as the feature vectors ordered by closeness tox in some dis-\\ntance dist(x,xi), e.g., the Euclidean distance∥x−x′∥. Let τ(x) := {(x(1),y(1)) ..., (x(K),y(K))}\\nbe the subset ofτthat contains K feature vectors xi that are closest to x. Then the K-nearest\\nneighborsK-nearest\\nneighbors\\nclassification rule classifies x according to the most frequently occurring class\\nlabels in τ(x). If two or more labels receive the same number of votes, the feature vector\\nis classified by selecting one of these labels randomly with equal probability. For the case\\nK = 1 the set τ(x) contains only one element, say ( x′,y′), and x is classified as y′. This\\ndivides the space into n regions\\nRi = {x : dist(x,xi) ⩽dist(x,xj), j , i}, i = 1,..., n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 285, 'page_label': '268'}, page_content='divides the space into n regions\\nRi = {x : dist(x,xi) ⩽dist(x,xj), j , i}, i = 1,..., n.\\nFor a feature space Rp with the Euclidean distance, this gives a V oronoi tessellation of the\\nfeature space, similar to what was done for vector quantization in Section 4.6.☞142\\nExample 7.5 (Nearest Neighbor Classification) The Python program below simulates\\n80 random points above and below the line x2 = x1. Points above the line x2 = x1 have\\nlabel 0 and points below this line have label 1. Figure 7.6 shows the V oronoi tessellation\\nobtained from the 1-nearest neighbor classification.\\n2\\n 1\\n 0\\n 1\\n 2\\n 3\\n4\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\nFigure 7.6: The 1-nearest neighbor algorithm divides up the space into V oronoi cells.\\nnearestnb.py\\nimport numpy as np\\nfrom numpy.random import rand,randn\\nimport matplotlib.pyplot as plt\\nfrom scipy.spatial import Voronoi, voronoi_plot_2d'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 286, 'page_label': '269'}, page_content=\"Classification 269\\nnp.random.seed(12345)\\nM = 80\\nx = randn(M,2)\\ny = np.zeros(M) # pre -allocate list\\nfor i in range (M):\\nif rand()<0.5:\\nx[i,1], y[i] = x[i,0] + np. abs (randn()), 0\\nelse :\\nx[i,1], y[i] = x[i,0] - np. abs (randn()), 1\\nvor = Voronoi(x)\\nplt_options = { 'show_vertices ':False, 'show_points ':False,\\n'line_alpha ':0.5}\\nfig = voronoi_plot_2d(vor, **plt_options)\\nplt.plot(x[y==0,0], x[y==0,1], 'bo',\\nx[y==1,0], x[y==1,1], 'rs', markersize=3)\\n7.7 Support Vector Machine\\nSuppose we are given the training set τ= {(xi,yi)}n\\ni=1, where each response2 yi takes either\\nthe value −1 or 1, and we wish to construct a classifier taking values in {−1,1}. As this\\nmerely involves a relabeling of the 0–1 classification problem in Section 7.1, the optimal\\nclassification function for the indicator loss, 1{y , by}, is, by Theorem 7.1, equal to\\ng∗(x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n1 if P[Y = 1 |X = x] ⩾1/2,\\n−1 if P[Y = 1 |X = x] <1/2.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 286, 'page_label': '269'}, page_content='g∗(x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n1 if P[Y = 1 |X = x] ⩾1/2,\\n−1 if P[Y = 1 |X = x] <1/2.\\nIt is not difficult to show, see Exercise 5, that the functiong∗can be viewed as the minimizer\\nof the risk for the hinge loss hinge lossfunction, Loss(y,by) = (1 −yby)+ := max{0, 1 −yby}, over all\\nprediction functions g (not necessarily taking values only in the set {−1,1}). That is,\\ng∗= argmin\\ng\\nE(1 −Y g(X))+. (7.18)\\nGiven the training set τ, we can approximate the risk ℓ(g) = E(1 −Y g(X))+ with the train-\\ning loss\\nℓτ(g) = 1\\nn\\nnX\\ni=1\\n(1 −yi g(xi))+,\\nand minimize this over a (smaller) class of functions to obtain the optimal prediction func-\\ntion gτ. Finally, as the prediction functiongτ generally is not a classifier by itself (it usually\\ndoes not only take values −1 or 1), we take the classifier\\nsign gτ(x).\\n2The reason why we use responses −1 and 1 here, instead of 0 and 1, is that the notation becomes easier.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 287, 'page_label': '270'}, page_content='270 Support Vector Machine\\nTherefore, a feature vector x is classified according to 1 or −1 depending on whether\\ngτ(x) ⩾0 or < 0, respectively. The optimal decision boundaryoptimal decision\\nboundary\\nis given by the set of x for\\nwhich gτ(x) = 0.\\nSimilar to the cubic smoothing spline or RKHS setting in (6.19), we can consider find-\\ning the best classifier, given the training data, via the penalized goodness-of-fit optimiza-\\ntion:\\nmin\\ng∈H⊕H0\\n1\\nn\\nnX\\ni=1\\n[1 −yi g(xi)]+ + eγ∥g∥2\\nH,\\nfor some regularization parameter eγ. It will be convenient to define γ := 2neγand to solve\\nthe equivalent problem\\nmin\\ng∈H⊕H0\\nnX\\ni=1\\n[1 −yi g(xi)]+ + γ\\n2 ∥g∥2\\nH.\\nWe know from the Representer Theorem 6.6 that if κ is the reproducing kernel cor-☞231\\nresponding to H, then the solution is of the form (assuming that the null space H0 has a\\nconstant term only):\\ng(x) = α0 +\\nnX\\ni=1\\nαi κ(xi,x). (7.19)\\nSubstituting into the minimization expression yields the analogue of (6.21):☞232\\nmin\\nα,α0\\nnX\\ni=1\\n[1 −yi(α0 + {Kα}i)]+ + γ'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 287, 'page_label': '270'}, page_content='min\\nα,α0\\nnX\\ni=1\\n[1 −yi(α0 + {Kα}i)]+ + γ\\n2 α⊤Kα, (7.20)\\nwhere K is the Gram matrix. This is a convex optimization problem, as it is the sum of a\\nconvex quadratic and piecewise linear term in α. Defining λi := γαi/yi, i = 1,..., n and\\nλ := [λ1,...,λ n]⊤, we show in Exercise 10 that the optimal α and α0 in (7.20) can be\\nobtained by solving the “dual” convex optimization problem\\nmax\\nλ\\nnX\\ni=1\\nλi − 1\\n2γ\\nnX\\ni=1\\nnX\\nj=1\\nλiλjyiyj κ(xi,xj)\\nsubject to: λ⊤y = 0, 0 ⩽λ⩽1,\\n(7.21)\\nand α0 = yj −P\\ni=1 αi κ(xi,xj) for any j for which λj ∈(0,1). In view of (7.19), the optimal\\nprediction function (pre-classifier) gτ is then given by\\ngτ(x) = α0 +\\nnX\\ni=1\\nαi κ(xi,x) = α0 + 1\\nγ\\nnX\\ni=1\\nyiλi κ(xi,x). (7.22)\\nTo mitigate possible numerical problems in the calculation ofα0 it is customary to take\\nan overall average:\\nα0 = 1\\n|J|\\nX\\nj∈J\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3yj −\\nnX\\ni=1\\nαi κ(xi,xj)\\n\\uf8fc\\uf8f4\\uf8f4\\uf8fd\\uf8f4\\uf8f4\\uf8fe,\\nwhere J:= {j : λj ∈(0,1)}.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 288, 'page_label': '271'}, page_content='Classification 271\\nNote that, from (7.22), the optimal pre-classifier g(x) and the classifier sign g(x) only\\ndepend on vectors xi for which λi , 0. These vectors are called the support vectors support vectorsof the\\nsupport vector machine. It is also important to note that the quadratic function in (7.21)\\ndepends on the regularization parameter γ. By defining νi := λi/γ, i = 1,..., n, we can\\nrewrite (7.21) as\\nmin\\nν\\n1\\n2\\nX\\ni,j\\nνiνjyiyj κ(xi,xj) −\\nnX\\ni=1\\nνi\\nsubject to:\\nnX\\ni=1\\nνiyi = 0, 0 ⩽νi ⩽1/γ=: C, i = 1,..., n.\\n(7.23)\\nFor perfectly separable data, that is, data for which an affine plane can be drawn to perfectly\\nseparate the two classes, we may take C = ∞, as explained below. Otherwise, C needs to\\nbe chosen via cross-validation or a test data set, for example.\\nGeometric interpretation\\nFor the linear kernel function κ(x,x′) = x⊤x′, we have\\ngτ(x) = β0 + β⊤x,\\nwith β0 = α0 and β= γ−1 Pn\\ni=1 λiyi xi = Pn\\ni=1 αi xi, and so the decision boundary is an affine'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 288, 'page_label': '271'}, page_content='with β0 = α0 and β= γ−1 Pn\\ni=1 λiyi xi = Pn\\ni=1 αi xi, and so the decision boundary is an affine\\nplane. The situation is illustrated in Figure 7.7. The decision boundary is formed by the\\npoints x such that gτ(x) = 0. The two sets {x : gτ(x) = −1}and {x : gτ(x) = 1}are called\\nthe margins. The distance from the points on a margin to the decision boundary is 1/∥β∥.\\n1 2\\n3\\nFigure 7.7: Classifying two classes (red and blue) using SVM.\\nBased on the “multipliers” {λi}, we can divide the training samples {(xi,yi)}into three\\ncategories (see Exercise 11):\\n• Points for which λi ∈(0,1). These are the support vectors on the margins (green\\nencircled in the figure) and are correctly classified.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 289, 'page_label': '272'}, page_content='272 Support Vector Machine\\n• Points for which λi = 1. These points, which are also support vectors, lie strictly\\ninside the margins (points 1, 2, and 3 in the figure). Such points may or may not be\\ncorrectly classified.\\n• Points for which λi = 0. These are the non-support vectors, which all lie outside the\\nmargins. Every such point is correctly classified.\\nIf the classes of points {xi : yi = 1}and {xi : yi = −1}are perfectly separable by some\\naffine plane, then there will be no points strictly inside the margins, so all support vectors\\nwill lie exactly on the margins. In this case (7.20) reduces to\\nmin\\nβ,β0\\n∥β∥2\\nsubject to: yi(β0 + x⊤\\ni β) ⩾1, i = 1,..., n,\\n(7.24)\\nusing the fact thatα0 = β0 and Kα= XX⊤α= Xβ. We may replace min∥β∥2 in (7.24) with\\nmax 1/∥β∥, as this gives the same optimal solution. As 1 /∥β∥is equal to half the margin\\nwidth, the latter optimization problem has a simple interpretation: separate the points via\\nan affine hyperplane such that the margin width is maximized.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 289, 'page_label': '272'}, page_content='an affine hyperplane such that the margin width is maximized.\\nExample 7.6 (Support Vector Machine) The data in Figure 7.8 was uniformly gener-\\nated on the unit disc. Class-1 points (blue dots) have a radius less than 1/2 (y-values 1) and\\nclass-2 points (red crosses) have a radius greater than 1/2 (y-values −1).\\n-1 -0.5 0 0.5 1\\n-0.8\\n-0.6\\n-0.4\\n-0.2\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nFigure 7.8: Separate the two classes.\\nOf course it is not possible to separate the two groups of points via a straight line in\\nR2. However, it is possible to separate them inR3 by considering three-dimensional feature\\nvectors z = [z1,z2,z3]⊤= [x1,x2,x2\\n1 + x2\\n2]⊤. For any x ∈R2, the corresponding feature vec-\\ntor z lies on a quadratic surface. In this space it is possible to separate the {zi}points into\\ntwo groups by means of a planar surface, as illustrated in Figure 7.9.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 290, 'page_label': '273'}, page_content='Classification 273\\nFigure 7.9: In feature space R3 the points can be separated by a plane.\\nWe wish to find a separating plane inR3 using the transformed features. The following\\nPython code uses the SVC function of the sklearn module to solve the quadratic optimiz-\\nation problem (7.23) (with C = ∞). The results are summarized in Table 7.6. The data is\\navailable from the book’s GitHub site as svmcirc.csv.\\nsvmquad.py\\nimport numpy as np\\nfrom numpy import genfromtxt\\nfrom sklearn.svm import SVC\\ndata = genfromtxt( \\'svmcirc.csv \\', delimiter= \\',\\')\\nx = data[:,[0,1]] #vectors are rows\\ny = data[:,[2]].reshape( len (x),) #labels\\ntmp = np. sum (np.power(x,2),axis=1).reshape( len (x),1)\\nz = np.hstack((x,tmp))\\nclf = SVC(C = np.inf, kernel= \\'linear \\')\\nclf.fit(z,y)\\nprint (\"Support Vectors \\\\n\", clf.support_vectors_)\\nprint (\"Support Vector Labels \",y[clf.support_])\\nprint (\"Nu\",clf.dual_coef_)\\nprint (\"Bias\",clf.intercept_)\\nSupport Vectors\\n[[ 0.038758 0.53796 0.29090314]\\n[-0.49116 -0.20563 0.28352184]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 290, 'page_label': '273'}, page_content='Support Vectors\\n[[ 0.038758 0.53796 0.29090314]\\n[-0.49116 -0.20563 0.28352184]\\n[-0.45068 -0.04797 0.20541358]\\n[-0.061107 -0.41651 0.17721465]]\\nSupport Vector Labels [-1. -1. 1. 1.]\\nNu [[ -46.49249413 -249.01807328 265.31805855 30.19250886]]\\nBias [5.617891]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 291, 'page_label': '274'}, page_content='274 Support Vector Machine\\nTable 7.6: Optimal support vector machine parameters for the R3 data.\\nz⊤ y α= νy\\n0.0388 0.5380 0.2909 −1 −46.4925\\n−0.4912 −0.2056 0.2835 −1 −249.0181\\n−0.4507 −0.0480 0.2054 1 265.3181\\n−0.0611 −0.4165 0.1772 1 30.1925\\nIt follows that the normal vector of the plane is\\nβ=\\nX\\ni∈S\\nαi zi = [−0.9128,0.8917,−24.2764]⊤,\\nwhere Sis the set of indices of the support vectors. We see that the plane is almost per-\\npendicular to the z1,z2 plane. The bias term β0 can also be found from the table above. In\\nparticular, for any x⊤and y in Table 7.6, we have y −β⊤z = β0 = 5.6179.\\nTo draw the separating boundary in R2 we need to project the intersection of the sep-\\narating plane with the quadratic surface onto the z1,z2 plane. That is, we need to find all\\npoints (z1,z2) such that\\n5.6179 −0.9128z1 + 0.8917z2 = 24.2764 (z2\\n1 + z2\\n2). (7.25)\\nThis is the equation of a circle with (approximate) center (0.019,−0.018) and radius 0.48,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 291, 'page_label': '274'}, page_content='This is the equation of a circle with (approximate) center (0.019,−0.018) and radius 0.48,\\nwhich is very close to the true circular boundary between the two groups, with center (0,0)\\nand radius 0.5. This circle is drawn in Figure 7.10.\\n-1 0 1\\n-1\\n0\\n1\\nFigure 7.10: The circular decision boundary can be viewed equivalently as (a) the pro-\\njection onto the x1,x2 plane of the intersection of the separating plane with the quadratic\\nsurface (both in R3), or (b) the set of points x = (x1,x2) for which gτ(x) = β0 +β⊤ϕ(x) = 0.\\nAn equivalent way to derive this circular separating boundary is to consider the feature\\nmap ϕ(x) = [x1,x2,x2\\n1 + x2\\n2]⊤on R2, which defines a reproducing kernel\\nκ(x,x′) = ϕ(x)⊤ϕ(x′),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 292, 'page_label': '275'}, page_content=\"Classification 275\\non R2, which in turn gives rise to a (unique) RKHS H. The optimal prediction function\\n(7.19) is now of the form\\ngτ(x) = α0 + 1\\nγ\\nnX\\ni=1\\nyi λi ϕ(xi)⊤ϕ(x) = β0 + β⊤ϕ(x), (7.26)\\nwhere α0 = β0 and\\nβ= 1\\nγ\\nnX\\ni=1\\nyi λi ϕ(xi).\\nThe decision boundary, {x : gτ(x) = 0}, is again a circle in R2. The following code de-\\ntermines the fitted model parameters and the decision boundary. Figure 7.10 shows the\\noptimal decision boundary, which is identical to (7.25). The function mykernel specifies\\nthe custom kernel above.\\nsvmkern.py\\nimport numpy as np, matplotlib.pyplot as plt\\nfrom numpy import genfromtxt\\nfrom sklearn.svm import SVC\\ndef mykernel(U,V):\\ntmpU = np. sum (np.power(U,2),axis=1).reshape( len (U),1)\\nU = np.hstack((U,tmpU))\\ntmpV = np. sum (np.power(V,2),axis=1).reshape( len (V),1)\\nV = np.hstack((V,tmpV))\\nK = U @ V.T\\nprint (K.shape)\\nreturn K\\n# read in the data\\ninp = genfromtxt( 'svmcirc.csv ', delimiter= ',')\\ndata = inp[:,[0,1]] #vectors are rows\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 292, 'page_label': '275'}, page_content='inp = genfromtxt( \\'svmcirc.csv \\', delimiter= \\',\\')\\ndata = inp[:,[0,1]] #vectors are rows\\ny = inp[:,[2]].reshape( len (data),) #labels\\nclf = SVC(C = np.inf, kernel=mykernel, gamma= \\'auto \\') # custom kernel\\n# clf = SVC(C = np.inf , kernel=\"rbf\", gamma= \\'scale \\') # inbuilt\\nclf.fit(data,y)\\nprint (\"Support Vectors \\\\n\", clf.support_vectors_)\\nprint (\"Support Vector Labels \",y[clf.support_])\\nprint (\"Nu \",clf.dual_coef_)\\nprint (\"Bias \",clf.intercept_)\\n# plot\\nd = 0.001\\nx_min, x_max = -1,1\\ny_min, y_max = -1,1\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, d), np.arange(y_min,\\ny_max, d))\\nplt.plot(data[clf.support_,0],data[clf.support_,1], \\'go\\')\\nplt.plot(data[y==1,0],data[y==1,1], \\'b.\\')'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 293, 'page_label': '276'}, page_content='276 Support Vector Machine\\nplt.plot(data[y==-1,0],data[y==-1,1], \\'rx\\')\\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\nZ = Z.reshape(xx.shape)\\nplt.contour(xx, yy, Z,colors =\"k\")\\nplt.show()\\nFinally, we illustrate the use of the Gaussian kernel\\nκ(x,x′) = e−c ∥x−x′∥2\\n, (7.27)\\nwhere c >0 is some tuning constant. This is an example of a radial basis function kernel,\\nwhich are reproducing kernels of the form κ(x,x′) = f (∥x −x′∥), for some positive real-\\nvalued function f . Each feature vector x is now transformed to a function κx = κ(x,·). We\\ncan think of it as the (unnormalized) pdf of a Gaussian distribution centered around x, and\\ngτ is a (signed) mixture of these pdfs, plus a constant; that is,\\ngτ(x) = α0 +\\nnX\\ni=1\\nαi e−c ∥xi−x∥2\\n.\\nReplacing in Line 2 of the previous code mykernel with ’rbf’ produces the SVM\\nparameters given in Table 7.7. Figure 7.11 shows the decision boundary, which is not ex-\\nactly circular, but is close to the true (circular) boundary {x : ∥x∥= 1/2}. There are now'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 293, 'page_label': '276'}, page_content='actly circular, but is close to the true (circular) boundary {x : ∥x∥= 1/2}. There are now\\nseven support vectors, rather than the four in Figure 7.10.\\nTable 7.7: Optimal support vector machine parameters for the Gaussian kernel case.\\nx⊤ y α(×109)\\n0.0388 0.5380 −1 −0.0635\\n−0.4912 −0.2056 −1 −9.4793\\n0.5086 0.1576 −1 −0.5240\\n−0.4507 −0.0480 1 5.5405\\nx⊤ y α(×109)\\n−0.4374 0.3854 −1 −1.4399\\n0.3402 −0.5740 −1 −0.1000\\n−0.4098 −0.1763 1 6.0662\\n-1 0 1\\n-1\\n0\\n1\\nFigure 7.11: Left: The decision boundary {x : gτ(x) = 0}is roughly circular, and separates\\nthe two classes well. There are seven support vectors, indicated by green circles. Right:\\nThe graph of gτ is a scaled mixture of Gaussian pdfs plus a constant.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 294, 'page_label': '277'}, page_content='Classification 277\\nRemark 7.2 (Scaling and Penalty Parameters) When using a radial basis function in\\nSVC in sklearn, the scaling c (7.27) can be set via the parameter gamma. Note that large\\nvalues of gammalead to highly peaked predicted functions, and small values lead to highly\\nsmoothed predicted functions. The parameter Cin SVC refers C = 1/γin (7.23).\\n7.8 Classification with Scikit-Learn\\nIn this section we apply several classification methods to a real-world data set, using the\\nPython module sklearn (the package name is Scikit-Learn). Specifically, the data is ob-\\ntained from UCI’s Breast Cancer Wisconsin data set. This data set, first published and\\nanalyzed in [118], contains the measurements related to 569 images of 357 benign and\\n212 malignant breast masses. The goal is to classify a breast mass as benign or malig-\\nnant based on 10 features: Radius, Texture, Perimeter, Area, Smoothness, Compactness,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 294, 'page_label': '277'}, page_content='nant based on 10 features: Radius, Texture, Perimeter, Area, Smoothness, Compactness,\\nConcavity, Concave Points, Symmetry, and Fractal Dimension of each mass. The mean,\\nstandard error, and “worst” of these attributes were computed for each image, resulting in\\n30 features. For instance, feature 1 is Mean Radius, feature 11 is Radius SE, feature 21 is\\nWorst Radius.\\nThe following Python code reads the data, extracts the response vector and model (fea-\\nture) matrix and divides the data into a training and test set.\\nskclass1.py\\nfrom numpy import genfromtxt\\nfrom sklearn.model_selection import train_test_split\\nurl1 = \"http://mlr.cs.umass.edu/ml/machine-learning-databases/\"\\nurl2 = \"breast-cancer-wisconsin/\"\\nname = \"wdbc.data\"\\ndata = genfromtxt(url1 + url2 + name, delimiter= \\',\\', dtype= str )\\ny = data[:,1] #responses\\nX = data[:,2:].astype( \\'float \\') #features as an ndarray matrix\\nX_train , X_test , y_train , y_test = train_test_split(\\nX, y, test_size = 0.4, random_state = 1234)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 294, 'page_label': '277'}, page_content=\"X_train , X_test , y_train , y_test = train_test_split(\\nX, y, test_size = 0.4, random_state = 1234)\\nTo visualize the data we create a 3D scatterplot for the features mean radius, mean\\ntexture, and mean concavity, which correspond to the columns 0, 1, and 6 of the model\\nmatrix X. Figure 7.12 suggests that the malignant and benign breast masses could be well\\nseparated using these three features.\\nskclass2.py\\nfrom skclass1 import X, y\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nimport numpy as np\\nBidx = np.where(y == 'B')\\nMidx= np.where(y == 'M')\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 295, 'page_label': '278'}, page_content=\"278 Classification with Scikit-Learn\\n# plot features Radius (column 0), Texture (1), Concavity (6)\\nfig = plt.figure()\\nax = fig.gca(projection = '3d')\\nax.scatter(X[Bidx,0], X[Bidx,1], X[Bidx,6],\\nc='r', marker= '^', label= 'Benign ')\\nax.scatter(X[Midx,0], X[Midx,1], X[Midx,6],\\nc='b', marker= 'o', label= 'Malignant ')\\nax.legend()\\nax.set_xlabel( 'Mean Radius ')\\nax.set_ylabel( 'Mean Texture ')\\nax.set_zlabel( 'Mean Concavity ')\\nplt.show()\\nMean Radius\\n10\\n15\\n20\\n25 Mean Texture10\\n15\\n20\\n25\\n30\\n35\\n40\\nMean Concavity\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nBenign\\nMalignant\\nFigure 7.12: Scatterplot of three features of the benign and malignant breast masses.\\nThe following code uses various classifiers to predict the category of breast masses\\n(benign or malignant). In this case the training set has 341 elements and the test set has 228\\nelements. For each classifier the percentage of correct predictions (that is, the accuracy) in\\nthe test set is reported. We see that in this case quadratic discriminant analysis gives the\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 295, 'page_label': '278'}, page_content='the test set is reported. We see that in this case quadratic discriminant analysis gives the\\nhighest accuracy (0.956). Exercise 18 explores the question whether this metric is the most\\nappropriate for these data.\\nskclass3.py\\nfrom skclass1 import X_train, y_train, X_test, y_test\\nfrom sklearn.metrics import accuracy_score\\nimport sklearn.discriminant_analysis as DA\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC\\nnames = [\"Logit\",\"NBayes\", \"LDA\", \"QDA\", \"KNN\", \"SVM\"]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 296, 'page_label': '279'}, page_content=\"Classification 279\\nclassifiers = [LogisticRegression(C=1e5),\\nGaussianNB(),\\nDA.LinearDiscriminantAnalysis(),\\nDA.QuadraticDiscriminantAnalysis(),\\nKNeighborsClassifier(n_neighbors=5),\\nSVC(kernel= 'rbf', gamma = 1e-4)]\\nprint ('Name Accuracy\\\\n '+14* '-')\\nfor name, clf in zip (names, classifiers):\\nclf.fit(X_train, y_train)\\ny_pred = clf.predict(X_test)\\nprint ('{:6} {:3.3f} '.format (name, accuracy_score(y_test,y_pred)))\\nName Accuracy\\n--------------\\nLogit 0.943\\nNBayes 0.908\\nLDA 0.943\\nQDA 0.956\\nKNN 0.925\\nSVM 0.939\\nFurther Reading\\nAn excellent source for understanding various pattern recognition techniques is the book\\n[35] by Duda et al. Theoretical foundations of classification, including the Vapnik–\\nChernovenkis dimension and the fundamental theorem of learning, are discussed in\\n[109, 121, 122]. A popular measure for characterizing the performance of a binary classi-\\nfier is the receiver operating characteristic (ROC) curve [38]. The naïve Bayes classific-\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 296, 'page_label': '279'}, page_content='fier is the receiver operating characteristic (ROC) curve [38]. The naïve Bayes classific-\\nation paradigm can be extended to handle explanatory variable dependency via graphical\\nmodels such as Bayesian networks and Markov random fields [46, 66, 69]. For a detailed\\ndiscussion on Bayesian decision theory, see [8].\\nExercises\\n1. Let 0 ⩽w ⩽1. Show that the solution to the convex optimization problem\\nmin\\np1,...,pn\\nnX\\ni=1\\np2\\ni\\nsubject to:\\nn−1X\\ni−1\\npi = w and\\nnX\\ni=1\\npi = 1,\\n(7.28)\\nis given by pi = w/(n −1),i = 1,..., n −1 and pn = 1 −w.\\n2. Derive the formulas (7.14) by minimizing the cross-entropy training loss:\\n−1\\nn\\nnX\\ni=1\\nln g(xi,yi |θ),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 297, 'page_label': '280'}, page_content='280 Exercises\\nwhere g(x,y |θ) is such that:\\nln g(x,y |θ) = ln αy −1\\n2 ln |Σy|− 1\\n2 (x −µy)⊤Σ−1\\ny (x −µy) −p\\n2 ln(2π).\\n3. Adapt the code in Example 7.2 to plot the estimated decision boundary instead of the\\ntrue one in Figure 7.3. Compare the true and estimated decision boundaries.\\n4. Recall from equation (7.16) that the decision boundaries of the multi-logit classifier are\\nlinear, and that the pre-classifier can be written as a conditional pdf of the form:\\ng(y |W,b,x) = exp(zy+1)Pc\\ni=1 exp(zi), y ∈{0,..., c −1},\\nwhere x⊤= [1,ex⊤] and z = Wex + b.\\n(a) Show that the linear discriminant pre-classifier in Section 7.4 can also be written as a\\nconditional pdf of the form (θ= {αy,Σy,µy}c−1\\ny=0):\\ng(y |θ,x) = exp(zy+1)Pc\\ni=1 exp(zi), y ∈{0,..., c −1},\\nwhere x⊤ = [1,ex⊤] and z = Wex + b. Find formulas for the corresponding b and W\\nin terms of the linear discriminant parameters {αy,µy,Σy}c−1\\ny=0, where Σy = Σ for all y.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 297, 'page_label': '280'}, page_content='in terms of the linear discriminant parameters {αy,µy,Σy}c−1\\ny=0, where Σy = Σ for all y.\\n(b) Explain which pre-classifier has smaller approximation error: the linear discriminant\\nor multi-logit one? Justify your answer by proving an inequality between the two\\napproximation errors.\\n5. Consider a binary classification problem where the response Y takes values in {−1,1}.\\nShow that optimal prediction function for the hinge loss Loss(y,by) = (1−yby)+ := max{0,1−\\nyby}is the same as the optimal prediction function g∗for the indicator loss:\\ng∗(x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n1 if P[Y = 1 |X = x] >1/2,\\n−1 if P[Y = 1 |X = x] <1/2.\\nThat is, show that\\nE(1 −Y h(X))+ ⩾E(1 −Y g∗(X))+ (7.29)\\nfor all functions h.\\n6. In Example 4.12, we applied a principal component analysis (PCA) to the iris data,☞159\\nbut refrained from classifying the flowers based on their feature vectors x. Implement a\\n1-nearest neighbor algorithm, using a training set of 50 randomly chosen data pairs ( x,y)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 297, 'page_label': '280'}, page_content='1-nearest neighbor algorithm, using a training set of 50 randomly chosen data pairs ( x,y)\\nfrom the iris data set. How many of the remaining 100 flowers are correctly classified?\\nNow classify these entries with an o ff-the-shelf multi-logit classifier, e.g., such as can be\\nfound in the sklearn and statsmodels packages.\\n7. Figure 7.13 displays two groups of data points, given in Table 7.8. The convex hulls\\nhave also been plotted. It is possible to separate the two classes of points via a straight line.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 298, 'page_label': '281'}, page_content='Classification 281\\nIn fact, many such lines are possible. SVM gives the best separation, in the sense that the\\ngap (margin) between the points is maximal.\\n-3 -2 -1 0 1 2 3 4\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nFigure 7.13: Separate the points by a straight line so that the separation between the two\\ngroups is maximal.\\nTable 7.8: Data for Figure 7.13.\\nx1 x2 y\\n2.4524 5 .5673 −1\\n1.2743 0 .8265 1\\n0.8773 −0.5478 1\\n1.4837 3 .0464 −1\\n0.0628 4 .0415 −1\\n−2.4151 −0.9309 1\\n1.8152 3 .9202 −1\\n1.8557 2 .7262 −1\\n−0.4239 1 .8349 1\\n1.9630 0 .6942 1\\nx1 x2 y\\n0.5819 −1.0156 1\\n1.2065 3 .2984 −1\\n2.6830 0 .4216 1\\n−0.0734 1 .3457 1\\n0.0787 0 .6363 1\\n0.3816 5 .2976 −1\\n0.3386 0 .2882 1\\n−0.1493 −0.7095 1\\n1.5554 4 .9880 −1\\n3.2031 4 .4614 −1\\n(a) Identify from the figure the three support vectors.\\n(b) For a separating boundary (line) given by β0 + β⊤x = 0, show that the margin width\\nis 2/∥β∥.\\n(c) Show that the parameters β0 and βthat solve the convex optimization problem (7.24)\\nprovide the maximal width between the margins.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 298, 'page_label': '281'}, page_content='provide the maximal width between the margins.\\n(d) Solve (7.24) using a penalty approach; see Section B.4. In particular, minimize the ☞ 415'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 299, 'page_label': '282'}, page_content='282 Exercises\\npenalty function\\nS (β,β0) = ∥β∥2 −C\\nnX\\ni=1\\nmin\\nn\\n(β0 + β⊤xi) yi −1, 0\\no\\nfor some positive penalty constant C.\\n(e) Find the solution the dual optimization problem (7.21) by using sklearn’s SCV\\nmethod. Note that, as the two point sets are separable, the constraint λ ⩽1 may\\nbe removed, and the value of γcan be set to 1.\\n8. In Example 7.6 we used the feature map ϕ(x) = [x1,x2,x2\\n1 + x2\\n2]⊤to classify the points.\\nAn easier way is to map the points intoR1 via the feature map ϕ(x) = ∥x∥or any monotone\\nfunction thereof. Translated back into R2 this yields a circular separating boundary. Find\\nthe radius and center of this circle, using the fact that here the sorted norms for the two\\ngroups are ..., 0.4889,0.5528,... .\\n9. Let Y ∈{0,1}be a response variable and let h(x) be the regression function\\nh(x) := E[Y |X = x] = P[Y = 1 |X = x].\\nRecall that the Bayes classifier is g∗(x) = 1{h(x) > 1/2}. Let g : R → {0,1}be any'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 299, 'page_label': '282'}, page_content='Recall that the Bayes classifier is g∗(x) = 1{h(x) > 1/2}. Let g : R → {0,1}be any\\nother classifier function. Below, we denote all probabilities and expectations conditional\\non X = x as Px[·] and Ex[·].\\n(a) Show that\\nPx[g(x) , Y] =\\nirreducible error\\nz           }|           {\\nPx[g∗(x) , Y] +|2h(x) −1|1{g(x) , g∗(x)}.\\nHence, deduce that for a learner gT constructed from a training set T, we have\\nE[Px[gT(x) , Y |T]] = Px[g∗(x) , Y] + |2h(x) −1|P[gT(x) , g∗(x)],\\nwhere the first expectation and last probability operations are with respect to T.\\n(b) Using the previous result, deduce that for the unconditional error (that is, we no longer\\ncondition on X = x), we have\\nP[g∗(X) , Y] ⩽P[gT(X) , Y].\\n(c) Show that, if gT := 1{hT(x) >1/2}is a classifier function such that as n →∞\\nhT(x)\\nd\\n−→Z ∼N(µ(x),σ2(x))\\nfor some mean and variance functions µ(x) and σ2(x), respectively, then\\nPx[gT(x) , g∗(x)] −→Φ\\n sign(1 −2h(x))(2µ(x) −1)\\n2σ(x)\\n!\\n,\\nwhere Φ is the cdf of a standard normal random variable.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 300, 'page_label': '283'}, page_content='Classification 283\\n10. The purpose of this exercise is to derive the dual program (7.21) from the primal\\nprogram (7.20). The starting point is to introduce a vector of auxiliary variables ξ :=\\n[ξ1,...,ξ n]⊤and write the primal program as\\nmin\\nα,α0,ξ\\nnX\\ni=1\\nξi + γ\\n2α⊤Kα\\nsubject to: ξ⩾0,\\nyi(α0 + {Kα}i) ⩾1 −ξi, i = 1,..., n.\\n(7.30)\\n(a) Apply the Lagrangian optimization theory from Section B.2.2 to obtain the Lag- ☞ 406\\nrangian function L({α0,α,ξ},{λ,µ}), where µand λare the Lagrange multipliers cor-\\nresponding to the first and second inequality constraints, respectively.\\n(b) Show that the Karush–Kuhn–Tucker (see Theorem B.2) conditions for optimizing L ☞ 407\\nare:\\nλ⊤y = 0\\nα= y ⊙λ/γ\\n0 ⩽λ⩽1\\n(1 −λ) ⊙ξ= 0, λ i (yig(xi) −1 + ξi) = 0, i = 1,..., n\\nξ⩾0, yig(xi) −1 + ξi ⩾0, i = 1,..., n.\\n(7.31)\\nHere ⊙stands for componentwise multiplication; e.g., y ⊙λ= [y1λ1,..., ynλn]⊤, and\\nwe have abbreviated α0 + {Kα}i to g(xi), in view of (7.19). [Hint: one of the KKT'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 300, 'page_label': '283'}, page_content='we have abbreviated α0 + {Kα}i to g(xi), in view of (7.19). [Hint: one of the KKT\\nconditions is λ= 1 −µ; thus we can eliminate µ.]\\n(c) Using the KKT conditions (7.31), reduce the Lagrange dual function L∗(λ) : =\\nminα0,α,ξ L({α0,α,ξ},{λ,1 −λ}) to\\nL∗(λ) =\\nnX\\ni=1\\nλi − 1\\n2γ\\nnX\\ni=1\\nnX\\nj=1\\nλiλjyiyj κ(xi,xj). (7.32)\\n(d) As a consequence of (7.19) and (a)–(c), show that the optimal prediction function gτ\\nis given by\\ngτ(x) = α0 + 1\\nγ\\nnX\\ni=1\\nyiλi κ(xi,x), (7.33)\\nwhere λis the solution to\\nmax\\nλ\\nL∗(λ)\\nsubject to: λ⊤y = 0, 0 ⩽λ⩽1,\\n(7.34)\\nand α0 = yj −1\\nγ\\nPn\\ni=1 yiλi κ(xi,xj) for any j such that λj ∈(0,1).\\n11. Consider SVM classification as illustrated in Figure 7.7. The goal of this exercise is to\\nclassify the training points{(xi,yi)}based on the value of the multipliers{λi}in Exercise 10.\\nLet ξi be the auxiliary variable in Exercise 10, i = 1,..., n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 301, 'page_label': '284'}, page_content='284 Exercises\\n(a) For λi ∈(0,1) show that (xi,yi) lies exactly on the decision border.\\n(b) For λi = 1, show that (xi,yi) lies strictly inside the margins.\\n(c) Show that for λi = 0 the point (xi,yi) lies outside the margins and is correctly classi-\\nfied.\\n12. A well-known data set is the MNIST handwritten digit database, containing many\\nthousands of digitalized numbers (from 0 to 9), each described by a 28×28 matrix of gray\\nscales. A similar but much smaller data set is described in [63]. Here, each handwritten\\ndigit is summarized by a 8 ×8 matrix with integer entries from 0 (white) to 15 (black).\\nFigure 7.14 shows the first 50 digitized images. The data set can be accessed with Python\\nusing the sklearn package as follows.\\nfrom sklearn import datasets\\ndigits = datasets.load_digits()\\nx_digits = digits.data # explanatory variables\\ny_digits = digits.target # responses\\nFigure 7.14: Classify the digitized images.\\n(a) Divide the data into a 75% training set and 25% test set.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 301, 'page_label': '284'}, page_content='(a) Divide the data into a 75% training set and 25% test set.\\n(b) Compare the e ffectiveness of the K-nearest neighbors and naïve Bayes method to\\nclassify the data.\\n(c) Assess which K to use in the K-nearest neighbors classification.\\n13. Download the winequality-red.csv data set from UCI’s wine-quality website.\\nThe response here is the wine quality (from 0 to 10) as specified by a wine “expert”\\nand the explanatory variables are various characteristics such as acidity and sugar con-\\ntent. Use the SVC classifier of sklearn.svm with a linear kernel and penalty para-\\nmeter C = 1 (see Remark 7.2) to fit the data. Use the method cross_val_score from'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 302, 'page_label': '285'}, page_content='Classification 285\\nsklearn.model_selection to obtain a five-fold cross-validation score as an estimate of\\nthe probability that the predicted class matches the expert’s class.\\n14. Consider the credit approval data set crx.data from UCI’s credit approval website.\\nThe data set is concerned with credit card applications. The last column in the data set\\nindicates whether the application is approved ( +) or not (−). With the view of preserving\\ndata privacy, all 15 explanatory variables were anonymized. Note that some explanatory\\nvariables are continuous and some are categorical.\\n(a) Load and prepare the data for analysis with sklearn. First, eliminate data\\nrows with missing values. Next, encode categorical explanatory variables using a\\nOneHotEncoder object from sklearn.preprocessing to create a model matrix X\\nwith indicator variables for the categorical variables, as described in Section 5.3.5. ☞ 177\\n(b) The model matrix should contain 653 rows and 46 columns. The response variable'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 302, 'page_label': '285'}, page_content='(b) The model matrix should contain 653 rows and 46 columns. The response variable\\nshould be a 0 /1 variable (reject/approve). We will consider several classification al-\\ngorithms and test their performance (using a zero-one loss) via ten-fold cross valida-\\ntion.\\ni. Write a function which takes 3 parameters: X,y, and a model, and returns the\\nten-fold cross-validation estimate of the expected generalization risk.\\nii. Consider the following sklearn classifiers: KNeighborsClassifier (k = 5),\\nLogisticRegression, and MPLClassifier (multilayer perceptron). Use the\\nfunction from (i) to identify the best performing classifier.\\n15. Consider a synthetic data set that was generated in the following fashion. The explan-\\natory variable follows a standard normal distribution. The response label is 0 if the explan-\\natory variable is between the 0.95 and 0.05 quantiles of the standard normal distribution,\\nand 1, otherwise. The data set was generated using the following code.\\nimport numpy as np'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 302, 'page_label': '285'}, page_content='and 1, otherwise. The data set was generated using the following code.\\nimport numpy as np\\nimport scipy.stats\\n# generate data\\nnp.random.seed(12345)\\nN = 100\\nX = np.random.randn(N)\\nq = scipy.stats.norm.ppf(0.95)\\ny = np.zeros(N)\\ny[X>=q] = 1\\ny[X<=-q] = 1\\nX = X.reshape(-1,1)\\nCompare the K-nearest neighbors classifier with K = 5 and logistic regression classi-\\nfier. Without computation, which classifier is likely to be better for these data? Verify your\\nanswer by coding both classifiers and printing the corresponding training 0–1 loss.\\n16. Consider the digits data set from Exercise 12. In this exercise, we would like to train\\na binary classifier for the identification of digit 8.\\n(a) Divide the data such that the first 1000 rows are used as the training set and the rest\\nare used as the test set.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 303, 'page_label': '286'}, page_content=\"286 Exercises\\n(b) Train the LogisticRegression classifier from the sklearn.linear_model pack-\\nage.\\n(c) “Train” a naïve classifier that always returns 0. That is, the naïve classifier identifies\\neach instance as being not 8.\\n(d) Compare the zero-one test losses of the logistic regression and the naïve classifiers.\\n(e) Find the confusion matrix, the precision, and the recall of the logistic regression clas-\\nsifier.\\n(f) Find the fraction of eights that are correctly detected by the logistic regression clas-\\nsifier.\\n17. Repeat Exercise 16 with the original MNIST data set. Use the first 60,000 rows as the\\ntrain set and the remaining 10,000 rows as the test set. The original data set can be obtained\\nusing the following code.\\nfrom sklearn.datasets import fetch_openml\\nX, y = fetch_openml( 'mnist_784 ', version=1, return_X_y=True)\\n18. For the breast cancer data in Section 7.8, investigate and discuss whether accuracy is☞277\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 303, 'page_label': '286'}, page_content='18. For the breast cancer data in Section 7.8, investigate and discuss whether accuracy is☞277\\nthe relevant metric to use or if other metrics discussed in Section 7.2 are more appropriate.☞253'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 304, 'page_label': '287'}, page_content='CHAPTER 8\\nDECISION TREES AND ENSEMBLE\\nMETHODS\\nStatistical learning methods based on decision trees have gained tremendous pop-\\nularity due to their simplicity, intuitive representation, and predictive accuracy. This\\nchapter gives an introduction to the construction and use of such trees. We also dis-\\ncuss two key ensemble methods, namely bootstrap aggregation and boosting, which\\ncan further improve the efficiency of decision trees and other learning methods.\\n8.1 Introduction\\nTree-based methods provide a simple, intuitive, and powerful mechanism for both regres-\\nsion and classification. The main idea is to divide a (potentially complicated) feature space\\nXinto smaller regions and fit a simple prediction function to each region. For example,\\nin a regression setting, one could take the mean of the training responses associated with\\nthe training features that fall in that specific region. In the classification setting, a com-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 304, 'page_label': '287'}, page_content='the training features that fall in that specific region. In the classification setting, a com-\\nmonly used prediction function takes the majority vote among the corresponding response\\nvariables. We start with a simple classification example.\\nExample 8.1 (Decision Tree for Classification) The left panel of Figure 8.1 shows a\\ntraining set of 15 two-dimensional points (features) falling into two classes (red and blue).\\nHow should the new feature vector (black point) be classified?\\n20 10 0 10 20\\n20\\n10\\n0\\n10\\n20\\n30\\n40\\n20 10 0 10 20\\n20\\n10\\n0\\n10\\n20\\n30\\n40\\nFigure 8.1: Left: training data and a new feature. Right: a partition of the feature space.\\n287'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 305, 'page_label': '288'}, page_content='288 Introduction\\nIt is not possible to linearly separate the training set, but we can partition the feature\\nspace X= R2 into rectangular regions and assign a class (color) to each region, as shown\\nin the right panel of Figure 8.1. Points in these regions are classified accordingly as blue\\nor red. The partition thus defines a classifier (prediction function) g that assigns to each\\nfeature vector x a class “red” or “blue”. For example, forx = [−15,0]⊤(solid black point),\\ng(x) = “blue”, since it belongs to a blue region of the feature space.\\nBoth the classification procedure and the partitioning of the feature space can be con-\\nveniently represented by a binary decision treedecision tree . This is a tree where each node v corres-\\nponds to a region (subset) Rv of the feature space X— the root node corresponding to the\\nfeature space itself.\\nx2 ≤ 12.0\\nx1 ≤ −20.5\\nx1 ≤ 20.0\\nx1 ≤ 2.5\\nx1 ≤ −5.0\\nTrue\\nTrue False\\nTrue\\nTrue\\nTrue False\\nFalse\\nFalse\\nFalse\\nFigure 8.2: The decision-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 305, 'page_label': '288'}, page_content='x1 ≤ 2.5\\nx1 ≤ −5.0\\nTrue\\nTrue False\\nTrue\\nTrue\\nTrue False\\nFalse\\nFalse\\nFalse\\nFigure 8.2: The decision-\\ntree that corresponds to the\\npartition in Figure 8.1.\\nEach internal node v contains a logical condition that di-\\nvides Rv into two disjoint subregions. The leaf nodes (the ter-\\nminal nodes of the tree) are not subdivided, and their corres-\\nponding regions form a partition ofX, as they are disjoint and\\ntheir union is X. Associated with each leaf node w is also a\\nregional prediction function gw on Rw.\\nThe partitioning of Figure 8.1 was obtained from\\nthe decision tree shown in Figure 8.2. As an illustra-\\ntion of the decision procedure, consider again the input\\nx = [x1,x2]⊤= [−15,0]⊤. The classification process starts\\nfrom the tree root, which contains the conditionx2 ⩽12.0. As\\nthe second component of x is 0, the root condition is satisfied\\nand we proceed to the left child, which contains the condition\\nx1 ⩽−20.5. The next step is similar. As −15 > −20.5, the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 305, 'page_label': '288'}, page_content='x1 ⩽−20.5. The next step is similar. As −15 > −20.5, the\\ncondition is not satisfied and we proceed to the right child.\\nSuch an evaluation of logical conditions along the tree path\\nwill eventually bring us to a leaf node and its associated re-\\ngion. In this case the process terminates in a leaf that corres-\\nponds to the left blue region in the right-hand panel of Fig-\\nure 8.1.\\nMore generally, a binary tree Twill partition the feature space Xinto as many regions\\nas there are leaf nodes. Denote the set of leaf nodes byW. The overall prediction function\\ng that corresponds to the tree can then be written as\\ng(x) =\\nX\\nw∈W\\ngw(x) 1{x ∈Rw}, (8.1)\\nwhere 1 denotes the indicator function. The representation (8.1) is very general and de-\\npends on (1) how the regions{Rw}are constructed via the logical conditions in the decision\\ntree, as well as (2) how the regional prediction functionsregional\\nprediction\\nfunctions\\nof the leaf nodes are defined.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 305, 'page_label': '288'}, page_content='prediction\\nfunctions\\nof the leaf nodes are defined.\\nSimple logical conditions of the form xj ⩽ξ split a Euclidean feature space into rect-\\nangles aligned with the axes. For example, Figure 8.2 partitions the feature space into six\\nrectangles: two blue and four red rectangles.\\nIn a classification setting, the regional prediction function gw corresponding to a leaf\\nnode w takes values in the set of possible class labels. In most cases, as in Example 8.1, it'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 306, 'page_label': '289'}, page_content='Decision Trees and Ensemble Methods 289\\nis taken to be constant on the corresponding region Rw. In a regression setting, gw is real-\\nvalued and also usually takes only one value. That is, every feature vector in Rw leads to\\nthe same predicted value. Of course, different regions will usually have different predicted\\nvalues.\\nConstructing a tree with a training set τ = {(xi,yi)}}n\\ni=1 amounts to minimizing the\\ntraining loss\\nℓτ (g) = 1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) (8.2)\\nfor some loss function; see Chapter 2. With g of the form (8.1), we can write ☞ 19\\nℓτ (g) = 1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) = 1\\nn\\nnX\\ni=1\\nX\\nw∈W\\n1{xi ∈Rw}Loss(yi,g(xi)) (8.3)\\n=\\nX\\nw∈W\\n1\\nn\\nnX\\ni=1\\n1{xi ∈Rw}Loss(yi,gw(xi))\\n|                                    {z                                    }\\n(∗)\\n, (8.4)\\nwhere (∗) is the contribution by the regional prediction function gw to the overall training\\nloss. In the case where all {xi}are different, finding a decision tree T that gives a zero'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 306, 'page_label': '289'}, page_content='loss. In the case where all {xi}are different, finding a decision tree T that gives a zero\\nsquared-error or zero–one training loss is easy, see Exercise 1, but such an “overfitted” tree\\nwill have poor predictive behavior, expressed in terms of the generalization risk. Instead\\nwe consider a restricted class of decision trees and aim to minimize the training loss within\\nthat class. It is common to use a top-down greedy approach, which can only achieve an\\napproximate minimization of the training loss.\\n8.2 Top-Down Construction of Decision Trees\\nLet τ = {(xi,yi)}n\\ni=1 be the training set. The key to constructing a binary decision tree T\\nis to specify a splitting rule splitting rulefor each node v, which can be defined as a logical function\\ns : X→{ False,True}or, equivalently, a binary function s : X→{ 0,1}. For example,\\nin the decision tree of Figure 8.2 the root node has splitting rule x 7→1{x2 ⩽12.0}, in'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 306, 'page_label': '289'}, page_content='in the decision tree of Figure 8.2 the root node has splitting rule x 7→1{x2 ⩽12.0}, in\\ncorrespondence with the logical condition {x2 ⩽12.0}. During the construction of the tree,\\neach node v is associated with a specific region Rv ⊆ Xand therefore also the training\\nsubset {(x,y) ∈τ: x ∈Rv}⊆ τ. Using a splitting rule s, we can divide any subset σof the\\ntraining set τinto two sets:\\nσT := {(x,y) ∈σ : s(x) = True}and σF := {(x,y) ∈σ : s(x) = False}. (8.5)\\nStarting from an empty tree and the initial data set τ, a generic decision tree con-\\nstruction takes the form of the recursive Algorithm 8.2.1. Here we use the notation\\nTv for a subtree of T starting from node v. The final tree T is thus obtained via T =\\nConstruct_Subtree(v0,τ), where v0 is the root of the tree.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 307, 'page_label': '290'}, page_content='290 Top-Down Construction of Decision Trees\\nAlgorithm 8.2.1: Construct_Subtree\\nInput: A node v and a subset of the training data: σ⊆τ.\\nOutput: A (sub) decision tree Tv.\\n1 if termination criterion is met then // v is a leaf node\\n2 Train a regional prediction function gv using the training data σ.\\n3 else // split the node\\n4 Find the best splitting rule sv for node v.\\n5 Create successors vT and vF of v.\\n6 σT ←{(x,y) ∈σ : sv(x) = True}\\n7 σF ←{(x,y) ∈σ : sv(x) = False}\\n8 TvT ←Construct_Subtree(vT,σT) // left branch\\n9 TvF ←Construct_Subtree(vF,σF) // right branch\\n10 return Tv\\nThe splitting rule sv divides the region Rv into two disjoint parts, say RvT and RvF . The\\ncorresponding prediction functions, gT and gF, satisfy\\ngv(x) = gT(x) 1{x ∈RvT}+ gF(x) 1{x ∈RvF}, x ∈Rv.\\nIn order to implement the procedure described in Algorithm 8.2.1, we need to address\\nthe construction of the regional prediction functions gv at the leaves (Line 2), the specific-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 307, 'page_label': '290'}, page_content='the construction of the regional prediction functions gv at the leaves (Line 2), the specific-\\nation of the splitting rule (Line 4), and the termination criterion (Line 1). These important\\naspects are detailed in the following Sections 8.2.1, 8.2.2, and 8.2.3, respectively.\\n8.2.1 Regional Prediction Functions\\nIn general, there is no restriction on how to choose the prediction function gw for a leaf\\nnode v = w in Line 2 of Algorithm 8.2.1. In principle we can train any model from the\\ndata; e.g., via linear regression. However, in practice very simple prediction functions are\\nused. Below, we detail a popular choice for classification, as well as one for regression.\\n1. In the classification setting with class labels 0 ,..., c −1, the regional prediction\\nfunction gw for leaf node w is usually chosen to be constant and equal to the most\\ncommon class label of the training data in the associated region Rw (ties can be'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 307, 'page_label': '290'}, page_content='common class label of the training data in the associated region Rw (ties can be\\nbroken randomly). More precisely, let nw be the number of feature vectors in region\\nRw and let\\npw\\nz = 1\\nnw\\nX\\n{(x,y)∈τ: x∈Rw}\\n1{y=z},\\nbe the proportion of feature vectors in Rw that have class label z = 0,..., c −1. The\\nregional prediction function for node w is chosen to be the constant\\ngw(x) = argmax\\nz∈{0,...,c−1}\\npw\\nz . (8.6)\\n2. In the regression setting, gw is usually chosen as the mean response in the region;\\nthat is,\\ngw(x) = yRw := 1\\nnw\\nX\\n{(x,y)∈τ: x∈Rw}\\ny, (8.7)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 308, 'page_label': '291'}, page_content='Decision Trees and Ensemble Methods 291\\nwhere nw is again the number of feature vectors in Rw. It is not difficult to show that\\ngw(x) = yRw minimizes the squared-error loss with respect to all constant functions,\\nin the region Rw; see Exercise 2.\\n8.2.2 Splitting Rules\\nIn Line 4 in Algorithm 8.2.1, we divide region Rv into two sets, using a splitting rule\\n(function) sv. Consequently, the data set σassociated with node v (that is, the subset of the\\noriginal data set τwhose feature vectors lie in Rv), is also split — into σT and σF. What is\\nthe benefit of such a split in terms of a reduction in the training loss? If v were set to a leaf\\nnode, its contribution to the training loss would be (see (8.4)):\\n1\\nn\\nnX\\ni=1\\n1{(xi,yi)∈σ}Loss(yi,gv(xi)). (8.8)\\nIf v were to be split instead, its contribution to the overall training loss would be:\\n1\\nn\\nnX\\ni=1\\n1{(xi,yi)∈σT}Loss(yi,gT(xi)) + 1\\nn\\nnX\\ni=1\\n1{(xi,yi)∈σF}Loss(yi,gF(xi)), (8.9)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 308, 'page_label': '291'}, page_content='1\\nn\\nnX\\ni=1\\n1{(xi,yi)∈σT}Loss(yi,gT(xi)) + 1\\nn\\nnX\\ni=1\\n1{(xi,yi)∈σF}Loss(yi,gF(xi)), (8.9)\\nwhere gT and gF are the prediction functions belonging to the child nodes vT and vF. A\\ngreedy heuristic is to pretend that the tree construction algorithm immediately terminates\\nafter the split, in which case vT and vF are leaf nodes, and gT and gF are readily evaluated\\n— e.g., as in Section 8.2.1. Note that for any splitting rule the contribution (8.8) is always\\ngreater than or equal to (8.9). It therefore makes sense to choose the splitting rule such that\\n(8.9) is minimized. Moreover, the termination criterion may involve comparing (8.9) with\\n(8.8). If their difference is too small it may not be worth further splitting the feature space.\\nAs an example, suppose the feature space is X= Rp and we consider splitting rules of\\nthe form\\ns(x) = 1{xj ⩽ξ}, (8.10)\\nfor some 1 ⩽j ⩽p and ξ∈R, where we identify 0 withFalseand 1 with True. Due to the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 308, 'page_label': '291'}, page_content='for some 1 ⩽j ⩽p and ξ∈R, where we identify 0 withFalseand 1 with True. Due to the\\ncomputational and interpretative simplicity, such binary splitting rules are implemented in\\nmany software packages and are considered to be the de facto standard. As we have seen,\\nthese rules divide up the feature space into rectangles, as in Figure 8.1. It is natural to ask\\nhow j and ξ should be chosen so as to minimize (8.9). For a regression problem, using a\\nsquared-error loss and a constant regional prediction function as in (8.7), the sum (8.9) is\\ngiven by\\n1\\nn\\nX\\n(x,y)∈τ:xj⩽ξ\\n\\x00y −yT\\n\\x012 + 1\\nn\\nX\\n(x,y)∈τ:xj>ξ\\n\\x00y −yF\\n\\x012 , (8.11)\\nwhere yT and yF are the average responses for theσT and σF data, respectively. Let{xj,k}m\\nk=1\\ndenote the possible values of xj, j = 1,..., p within the training subset σ (with m ⩽n\\nelements). Note that, for a fixed j, (8.11) is a piecewise constant function of ξ, and that its\\nminimal value is attained at some value xj,k. As a consequence, to minimize (8.11) over'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 308, 'page_label': '291'}, page_content='minimal value is attained at some value xj,k. As a consequence, to minimize (8.11) over\\nall j and ξ, it suffices to evaluate (8.11) for each of the m ×p values xj,k and then take the\\nminimizing pair ( j,xj,k).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 309, 'page_label': '292'}, page_content='292 Top-Down Construction of Decision Trees\\nFor a classification problem, using the indicator loss and a constant regional prediction\\nfunction as in (8.6), the aim is to choose a splitting rule that minimizes\\n1\\nn\\nX\\n(x,y)∈σT\\n1{y , y∗\\nT}+ 1\\nn\\nX\\n(x,y)∈σF\\n1{y , y∗\\nF}, (8.12)\\nwhere y∗\\nT = gT(x) is the most prevalent class (majority vote) in the data set σT and y∗\\nF\\nis the most prevalent class in σF. If the feature space is X= Rp and the splitting rules\\nare of the form (8.10), then the optimal splitting rule can be obtained in the same way as\\ndescribed above for the regression case; the only di fference is that (8.11) is replaced with\\n(8.12).\\nWe can view the minimization of (8.12) as minimizing a weighted average of “impur-\\nities” of nodes σT and σF. Namely, for an arbitrary training subset σ⊆τ, if y∗is the most\\nprevalent label, then\\n1\\n|σ|\\nX\\n(x,y)∈σ\\n1{y , y∗}= 1 − 1\\n|σ|\\nX\\n(x,y)∈σ\\n1{y = y∗}= 1 −py∗ = 1 − max\\nz∈{0,...,c−1}\\npz,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 309, 'page_label': '292'}, page_content='1\\n|σ|\\nX\\n(x,y)∈σ\\n1{y , y∗}= 1 − 1\\n|σ|\\nX\\n(x,y)∈σ\\n1{y = y∗}= 1 −py∗ = 1 − max\\nz∈{0,...,c−1}\\npz,\\nwhere pz is the proportion of data points in σthat have class label z, z = 0,..., c −1. The\\nquantity\\n1 − max\\nz∈{0,...,c−1}\\npz\\nmeasures the diversity of the labels in σand is called the misclassification impurity. Con-misclassification\\nimpurity sequently, (8.12) is the weighted sum of the misclassification impurities of σT and σF,\\nwith weights by |σT|/n and |σF|/n, respectively. Note that the misclassification impurity\\nonly depends on the label proportions rather than on the individual responses. Instead of\\nusing the misclassification impurity to decide if and how to split a data set σ, we can use\\nother impurity measures that only depend on the label proportions. Two popular choices\\nare the entropy impurityentropy\\nimpurity\\n:\\n−\\nc−1X\\nz=0\\npz log2(pz)\\nand the Gini impurityGini impurity :\\n1\\n2\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 −\\nc−1X\\nz=0\\np2\\nz\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 309, 'page_label': '292'}, page_content='−\\nc−1X\\nz=0\\npz log2(pz)\\nand the Gini impurityGini impurity :\\n1\\n2\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 −\\nc−1X\\nz=0\\np2\\nz\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.\\nAll of these impurities are maximal when the label proportions are equal to 1 /c. Typical\\nshapes of the above impurity measures are illustrated in Figure 8.3 for the two-label case,\\nwith class probabilities p and 1 −p. We see here the similarity of the di fferent impurity\\nmeasures. Note that impurities can be arbitrarily scaled, and so using ln(pz) = log2(pz) ln(2)\\ninstead of log2(pz) above gives an equivalent entropy impurity.\\n8.2.3 Termination Criterion\\nWhen building a tree, one can define various types of termination conditions. For example,\\nwe might stop when the number of data points in the tree node (the size of the input σset\\nin Algorithm 8.2.1) is less than or equal to some predefined number. Or we might choose'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 310, 'page_label': '293'}, page_content='Decision Trees and Ensemble Methods 293\\n0 0 .2 0 .4 0 .6 0 .8 10\\n0.2\\n0.4\\n0.6\\np\\nimpurity\\ncross-entropy\\nGini index\\nmisclassiﬁcation\\nFigure 8.3: Entropy, Gini, and misclassification impurities for binary classification, with\\nclass frequencies p1 = p and p2 = 1 −p. The entropy impurity was normalized (divided by\\n2), to ensure that all impurity measures attain the same maximum value of 1/2 at p = 1/2.\\nthe maximal depth of the tree in advance. Another possibility is to stop when there is no\\nsignificant advantage, in terms of training loss, to split regions. Ultimately, the quality of a\\ntree is determined by its predictive performance (generalization risk) and the termination\\ncondition should aim to strike a balance between minimizing the approximation error and\\nminimizing the statistical error, as discussed in Section 2.4. ☞ 31\\nExample 8.2 (Fixed Tree Depth) To illustrate how the tree depth impacts on the gener-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 310, 'page_label': '293'}, page_content='Example 8.2 (Fixed Tree Depth) To illustrate how the tree depth impacts on the gener-\\nalization risk, consider Figure 8.4, which shows the typical behavior of the cross-validation\\nloss as a function of the tree depth. Recall that the cross-validation loss is an estimate of the\\nexpected generalization risk. Complicated (deep) trees tend to overfit the training data by\\nproducing many divisions of the feature space. As we have seen, this overfitting problem is\\ntypical of all learning methods; see Chapter 2 and in particular Example 2.1. To conclude, ☞ 26\\nincreasing the maximal depth does not necessarily result in better performance.\\n0 5 10 15 20 25 30\\n0.3\\n0.35\\n0.4\\n0.45\\ntree depth\\nloss\\nFigure 8.4: The ten-fold cross-validation loss as a function of the maximal tree depth for a\\nclassification problem. The optimal maximal tree depth is here 6.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 311, 'page_label': '294'}, page_content='294 Top-Down Construction of Decision Trees\\nTo create Figure 8.4 we used 1 the Python method make_blobs from the sklearn\\nmodule to produce a training set of size n = 5000 with ten-dimensional feature vectors☞491\\n(thus, p = 10 and X= R10), each of which is classified into one of c = 3 classes. The full\\ncode is given below.\\nTreeDepthCV.py\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import zero_one_loss\\nimport matplotlib.pyplot as plt\\ndef ZeroOneScore(clf, X, y):\\ny_pred = clf.predict(X)\\nreturn zero_one_loss(y, y_pred)\\n# Construct the training set\\nX, y = make_blobs(n_samples=5000, n_features=10, centers=3,\\nrandom_state=10, cluster_std=10)\\n# construct a decision tree classifier\\nclf = DecisionTreeClassifier(random_state=0)\\n# Cross -validation loss as a function of tree depth (1 to 30)\\nxdepthlist = []\\ncvlist = []\\ntree_depth = range (1,30)\\nfor d in tree_depth:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 311, 'page_label': '294'}, page_content=\"xdepthlist = []\\ncvlist = []\\ntree_depth = range (1,30)\\nfor d in tree_depth:\\nxdepthlist.append(d)\\nclf.max_depth=d\\ncv = np.mean(cross_val_score(clf, X, y, cv=10, scoring=\\nZeroOneScore))\\ncvlist.append(cv)\\nplt.xlabel( 'tree depth ', fontsize=18, color= 'black ')\\nplt.ylabel( 'loss ', fontsize=18, color= 'black ')\\nplt.plot(xdepthlist, cvlist, '-*' , linewidth=0.5)\\nThe code above relies heavily on sklearn and hides the implementation details. To\\nshow how decision trees are actually constructed using the previous theory, we proceed\\nwith a very basic implementation.\\n8.2.4 Basic Implementation\\nIn this section we implement a regression tree, step by step. To run the program, amalgam-\\nate the code snippets below into one file, in the order presented. First, we import various\\npackages and define a function to generate the training and test data.\\n1The data used for Figure 8.1 was produced in a similar way.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 312, 'page_label': '295'}, page_content='Decision Trees and Ensemble Methods 295\\nBasicTree.py\\nimport numpy as np\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.model_selection import train_test_split\\ndef makedata():\\nn_points = 500 # number of samples\\nX, y = make_friedman1(n_samples=n_points, n_features=5,\\nnoise=1.0, random_state=100)\\nreturn train_test_split(X, y, test_size=0.5, random_state=3)\\nThe “main” method calls themakedata method, uses the training data to build a regres-\\nsion tree, and then predicts the responses of the test set and reports the mean squared-error\\nloss.\\ndef main():\\nX_train, X_test, y_train, y_test = makedata()\\nmaxdepth = 10 # maximum tree depth\\n# Create tree root at depth 0\\ntreeRoot = TNode(0, X_train,y_train)\\n# Build the regression tree with maximal depth equal to max_depth\\nConstruct_Subtree(treeRoot, maxdepth)\\n# Predict\\ny_hat = np.zeros( len (X_test))\\nfor i in range (len (X_test)):\\ny_hat[i] = Predict(X_test[i],treeRoot)\\nMSE = np.mean(np.power(y_hat - y_test,2))'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 312, 'page_label': '295'}, page_content='y_hat[i] = Predict(X_test[i],treeRoot)\\nMSE = np.mean(np.power(y_hat - y_test,2))\\nprint (\"Basic tree: tree loss = \", MSE)\\nThe next step is to specify a tree node as a Python class. Each node has a number of\\nattributes, including the features and the response data ( X and y) and the depth at which\\nthe node is placed in the tree. The root node has depth 0. Each node w can calculate its\\ncontribution to the squared-error training loss Pn\\ni=1 1{xi ∈Rw}(yi −gw(xi))2. Note that we\\nhave omitted the constant 1 /n term when training the tree, which simply scales the loss\\n(8.2).\\nclass TNode:\\ndef __init__(self, depth, X, y):\\nself.depth = depth\\nself.X = X # matrix of features\\nself.y = y # vector of response variables\\n# initialize optimal split parameters\\nself.j = None\\nself.xi = None\\n# initialize children to be None\\nself.left = None\\nself.right = None'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 313, 'page_label': '296'}, page_content='296 Top-Down Construction of Decision Trees\\n# initialize the regional predictor\\nself.g = None\\ndef CalculateLoss(self):\\nif (len (self.y)==0):\\nreturn 0\\nreturn np.sum (np.power(self.y - self.y.mean(),2))\\nThe function below implements the training (tree-building) Algorithm 8.2.1.\\ndef Construct_Subtree(node, max_depth):\\nif (node.depth == max_depth or len (node.y) == 1):\\nnode.g = node.y.mean()\\nelse :\\nj, xi = CalculateOptimalSplit(node)\\nnode.j = j\\nnode.xi = xi\\nXt, yt, Xf, yf = DataSplit(node.X, node.y, j, xi)\\nif (len (yt)>0):\\nnode.left = TNode(node.depth+1,Xt,yt)\\nConstruct_Subtree(node.left, max_depth)\\nif (len (yf)>0):\\nnode.right = TNode(node.depth+1, Xf,yf)\\nConstruct_Subtree(node.right, max_depth)\\nreturn node\\nThis requires an implementation of the CalculateOptimalSplit function. To start,\\nwe implement a function DataSplit that splits the data according to s(x) = 1{xj ⩽ξ}.\\ndef DataSplit(X,y,j,xi):\\nids = X[:,j]<=xi\\nXt = X[ids == True,:]\\nXf = X[ids == False,:]\\nyt = y[ids == True]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 313, 'page_label': '296'}, page_content='ids = X[:,j]<=xi\\nXt = X[ids == True,:]\\nXf = X[ids == False,:]\\nyt = y[ids == True]\\nyf = y[ids == False]\\nreturn Xt, yt, Xf, yf\\nThe CalculateOptimalSplit method runs through the possible splitting thresholds\\nξfrom the set {xj,k}and finds the optimal split.\\ndef CalculateOptimalSplit(node):\\nX = node.X\\ny = node.y\\nbest_var = 0\\nbest_xi = X[0,best_var]\\nbest_split_val = node.CalculateLoss()'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 314, 'page_label': '297'}, page_content='Decision Trees and Ensemble Methods 297\\nm, n = X.shape\\nfor j in range (0,n):\\nfor i in range (0,m):\\nxi = X[i,j]\\nXt, yt, Xf, yf = DataSplit(X,y,j,xi)\\ntmpt = TNode(0, Xt, yt)\\ntmpf = TNode(0, Xf, yf)\\nloss_t = tmpt.CalculateLoss()\\nloss_f = tmpf.CalculateLoss()\\ncurr_val = loss_t + loss_f\\nif (curr_val < best_split_val):\\nbest_split_val = curr_val\\nbest_var = j\\nbest_xi = xi\\nreturn best_var, best_xi\\nFinally, we implement the recursive method for prediction.\\ndef Predict(X,node):\\nif (node.right == None and node.left != None):\\nreturn Predict(X,node.left)\\nif (node.right != None and node.left == None):\\nreturn Predict(X,node.right)\\nif (node.right == None and node.left == None):\\nreturn node.g\\nelse :\\nif (X[node.j] <= node.xi):\\nreturn Predict(X,node.left)\\nelse :\\nreturn Predict(X,node.right)\\nRunning the main function defined above gives a similar 2 result to what one would\\nachieve with the sklearn package, using the DecisionTreeRegressor method.\\nmain() # run the main program\\n# compare with sklearn'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 314, 'page_label': '297'}, page_content='main() # run the main program\\n# compare with sklearn\\nfrom sklearn.tree import DecisionTreeRegressor\\nX_train, X_test, y_train, y_test = makedata() # use the same data\\nregTree = DecisionTreeRegressor(max_depth = 10, random_state=0)\\nregTree.fit(X_train,y_train)\\ny_hat = regTree.predict(X_test)\\nMSE2 = np.mean(np.power(y_hat - y_test,2))\\nprint (\"DecisionTreeRegressor: tree loss = \", MSE2)\\nBasic tree: tree loss = 9.067077996170276\\nDecisionTreeRegressor: tree loss = 10.197991295531748\\n2After establishing a best split ξ = xj,k, sklearn assigns the corresponding feature vector randomly to\\none of the two child nodes, rather than to the Truechild.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 315, 'page_label': '298'}, page_content='298 Additional Considerations\\n8.3 Additional Considerations\\n8.3.1 Binary Versus Non-Binary Trees\\nWhile it is possible to split a tree node into more than two groups (multiway splits), it\\ngenerally produces inferior results compared to the simple binary split. The major reason\\nis that multiway splits can lead to too many nodes near the tree root that have only a\\nfew data points, thus leaving insu fficient data for later splits. As multiway splits can be\\nrepresented as several binary splits, the latter is preferred [55].\\n8.3.2 Data Preprocessing\\nSometimes, it can be beneficial to preprocess the data prior to the tree construction. For\\nexample, PCA can be used with a view to identify the most important dimensions, which☞153\\nin turn will lead to simpler and possibly more informative splitting rules in the internal\\nnodes.\\n8.3.3 Alternative Splitting Rules\\nWe restricted our attention to splitting rules of the type s(x) = 1{xj ⩽ξ}, where j ∈'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 315, 'page_label': '298'}, page_content='We restricted our attention to splitting rules of the type s(x) = 1{xj ⩽ξ}, where j ∈\\n{1,..., p}and ξ ∈ R. These types of rules may not always result in a simple partition\\nof the feature space, as illustrated by the binary data in Figure 8.5. In this case, the feature\\nspace could have been partitioned into just two regions, separated by a straight line.\\nFigure 8.5: The two groups of points can here be separated by a straight line. Instead, the\\nclassification tree divides up the space into many rectangles, leading to an unnecessarily\\ncomplicated classification procedure.\\nIn this case many classification methods discussed in Chapter 7, such as linear discrim-\\ninant analysis (Section 7.4), will work very well, whereas the classification tree is rather☞259\\nelaborate, dividing the feature set into too many regions. An obvious remedy is to use\\nsplitting rules of the form\\ns(x) = 1{a⊤x ⩽ξ}.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 316, 'page_label': '299'}, page_content='Decision Trees and Ensemble Methods 299\\nIn some cases, such as the one just discussed, it may be useful to use a splitting rule\\nthat involves several variables, as opposed to a single one. The decision regarding the split\\ntype clearly depends on the problem domain. For example, for logical (binary) variables\\nour domain knowledge may indicate that a different behavior is expected when both xi and\\nxj (i , j) are True. In this case, we will naturally introduce a decision rule of the form:\\ns(x) = 1{xi = T and xj = T}.\\n8.3.4 Categorical Variables\\nWhen an explanatory variable is categorical with labels (levels) say {1,..., k}, the split-\\nting rule is generally defined via a partition of the label set {1,..., k}into two subsets.\\nSpecifically, let L and R be a partition of {1,..., k}. Then, the splitting rule is defined via\\ns(x) = 1{xj ∈L}.\\nFor the general supervised learning case, finding the optimal partition in the sense of min-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 316, 'page_label': '299'}, page_content='For the general supervised learning case, finding the optimal partition in the sense of min-\\nimal loss requires one to consider 2 k subsets of {1,..., k}. Consequently, finding a good\\nsplitting rule for categorical variables can be challenging when the number of labels k is\\nlarge.\\n8.3.5 Missing Values\\nMissing data is present in many real-life problems. Generally, when working with incom-\\nplete feature vectors, where one or more values are missing, it is typical to either com-\\npletely delete the feature vector from the data (which may distort the data) or to impute\\n(guess) its missing values from the available data; see e.g., [120]. Tree methods, however,\\nallow an elegant approach for handling missing data. Specifically, in the general case, the\\nmissing data problem can be handled via surrogate splitting rules [20].\\nWhen dealing with categorical (factor) features, we can introduce an additional cat-\\negory “missing” for the absent data.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 316, 'page_label': '299'}, page_content='egory “missing” for the absent data.\\nThe main idea of surrogate rules is as follows. First, we construct a decision (regression\\nor a classification) tree via Algorithm 8.2.1. During this construction process, the solution\\nof the optimization problem (8.9) is calculated only over the observations that are not\\nmissing a particular variable. Suppose that a tree nodev has a splitting rule s∗(x) = 1{xj∗ ⩽\\nξ∗}for some 1 ⩽j∗⩽k and threshold ξ∗.\\nFor the node v we can introduce a set of alternative splitting rules that resemble the\\noriginal splitting rule, sometimes called theprimary splitting rule, using different variables\\nand thresholds. Namely, we look for a binary splitting rule s(x |j,ξ), j , j∗ such that the\\ndata split introduced by s will be similar to the original data split from s∗. The similarity is\\ngenerally measured via a binary misclassification loss, where the true classes of observa-\\ntions are determined by the primary splitting rule and the surrogate splitting rules serve as'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 316, 'page_label': '299'}, page_content='tions are determined by the primary splitting rule and the surrogate splitting rules serve as\\nclassifiers. Consider, for example, the data in Table 8.1 and suppose that the primary split-\\nting rule at node v is 1{Age ⩽25}. That is, the five data points are split such that the left\\nand the right child ofv contains two and three data points, respectively. Next, the following\\nsurrogate splitting rules can be considered:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 317, 'page_label': '300'}, page_content='300 Controlling the Tree Shape\\n1. 1{Salary ⩽1500}, and\\n2. 1{Height ⩽173}.\\nTable 8.1: Example data with three variables (Age, Height, and Salary).\\nId Age Height Salary\\n1 20 173 1000\\n2 25 168 1500\\n3 38 191 1700\\n4 49 170 1900\\n5 62 182 2000\\nThe 1{Salary ⩽1500}surrogate rule completely mimics the primary rule, in the sense\\nthat the data splits induced by these rules are identical. Namely, both rules partition the\\ndata into two sets (by Id) {1,2}and {3,4,5}. On the other hand, the 1{Height ⩽173}rule\\nis less similar to the primary rule, since it causes the different partition {1,2,4}and {3,5}.\\nIt is up to the user to define the number of surrogate rules for each tree node. As soon as\\nthese surrogate rules are available, we can use them to handle a new data point, even if the\\nmain rule cannot be applied due to a missing value of the primary variablexj∗. Specifically,\\nif the observation is missing the primary split variable, we apply the first (best) surrogate'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 317, 'page_label': '300'}, page_content='if the observation is missing the primary split variable, we apply the first (best) surrogate\\nrule. If the first surrogate variable is also missing, we apply the second best surrogate rule,\\nand so on.\\n8.4 Controlling the Tree Shape\\nEventually, we are interested in getting the right-size tree. Namely, a tree that shows good\\ngeneralization properties. It was already discussed in Section 8.2.3 (Figure 8.4) that shal-\\nlow trees tend to underfit and deep trees tend to overfit the data. Basically, a shallow tree\\ndoes not produce a sufficient number of splits and a deep tree will produce many partitions\\nand thus many leaf nodes. If we grow the tree to a su fficient depth, each training sample\\nwill occupy a separate leaf and we will observe a zero loss with respect to the training data.\\nThe above phenomenon is illustrated in Figure 8.6, which presents the cross-validation loss\\nand the training loss as a function of the tree depth.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 317, 'page_label': '300'}, page_content='and the training loss as a function of the tree depth.\\nIn order to overcome the under- and the overfitting problem, Breiman et al. [20] ex-\\namined the possibility of stopping the tree from growing as soon as the decrease in loss\\ndue to a split of node v, as expressed in the di fference of (8.8) and (8.9), is smaller than\\nsome predefined parameter δ ∈R. Under this setting, the tree construction process will\\nterminate when no leaf node can be split such that the contribution to the training loss after\\nthis split is greater than δ.\\nThe authors found that this approach was unsatisfactory. Specifically, it was noted that a\\nvery smallδleads to an excessive amount of splitting and thus causes overfitting. Increasing\\nδdid not work either. The problem is that the nature of the proposed rule is one-step-look-\\nahead. To see this, consider a tree node for which the best possible decrease in loss is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 318, 'page_label': '301'}, page_content='Decision Trees and Ensemble Methods 301\\n0 5 10 15 20\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\ntree depth\\nloss\\ntrain\\nCV\\nFigure 8.6: The cross-validation and the training loss as a function of the tree depth for a\\nbinary classification problem.\\nsmaller than δ. According to the proposed procedure, this node will not be split further. This\\nmay, however, be sub-optimal, because it could happen that one of the node’sdescendants,\\nif split, could lead to a major decrease in loss.\\nTo address these issues, a so-called pruning tree pruningroutine can be employed. The idea is as\\nfollows. We first grow a very deep tree and then prune (remove nodes) it upwards until we\\nreach the root node. Consequently, the pruning process causes the number of tree nodes\\nto decrease. While the tree is being pruned, the generalization risk gradually decreases up\\nto the point where it starts increasing again, at which point the pruning is stopped. This\\ndecreasing/increasing behavior is due to the bias–variance tradeoff (2.22).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 318, 'page_label': '301'}, page_content='decreasing/increasing behavior is due to the bias–variance tradeoff (2.22).\\nWe next describe the details. To start with, let v and v′be tree nodes. We say that v′is\\na descendant of v if there is a path down the tree, which leads from v to v′. If such a path\\nexists, we also say that v is an ancestor of v′. Consider the tree in Figure 8.7.\\nTo formally define pruning, we will require the following Definition 8.1. An example\\nof pruning is demonstrated in Figure 8.8.\\nDefinition 8.1: Branches and Pruning\\n1. A tree branch tree branchTv of the tree Tis a sub-tree of Trooted at node v ∈T.\\n2. The pruning of branch Tv from a tree Tis performed via deletion of the entire\\nbranch Tv from Texcept the branch’s root nodev. The resulting pruned tree is\\ndenoted by T−Tv.\\n3. A sub-tree T−Tv is called a pruned sub-tree of T. We indicate this with the\\nnotation T−Tv ≺Tor T≻T−Tv.\\nA basic decision tree pruning procedure is summarized in Algorithm 8.4.1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 319, 'page_label': '302'}, page_content='302 Controlling the Tree Shape\\nv1\\nv2\\nv4\\nv7\\nv5\\nv8 v9\\nv3\\nv6\\nv10 v11\\nFigure 8.7: The node v9 is a descendant of v2, and v2 is an ancestor of {v4,v5,v7,v8,v9}, but\\nv6 is not a descendant of v2.\\nv1\\nv2\\nv4\\nv7\\nv5\\nv8 v9\\nv3\\nv6\\nv10 v11\\n(a) T\\nv2\\nv4\\nv7\\nv5\\nv8 v9 (b) Tv2\\nv1\\nv2 v3\\nv6\\nv10 v11 (c) T−Tv2\\nFigure 8.8: The pruned tree T−Tv2 in (c) is the result of pruning theTv2 branch in (b) from\\nthe original tree Tin (a).\\nAlgorithm 8.4.1: Decision Tree Pruning\\nInput: Training set τ.\\nOutput: Sequence of decision trees T0 ≻T1 ≻···\\n1 Build a large decision tree T0 via Algorithm 8.2.1. [A possible termination\\ncriterion for that algorithm is to have some small predetermined number of data\\npoints at each terminal node of T0.]\\n2 T′←T0\\n3 k ←0\\n4 while T′has more than one node do\\n5 k ←k + 1\\n6 Choose v ∈T′.\\n7 Prune the branch rooted at v from T′.\\n8 Tk ←T′−Tv and T′←Tk.\\n9 return T0,T1,..., Tk'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 320, 'page_label': '303'}, page_content='Decision Trees and Ensemble Methods 303\\nLet T0 be the initial (deep) tree and let Tk be the tree obtained after the k-th pruning\\noperation, for k = 1,..., K. As soon as the sequence of trees T0 ≻T1 ≻···≻ TK is avail-\\nable, one can choose the best tree of {Tk}K\\nk=1 according to the smallest generalization risk.\\nSpecifically, we can split the data into training and validation sets. In this case, Algorithm\\n8.4.1 is executed using the training set and the generalization risks of{Tk}K\\nk=1 are estimated\\nvia the validation set.\\nWhile Algorithm 8.4.1 and the corresponding best tree selection process look appeal-\\ning, there is still an important question to consider; namely, how to choose the node v and\\nthe corresponding branch Tv in Line 6 of the algorithm. In order to overcome this problem,\\nBreiman proposed a method called cost complexity pruning, which we discuss next.\\n8.4.1 Cost-Complexity Pruning\\nLet T ≺T0 be a tree obtained via pruning of a tree T0. Denote the set of leaf (terminal)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 320, 'page_label': '303'}, page_content='Let T ≺T0 be a tree obtained via pruning of a tree T0. Denote the set of leaf (terminal)\\nnodes of Tby W. The number of leaves |W|is a measure for the complexity of the tree;\\nrecall that |W|is the number of regions {Rw}in the partition of X. Corresponding to each\\ntree Tis a prediction function g, as in (8.1). In cost-complexity pruning cost-complexity\\npruning\\nthe objective is to\\nfind a prediction function g (or, equivalently, tree T) that minimizes the training loss ℓτ(g)\\nwhile taking into account the complexity of the tree. The idea is to regularize the training\\nloss, similar to what was done in Chapter 6, by adding a penalty term for the complexity\\nof the tree. This leads to the following definition.\\nDefinition 8.2: Cost-Complexity Measure\\nLet τ = {(xi,yi)}n\\ni=1 be a data set and γ ⩾0 be a real number. For a given tree T, the\\ncost-complexity measure cost-complexity\\nmeasure\\nCτ(γ,T) is defined as:\\nCτ(γ,T) := 1\\nn\\nX\\nw∈W\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nnX\\ni=1\\n1{xi ∈Rw}Loss(yi,gw(xi))\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 + γ|W| (8.13)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 320, 'page_label': '303'}, page_content='Cτ(γ,T) := 1\\nn\\nX\\nw∈W\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nnX\\ni=1\\n1{xi ∈Rw}Loss(yi,gw(xi))\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 + γ|W| (8.13)\\n= ℓτ (g) + γ|W|,\\nwhere ℓτ (g) is the training loss (8.2).\\nSmall values of γresult in a small penalty for the tree complexity |W|, and thus large\\ntrees (that fit the entiretraining data well) will minimize the measureCτ(γ,T). In particular,\\nfor γ = 0, T = T0 will be the minimizer of Cτ(γ,T). On the other hand, large values of γ\\nwill prefer smaller trees or, more precisely, trees with fewer leaves. For su fficiently large\\nγ, the solution Twill collapse to a single (root) node.\\nIt can be shown that, for every value of γ, there exists a smallest minimizing sub-tree\\nof T0 with respect to the cost-complexity measure. In practice, a suitable γis selected via\\nobserving the performance of the learner on the validation set or by cross-validation.\\nThese advantages and the corresponding limitations are detailed next.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 321, 'page_label': '304'}, page_content='304 Controlling the Tree Shape\\n8.4.2 Advantages and Limitations of Decision Trees\\nWe list a number of advantages and disadvantages of decision trees, as compared with\\nother supervised learning methods such as were discussed in Chapters 5, 6, and 7.\\nAdvantages\\n1. The tree structure can handle both categorical and numerical features in a natural\\nand straightforward way. Specifically, there is no need to pre-process categorical\\nfeatures, say via the introduction of dummy variables.\\n2. The final tree obtained after the training phase can be compactly stored for the pur-\\npose of making predictions for new feature vectors. The prediction process only\\ninvolves a single tree traversal from the tree root to a leaf.\\n3. The hierarchical nature of decision trees allows for an e fficient encoding of the fea-\\nture’s conditional information. Specifically, after an internal split of a feature xj via\\nthe standard splitting rule (8.10), Algorithm 8.2.1 will only consider such subsets of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 321, 'page_label': '304'}, page_content='the standard splitting rule (8.10), Algorithm 8.2.1 will only consider such subsets of\\ndata that were constructed based on this split, thus implicitly exploiting the corres-\\nponding conditional information from the initial split of xj.\\n4. The tree structure can be easily understood and interpreted by domain experts with\\nlittle statistical knowledge, since it is essentially a logical decision flow diagram.\\n5. The sequential decision tree growth procedure in Algorithm 8.2.1, and in particular\\nthe fact that the tree has been split using the most important features, provides an\\nimplicit step-wise variable elimination procedure. In addition, the partition of the\\nvariable space into smaller regions results in simpler prediction problems in these\\nregions.\\n6. Decision trees are invariant under monotone transformations of the data. To see this,\\nconsider the (optimal) splitting rule s(x) = 1{x3 ⩽2}, where x3 is a positive feature.\\nSuppose that x3 is transformed to x′\\n3 = x2'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 321, 'page_label': '304'}, page_content='Suppose that x3 is transformed to x′\\n3 = x2\\n3. Now, the optimal splitting rule will take\\nthe form s(x) = 1{x′\\n3 ⩽4}.\\n7. In the classification setting, it is common to report not only the predicted value of a\\nfeature vector, e.g., as in (8.6), but also the respective class probabilities. Decision\\ntrees handle this task without any additional effort. Specifically, consider a new fea-\\nture vector. During the estimation process, we will perform a tree traversal and the\\npoint will end up in a certain leaf w. The probability of this feature vector lying in\\nclass z can be estimated as the proportion of training points in w that are in class z.\\n8. As each training point is treated equally in the construction of a tree, the structure of\\nthe tree will be relatively robust to outliers. In a way, trees exhibit a similar kind of\\nrobustness as the sample median does for real-valued data.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 322, 'page_label': '305'}, page_content='Decision Trees and Ensemble Methods 305\\nLimitations\\nDespite the fact that the decision trees are extremely interpretable, the predictive accuracy\\nis generally inferior to other established statistical learning methods. In addition, decision\\ntrees, and in particular very deep trees that were not subject to pruning, are heavily reliant\\non their training set. A small change in the training set can result in a dramatic change of the\\nresulting decision tree. Their inferior predictive accuracy, however, is a direct consequence\\nof the bias–variance tradeoff. Specifically, a decision tree model generally exhibits a high\\nvariance. To overcome the above limitations, several promising approaches such as bag-\\nging, random forest, and boosting are introduced below.\\nThe bagging approach was initially introduced in the context of an ensemble of\\ndecision trees. However, both the bagging and the boosting methods can be applied\\nto improve the accuracy of general prediction functions.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 322, 'page_label': '305'}, page_content='to improve the accuracy of general prediction functions.\\n8.5 Bootstrap Aggregation\\nThe major idea of the bootstrap aggregation or bagging baggingmethod is to combine prediction\\nfunctions learned from multiple data sets, with a view to improving overall prediction\\naccuracy. Bagging is especially beneficial when dealing with predictors that tend to overfit\\nthe data, such as in decision trees, where the (unpruned) tree structure is very sensitive to\\nsmall changes in the training set [37, 55].\\nTo start with, consider an idealized setting for a regression tree, where we have access\\nto B iid copies3 T1,..., TB of a training set T. Then, we can train B separate regression\\nmodels (B different decision trees) using these sets, giving learners gT1 ,..., gTB , and take\\ntheir average:\\ngavg(x) = 1\\nB\\nBX\\nb=1\\ngTb (x). (8.14)\\nBy the law of large numbers, as B → ∞, the average prediction function converges to ☞ 445\\nthe expected prediction function g† := EgT. The following result shows that using g† as'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 322, 'page_label': '305'}, page_content='the expected prediction function g† := EgT. The following result shows that using g† as\\na prediction function (if it were known) would result in an expected squared-error gen-\\neralization risk that is less than or equal to the expected generalization risk for a general ☞ 24\\nprediction function gT. It thus suggests that taking an average of prediction functions may\\nlead to a better expected squared-error generalization risk.\\nTheorem 8.1: Expected Squared-Error Generalization Risk\\nLet Tbe a random training set and let X,Y be a random feature vector and response\\nthat are independent of T. Then,\\nE\\n\\x12\\nY −gT(X)\\n\\x132\\n⩾E\\n\\x10\\nY −g†(X)\\n\\x112\\n.\\n3In this section Tk means the k-th training set, not a training set of size k.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 323, 'page_label': '306'}, page_content='306 Bootstrap Aggregation\\nProof: We have\\nE\\n\"\\x12\\nY −gT(X)\\n\\x132 \\x0c\\x0c\\x0c\\x0c\\x0c X,Y\\n#\\n⩾\\n\\x12\\nE[Y |X,Y] −E[gT(X) |X,Y]\\n\\x132\\n=\\n\\x12\\nY −g†(X)\\n\\x132\\n,\\nwhere the inequality follows from EU2 ⩾(EU)2 for any (conditional) expectation. Con-\\nsequently, by the tower property,☞431\\nE\\n\\x12\\nY −gT(X)\\n\\x132\\n= E\\nh\\nE\\nh\\x00Y −gT(X)\\x012 |X,Y\\nii\\n⩾E\\n\\x12\\nY −g†(X)\\n\\x132\\n.\\n□\\nUnfortunately, multiple independent data sets are rarely available. But we can substi-\\ntute them by bootstrapped ones. Specifically, instead of the T1,..., TB sets, we can obtain\\nrandom training sets T∗\\n1 ,..., T∗\\nB by resampling them from a single (fixed) training set τ,☞76\\nsimilar to Algorithm 3.2.6, and use them to train B separate models. By model averaging\\nas in (8.14) we obtain the bootstrapped aggregated estimator or bagged estimatorbagged\\nestimator\\nof the\\nform:\\ngbag(x) = 1\\nB\\nBX\\nb=1\\ngT∗\\nb\\n(x). (8.15)\\nAlgorithm 8.5.1: Bootstrap Aggregation Sampling\\nInput: Training set τ= {(xi,yi)}n\\ni=1 and resample size B.\\nOutput: Bootstrapped data sets.\\n1 for b = 1 to B do\\n2 T∗\\nb ←∅'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 323, 'page_label': '306'}, page_content='i=1 and resample size B.\\nOutput: Bootstrapped data sets.\\n1 for b = 1 to B do\\n2 T∗\\nb ←∅\\n3 for i = 1 to n do\\n4 Draw U ∼U(0,1)\\n5 I ←⌈nU⌉ // select random index\\n6 T∗\\nb ←T∗\\nb ∪{(xI,yI)}.\\n7 return T∗\\nb ,b = 1,..., B.\\nRemark 8.1 (Bootstrap Aggregation for Classification Problems) Note that (8.15)\\nis suitable for handling regression problems. However, the bagging idea can be readily\\nextended to handle classification settings as well. For example, gbag can take the majority\\nvote among {gT∗\\nb\\n},b = 1,..., B; that is, to accept the most frequent class among B predict-\\nors.\\nWhile bagging can be applied for any statistical model (such as decision trees, neural\\nnetworks, linear regression, K-nearest neighbors, and so on), it is most e ffective for pre-\\ndictors that are sensitive to small changes in the training set. The reason becomes clear\\nwhen we decompose the expected generalization risk as\\nEℓ(gT) = ℓ∗+ E(E[gT(X) |X] −g∗(X))2\\n|                           {z                           }'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 323, 'page_label': '306'}, page_content='Eℓ(gT) = ℓ∗+ E(E[gT(X) |X] −g∗(X))2\\n|                           {z                           }\\nexpected squared bias\\n+ E[Var[gT(X) |X]]|                 {z                 }\\nexpected variance\\n, (8.16)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 324, 'page_label': '307'}, page_content='Decision Trees and Ensemble Methods 307\\nsimilar to (2.22). Compare this with the same decomposition for the average prediction ☞ 35\\nfunction gbag in (8.14). As Egbag(x) = EgT(x), we see that any possible improvement in\\nthe generalization risk must be due to the expected variance term. Averaging and bagging\\nare thus only useful for predictors with a large expected variance, relative to the other two\\nterms. Examples of such “unstable” predictors include decision trees, neural networks, and\\nsubset selection in linear regression [22]. On the other hand, “stable” predictors are in-\\nsensitive to small data changes, an example being the K-nearest neighbors method. Note\\nthat for independent training sets T1,..., TB a reduction of the variance by a factor B is\\nachieved: Var gbag(x) = B−1Var gT(x). Again, it depends on the squared bias and irredu-\\ncible loss how significant this reduction is for the generalization risk.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 324, 'page_label': '307'}, page_content='cible loss how significant this reduction is for the generalization risk.\\nRemark 8.2 (Limitations of Bagging) It is important to remember that gbag is not ex-\\nactly equal to gavg, which in turn is not exactlyg†. Specifically, gbag is constructed from the\\nbootstrap approximation of the sampling pdf f . As a consequence, for stable predictors,\\nit can happen that gbag will perform worse than gT. In addition to the deterioration of the\\nbagging performance for stable procedures, it can also happen thatgT has already achieved\\na near optimal predictive accuracy given the available training data. In this case, bagging\\nwill not introduce a significant improvement.\\nThe bagging process provides an opportunity to estimate the generalization risk of\\nthe bagged model without an additional test set. Specifically, recall that we obtain the\\nT∗\\n1 ,..., T∗\\nB sets from a single training set τby sampling via Algorithm 8.5.1, and use them'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 324, 'page_label': '307'}, page_content='T∗\\n1 ,..., T∗\\nB sets from a single training set τby sampling via Algorithm 8.5.1, and use them\\nto train B separate models. It can be shown (see Exercise 8) that, for large sample sizes, on\\naverage about a third (more precisely, a fraction e−1 ≈0.37) of the original sample points\\nare not included in bootstrapped set T∗\\nb for 1 ⩽b ⩽B. Therefore, these samples can be\\nused for the loss estimation. These samples are called out-of-bag out-of-bag(OOB) observations.\\nSpecifically, for each sample from the original data set, we calculate the OOB loss using\\npredictors that were trained without this particular sample. The estimation procedure is\\nsummarized in Algorithm 8.5.2. Hastie et al. [55] observe that, under certain conditions, the\\nOOB loss is almost identical to the n-fold cross-validation loss. In addition, the OOB loss\\ncan be used to determine the number of trees required. Specifically, we can train predictors'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 324, 'page_label': '307'}, page_content='can be used to determine the number of trees required. Specifically, we can train predictors\\nuntil the OOB loss stops changing. Namely, decision trees are added until the OOB loss\\nstabilizes.\\nAlgorithm 8.5.2: Out-of-Bag Loss Estimation\\nInput: The original data set τ= {(x1,y1),..., (xn,yn)}, the bootstrapped data sets\\n{T∗\\n1 ,..., T∗\\nB}, and the trained predictors\\nn\\ngT∗\\n1 ,..., gT∗\\nB\\no\\n.\\nOutput: Out-of-bag loss for the averaged model.\\n1 for i = 1 to n do\\n2 Ci ←∅ // Indices of predictors not depending on (xi,yi)\\n3 for b = 1 to B do\\n4 if (xi,yi) < T∗\\nb then Ci ←Ci ∪{b}\\n5 Y′\\ni ←|Ci|−1 P\\nb∈Ci gT∗\\nb\\n(xi)\\n6 Li ←Loss\\n\\x10\\nyi,Y′\\ni\\n\\x11\\n7 LOOB ←1\\nn\\nPn\\ni=1 Li\\n8 return LOOB.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 325, 'page_label': '308'}, page_content='308 Bootstrap Aggregation\\nExample 8.3 (Bagging for a Regression Tree) We next proceed with a basic bagging\\nexample for a regression tree, in which we compare the decision tree estimator with the\\ncorresponding bagged estimator. We use the R2 metric (coefficient of determination) for\\ncomparison.\\nBaggingExample.py\\nimport numpy as np\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\nnp.random.seed(100)\\n# create regression problem\\nn_points = 1000 # points\\nx, y = make_friedman1(n_samples=n_points, n_features=15,\\nnoise=1.0, random_state=100)\\n# split to train/test set\\nx_train, x_test, y_train, y_test = \\\\\\ntrain_test_split(x, y, test_size=0.33, random_state=100)\\n# training\\nregTree = DecisionTreeRegressor(random_state=100)\\nregTree.fit(x_train,y_train)\\n# test\\nyhat = regTree.predict(x_test)\\n# Bagging construction\\nn_estimators=500'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 325, 'page_label': '308'}, page_content='# test\\nyhat = regTree.predict(x_test)\\n# Bagging construction\\nn_estimators=500\\nbag = np.empty((n_estimators), dtype= object )\\nbootstrap_ds_arr = np.empty((n_estimators), dtype= object )\\nfor i in range (n_estimators):\\n# sample bootstrapped data set\\nids = np.random.choice( range (0,len (x_train)),size= len (x_train),\\nreplace=True)\\nx_boot = x_train[ids]\\ny_boot = y_train[ids]\\nbootstrap_ds_arr[i] = np.unique(ids)\\nbag[i] = DecisionTreeRegressor()\\nbag[i].fit(x_boot,y_boot)\\n# bagging prediction\\nyhatbag = np.zeros( len (y_test))\\nfor i in range (n_estimators):\\nyhatbag = yhatbag + bag[i].predict(x_test)\\nyhatbag = yhatbag/n_estimators\\n# out of bag loss estimation'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 326, 'page_label': '309'}, page_content='Decision Trees and Ensemble Methods 309\\noob_pred_arr = np.zeros( len (x_train))\\nfor i in range (len (x_train)):\\nx = x_train[i].reshape(1, -1)\\nC = []\\nfor b in range (n_estimators):\\nif (np.isin(i, bootstrap_ds_arr[b])==False):\\nC.append(b)\\nfor pred in bag[C]:\\noob_pred_arr[i] = oob_pred_arr[i] + (pred.predict(x)/ len (C))\\nL_oob = r2_score(y_train, oob_pred_arr)\\nprint (\"DecisionTreeRegressor R^2 score = \",r2_score(y_test, yhat),\\n\"\\\\nBagging R^2 score = \", r2_score(y_test, yhatbag),\\n\"\\\\nBagging OOB R^2 score = \",L_oob)\\nDecisionTreeRegressor R^2 score = 0.575438224929718\\nBagging R^2 score = 0.7612121189201985\\nBagging OOB R^2 score = 0.7758253149069059\\nThe decision tree bagging improves the test-set R2 score by about 32% (from 0 .575\\nto 0.761). Moreover, the OOB score (0 .776) is very close to the true generalization risk\\n(0.761) of the bagged estimator.\\nThe bagging procedure can be further enhanced by introducing random forests, which\\nis discussed next.\\n8.6 Random Forests'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 326, 'page_label': '309'}, page_content='is discussed next.\\n8.6 Random Forests\\nIn Section 8.5, we discussed the intuition behind the prediction averaging procedure. Spe-\\ncifically, for some feature vector x let Zb = gTb (x),b = 1,2,..., B be iid prediction val-\\nues, obtained from independent training sets T1,..., TB. Suppose that Var Zb = σ2 for all\\nb = 1,..., B. Then the variance of the average prediction value ZB is equal to σ2/B. How-\\never, if bootstrapped data sets {T∗\\nb }are used instead, the corresponding random variables\\n{Zb}will be correlated. In particular, Zb = gT∗\\nb\\n(x) for b = 1,..., B are identically distrib-\\nuted (but not independent) with some positive pairwise correlationϱ. It then holds that (see\\nExercise 9)\\nVar ZB = ϱσ2 + σ2 (1 −ϱ)\\nB . (8.17)\\nWhile the second term of (8.17) goes to zero as the number of observationB increases, the\\nfirst term remains constant.\\nThis issue is particularly relevant for bagging with decision trees. For example, con-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 326, 'page_label': '309'}, page_content='This issue is particularly relevant for bagging with decision trees. For example, con-\\nsider a situation in which there exists a feature that provides a very good split of the data.\\nSuch a feature will be selected and split for every {gT∗\\nb\\n}B\\nb=1 at the root level and we will\\nconsequently end up with highly correlated predictions. In such a situation, prediction\\naveraging will not introduce the desired improvement in the performance of the bagged\\npredictor.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 327, 'page_label': '310'}, page_content='310 Random Forests\\nThe major idea of random forests is to perform bagging in combination with a “decor-\\nrelation” of the trees by including only a subset of features during the tree construction. For\\neach bootstrapped training setT∗\\nb we build a decision tree using a randomly selected subset\\nof m ⩽p features for the splitting rules. This simple but powerful idea will decorrelate the\\ntrees, since strong predictors will have a smaller chance to be considered at the root levels.\\nConsequentially, we can expect to improve the predictive performance of the bagged\\nestimator. The resulting predictor (random forest) construction is summarized in Algorithm\\n8.6.1.\\nAlgorithm 8.6.1: Random Forest Construction\\nInput: Training set τ= {(xi,yi)}n\\ni=1, the number of trees in the forest B, and the\\nnumber m ⩽p of features to be included, where p is the total number of\\nfeatures in x.\\nOutput: Ensemble of trees.\\n1 Generate bootstrapped training sets {T∗\\n1 ,..., T∗\\nB}via Algorithm 8.5.1.\\n2 for b = 1 to B do'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 327, 'page_label': '310'}, page_content='1 Generate bootstrapped training sets {T∗\\n1 ,..., T∗\\nB}via Algorithm 8.5.1.\\n2 for b = 1 to B do\\n3 Train a decision tree gT∗\\nb\\nvia Algorithm 8.2.1, where each split is performed\\nusing m randomly selected features out of p.\\n4 return {gT∗\\nb\\n}B\\nb=1.\\nFor regression problems, the output of Algorithm 8.6.1 is combined to yield the random\\nforest prediction function:\\ngRF(x) = 1\\nB\\nBX\\nb=1\\ngT∗\\nb\\n(x).\\nIn the classification setting, similar to Remark 8.1, we take instead the majority vote from\\nthe {gT∗\\nb\\n}.\\nExample 8.4 (Random Forest for a Regression Tree) We continue with the basic\\nbagging Example 8.3 for a regression tree, in which we compared the decision tree es-\\ntimator with the corresponding bagged estimator. Here, however, we use the random forest\\nwith B = 500 trees and a subset sizem = 8. It can be seen that the random forest’sR2 score\\nis outperforming that of the bagged estimator.\\nBaggingExampleRF.py\\nfrom sklearn.datasets import make_friedman1'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 327, 'page_label': '310'}, page_content='BaggingExampleRF.py\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\nfrom sklearn.ensemble import RandomForestRegressor\\n# create regression problem\\nn_points = 1000 # points\\nx, y = make_friedman1(n_samples=n_points, n_features=15,\\nnoise=1.0, random_state=100)\\n# split to train/test set\\nx_train, x_test, y_train, y_test = \\\\\\ntrain_test_split(x, y, test_size=0.33, random_state=100)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 328, 'page_label': '311'}, page_content='Decision Trees and Ensemble Methods 311\\nrf = RandomForestRegressor(n_estimators=500, oob_score = True,\\nmax_features=8,random_state=100)\\nrf.fit(x_train,y_train)\\nyhatrf = rf.predict(x_test)\\nprint (\"RF R^2 score = \", r2_score(y_test, yhatrf),\\n\"\\\\nRF OOB R^2 score = \", rf.oob_score_)\\nRF R^2 score = 0.8106589580845707\\nRF OOB R^2 score = 0.8260541058404149\\nRemark 8.3 (The Optimal Number of Subset Features m) The default values for m\\nare ⌊p/3⌋and\\nj√p\\nk\\nfor regression and classification setting, respectively. However, the\\nstandard practice is to treat m as a hyperparameter that requires tuning, depending on the\\nspecific problem at hand [55].\\nNote that the procedure of bagging decision trees is a special case of a random forest\\nconstruction (see Exercise 11). Consequently, the OOB loss is readily available for random\\nforests.\\nWhile the advantage of bagging in the sense of enhanced accuracy is clear, we should'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 328, 'page_label': '311'}, page_content='forests.\\nWhile the advantage of bagging in the sense of enhanced accuracy is clear, we should\\nalso consider its negative aspects and, in particular, the loss of interpretability. Specifically\\na random forest consists of many trees, thus making the prediction process both hard to\\nvisualize and interpret. For example, given a random forest, it is not easy to determine a\\nsubset of features that are essential for accurate prediction.\\nThe feature importance measure intends to address this issue. The idea is as follows.\\nEach internal node of a decision tree induces a certain decrease in the training loss; see\\n(8.9). Let us denote this decrease in the training loss by ∆Loss(v), where v is not a leaf node\\nof T. In addition, recall that for splitting rules of the type 1{xj ⩽ξ}(1 ⩽j ⩽p), each node\\nv is associated with a feature xj that determines the split. Using the above definitions, we\\ncan define the feature importance feature\\nimportance\\nof xj as\\nIT(xj) =\\nX\\nv internal ∈T'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 328, 'page_label': '311'}, page_content='can define the feature importance feature\\nimportance\\nof xj as\\nIT(xj) =\\nX\\nv internal ∈T\\n∆Loss(v) 1{xj is associated with v}, 1 ⩽j ⩽p. (8.18)\\nWhile (8.18) is defined for a single tree, it can be readily extended to random forests.\\nSpecifically, the feature importance in that case will be averaged over all trees of the forest;\\nthat is, for a forest consisting of B trees {T1,..., TB}, the feature importance measure is:\\nIRF(xj) = 1\\nB\\nBX\\nb=1\\nITb (xj), 1 ⩽j ⩽p. (8.19)\\nExample 8.5 (Feature Importance) We consider a classification problem with 15 fea-\\ntures. The data is specifically designed to contain only 5 informative features out of 15.\\nIn the code below, we apply the random forest procedure and calculate the corresponding\\nfeature importance measures, which are summarized in Figure 8.9.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 329, 'page_label': '312'}, page_content='312 Random Forests\\nVarImportance.py\\nimport numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.ensemble import RandomForestClassifier\\nimport matplotlib.pyplot as plt, pylab\\nn_points = 1000 # create regression data with 1000 data points\\nx, y = make_classification(n_samples=n_points, n_features=15,\\nn_informative=5, n_redundant=0, n_repeated=0, random_state=100,\\nshuffle=False)\\nrf = RandomForestClassifier(n_estimators=200, max_features=\"log2\")\\nrf.fit(x,y)\\nimportances = rf.feature_importances_\\nindices = np.argsort(importances)[::-1]\\nfor f in range (15):\\nprint (\"Feature %d (%f)\" % (indices[f]+1, importances[indices[f\\n]]))\\nstd = np.std([rf.feature_importances_ for tree in rf.estimators_],\\naxis=0)\\nf = plt.figure()\\nplt.bar( range (x.shape[1]), importances[indices],\\ncolor=\"b\", yerr=std[indices], align=\"center\")\\nplt.xticks( range (x.shape[1]), indices+1)\\nplt.xlim([-1, x.shape[1]])\\npylab.xlabel(\"feature index\")\\npylab.ylabel(\"importance\")\\nplt.show()'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 329, 'page_label': '312'}, page_content='plt.xlim([-1, x.shape[1]])\\npylab.xlabel(\"feature index\")\\npylab.ylabel(\"importance\")\\nplt.show()\\n5 1 2 4 3 7 11 13 6 9 15 8 14 10 12\\n0\\n0.1\\n0.2\\nfeature index\\nimportance\\nFigure 8.9: Importance measure for the 15-feature data set with only 5 informative features\\nx1,x2,x3,x4, and x5.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 330, 'page_label': '313'}, page_content='Decision Trees and Ensemble Methods 313\\nClearly, it is hard to visualize and understand the prediction process based on 200 trees.\\nHowever, Figure 8.9 shows that the features x1,x2,x3,x4,and x5 were correctly identified\\nas being important.\\n8.7 Boosting\\nBoosting is a powerful idea that aims to improve the accuracy of any learning algorithm,\\nespecially when involving weak learners weak learners— simple prediction functions that exhibit per-\\nformance slightly better than random guessing. Shallow decision trees typically yield weak\\nlearners.\\nOriginally, boosting was developed for binary classification tasks, but it can be readily\\nextended to handle general classification and regression problems. The boosting approach\\nhas some similarity with the bagging method in the sense that boosting uses an ensemble of\\nprediction functions. Despite this similarity, there exists a fundamental difference between\\nthese methods. Specifically, while bagging involves the fitting of prediction functions to'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 330, 'page_label': '313'}, page_content='these methods. Specifically, while bagging involves the fitting of prediction functions to\\nbootstrapped data, the predicting functions in boosting are learned sequentially. That is,\\neach learner uses information from previous learners.\\nThe idea is to start with a simple model (weak learner) g0 for the data τ = {(xi,yi)}n\\ni=1\\nand then to improve or “boost” this learner to a learner g1 := g0 + h1. Here, the function h1\\nis found by minimizing the training loss for g0 + h1 over all functions h in some class of\\nfunctions H. For example, Hcould be the set of prediction functions that can be obtained\\nvia a decision tree of maximal depth 2. Given a loss function Loss, the function h1 is thus\\nobtained as the solution to the optimization problem\\nh1 = argmin\\nh∈H\\n1\\nn\\nnX\\ni=1\\nLoss (yi,g0(xi) + h (xi)). (8.20)\\nThis process can be repeated for g1 to obtain g2 = g1 + h2, and so on, yielding the boosted\\nprediction function\\ngB(x) = g0(x) +\\nBX\\nb=1\\nhb(x). (8.21)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 330, 'page_label': '313'}, page_content='prediction function\\ngB(x) = g0(x) +\\nBX\\nb=1\\nhb(x). (8.21)\\nInstead of using the updating step gb = gb−1 + hb, one prefers to use the smooth updating\\nstep gb = gb−1 + γhb, for some suitably chosen step-size parameter γ. As we shall see\\nshortly, this helps reduce overfitting.\\nBoosting can be used for regression and classification problems. We start with a simple\\nregression setting, using the squared-error loss; thus, Loss( y,by) = (y −by)2. In this case, it\\nis common to start with g0(x) = n−1 Pn\\ni=1 yi, and each hb for b = 1,..., B is chosen as a\\nlearner for the data set τb of residuals corresponding to gb−1. That is, τb :=\\nn\\x10\\nxi,e(b)\\ni\\n\\x11on\\ni=1,\\nwith\\ne(b)\\ni := yi −gb−1(xi). (8.22)\\nThis leads to the following boosting procedure for regression with squared-error loss.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 331, 'page_label': '314'}, page_content='314 Boosting\\nAlgorithm 8.7.1: Regression Boosting with Squared-Error Loss\\nInput: Training set τ= {(xi,yi)}n\\ni=1, the number of boosting rounds B, and a\\nshrinkage step-size parameter γ.\\nOutput: Boosted prediction function.\\n1 Set g0(x) ←n−1 Pn\\ni=1 yi.\\n2 for b = 1 to B do\\n3 Set e(b)\\ni ←yi −gb−1(xi) for i = 1,..., n, and let τb ←\\nn\\x10\\nxi,e(b)\\ni\\n\\x11on\\ni=1.\\n4 Fit a prediction function hb on the training data τb.\\n5 Set gb(x) ←gb−1(x) + γhb(x).\\n6 return gB.\\nThe step-size parameter γstep-size\\nparameter γ\\nintroduced in Algorithm 8.7.1 controls the speed of the\\nfitting process. Specifically, for small values of γ, boosting takes smaller steps to-\\nwards the training loss minimization. The step-size γ is of great practical import-\\nance, since it helps the boosting algorithm to avoid overfitting. This phenomenon is\\ndemonstrated in Figure 8.10.\\n−2 −1 0 1 2\\n−50\\n0\\n50\\ntrain data\\ng1000, γ = 1\\n−2 −1 0 1 2\\n−50\\n0\\n50\\ntrain data\\ng1000, γ = 0. 005'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 331, 'page_label': '314'}, page_content='−2 −1 0 1 2\\n−50\\n0\\n50\\ntrain data\\ng1000, γ = 1\\n−2 −1 0 1 2\\n−50\\n0\\n50\\ntrain data\\ng1000, γ = 0. 005\\nFigure 8.10: The left and the right panels show the fitted boosting regression model g1000\\nwith γ= 1.0 and γ= 0.005, respectively. Note the overfitting on the left.\\nA very basic implementation of Algorithm 8.7.1 which reproduces Figure 8.10 is\\nprovided below.\\nRegressionBoosting.py\\nimport numpy as np\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import make_regression\\nimport matplotlib.pyplot as plt'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 332, 'page_label': '315'}, page_content=\"Decision Trees and Ensemble Methods 315\\ndef TrainBoost(alpha,BoostingRounds,x,y):\\ng_0 = np.mean(y)\\nresiduals = y-alpha*g_0\\n# list of basic regressor\\ng_boost = []\\nfor i in range (BoostingRounds):\\nh_i = DecisionTreeRegressor(max_depth=1)\\nh_i.fit(x,residuals)\\nresiduals = residuals - alpha*h_i.predict(x)\\ng_boost.append(h_i)\\nreturn g_0, g_boost\\ndef Predict(g_0, g_boost,alpha, x):\\nyhat = alpha*g_0*np.ones( len (x))\\nfor j in range (len (g_boost)):\\nyhat = yhat+alpha*g_boost[j].predict(x)\\nreturn yhat\\nnp.random.seed(1)\\nsz = 30\\n# create data set\\nx,y = make_regression(n_samples=sz, n_features=1, n_informative=1,\\nnoise=10.0)\\n# boosting algorithm\\nBoostingRounds = 1000\\nalphas = [1, 0.005]\\nfor alpha in alphas:\\ng_0, g_boost = TrainBoost(alpha,BoostingRounds,x,y)\\nyhat = Predict(g_0, g_boost, alpha, x)\\n# plot\\ntmpX = np.reshape(np.linspace(-2.5,2,1000),(1000,1))\\nyhatX = Predict(g_0, g_boost, alpha, tmpX)\\nf = plt.figure()\\nplt.plot(x,y, '*')\\nplt.plot(tmpX,yhatX)\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 332, 'page_label': '315'}, page_content=\"f = plt.figure()\\nplt.plot(x,y, '*')\\nplt.plot(tmpX,yhatX)\\nplt.show()\\nThe parameter γ can be viewed as a step size made in the direction of the negative\\ngradient of the squared-error training loss. To see this, note that the negative gradient\\n−∂Loss (yi,z)\\n∂z\\n\\x0c\\x0c\\x0c\\x0c\\x0cz=gb−1(xi)\\n= −∂ (yi −z)2\\n∂z\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cz=gb−1(xi)\\n= 2(yi −gb−1(xi))\\nis two times the residual e(b)\\ni given in (8.22) that is used in Algorithm 8.7.1 to fit the pre-\\ndiction function hb.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 333, 'page_label': '316'}, page_content='316 Boosting\\nIn fact, one of the major advances in the theory of boosting was the recognition that\\none can use a similar gradient descent method for any di fferentiable loss function. The\\nresulting algorithm is called gradient boostinggradient\\nboosting\\n. The general gradient boosting algorithm is\\nsummarized in Algorithm 8.7.2. The main idea is to mimic a gradient descent algorithm\\nin the following sense. At each stage of the boosting procedure, we calculate a negative☞412\\ngradient on n training points x1,..., xn (Lines 3–4). Then, we fit a simple model (such as\\na shallow decision tree) to approximate the gradient (Line 5) for any feature x. Finally,\\nsimilar to the gradient descent method, we make a γ-sized step in the direction of the\\nnegative gradient (Line 6).\\nAlgorithm 8.7.2: Gradient Boosting\\nInput: Training set τ= {(xi,yi)}n\\ni=1, the number of boosting rounds B, a\\ndifferentiable loss function Loss(y,by), and a gradient step-size parameter γ.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 333, 'page_label': '316'}, page_content='differentiable loss function Loss(y,by), and a gradient step-size parameter γ.\\nOutput: Gradient boosted prediction function.\\n1 Set g0(x) ←0.\\n2 for b = 1 to B do\\n3 for i = 1 to n do\\n4 Evaluate the negative gradient of the loss at (xi,yi) via\\nr(b)\\ni ←− ∂Loss (yi,z)\\n∂z\\n\\x0c\\x0c\\x0c\\x0c\\x0cz=gb−1(xi)\\ni = 1,..., n.\\n5 Approximate the negative gradient by solving\\nhb = argmin\\nh∈H\\n1\\nn\\nnX\\ni=0\\n\\x10\\nr(b)\\ni −h (xi)\\n\\x112\\n. (8.23)\\n6 Set gb(x) ←gb−1(x) + γhb(x).\\n7 return gB\\nExample 8.6 (Gradient Boosting for a Regression Tree) Let us continue with the ba-\\nsic bagging and random forest examples for a regression tree (Examples 8.3 and 8.4), where\\nwe compared the standard decision tree estimator with the corresponding bagging and ran-\\ndom forest estimators. Now, we use the gradient boosting estimator from Algorithm 8.7.2,\\nas implemented in sklearn. We use γ = 0.1 and perform B = 100 boosting rounds. As\\na prediction function hb for b = 1,..., B we use small regression trees of depth at most'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 333, 'page_label': '316'}, page_content='a prediction function hb for b = 1,..., B we use small regression trees of depth at most\\n3. Note that such individual trees do not usually give good performance; that is, they are\\nweak prediction functions. We can see that the resulting boosting prediction function gives\\nthe R2 score equal to 0.899, which is better than R2 scores of simple decision tree (0.5754),\\nthe bagged tree (0.761), and the random forest (0.8106).\\nGradientBoostingRegression.py\\nimport numpy as np\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.tree import DecisionTreeRegressor'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 334, 'page_label': '317'}, page_content='Decision Trees and Ensemble Methods 317\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n# create regression problem\\nn_points = 1000 # points\\nx, y = make_friedman1(n_samples=n_points, n_features=15,\\nnoise=1.0, random_state=100)\\n# split to train/test set\\nx_train, x_test, y_train, y_test = \\\\\\ntrain_test_split(x, y, test_size=0.33, random_state=100)\\n# boosting sklearn\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nbreg = GradientBoostingRegressor(learning_rate=0.1,\\nn_estimators=100, max_depth =3, random_state=100)\\nbreg.fit(x_train,y_train)\\nyhat = breg.predict(x_test)\\nprint (\"Gradient Boosting R^2 score = \",r2_score(y_test, yhat))\\nGradient Boosting R^2 score = 0.8993055635639531\\nWe proceed with the classification setting and consider the original boosting algorithm:\\nAdaBoost AdaBoost. The inventors of the AdaBoost method considered a binary classification prob-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 334, 'page_label': '317'}, page_content='AdaBoost AdaBoost. The inventors of the AdaBoost method considered a binary classification prob-\\nlem, where the response variable belongs to the{−1,1}set. The idea of AdaBoost is similar\\nto the one presented in the regression setting, that is, AdaBoost fits a sequence of prediction\\nfunctions g0,g1 = g0 + h1,g2 = g0 + h1 + h2,... with final prediction function\\ngB(x) = g0(x) +\\nBX\\nb=1\\nhb(x), (8.24)\\nwhere each function hb is of the form hb(x) = αb cb(x), with αb ∈R+ and where cb is a\\nproper (but weak) classifier in some classC. Thus, cb(x) ∈{−1,1}. Exactly as in (8.20), we\\nsolve at each boosting iteration the optimization problem\\n(αb,cb) = argmin\\nα⩾0,c∈C\\n1\\nn\\nnX\\ni=1\\nLoss (yi,gb−1(xi) + αc(xi)). (8.25)\\nHowever, in this case the loss function is defined as Loss(y,by) = e−yby. The algorithm starts\\nwith a simple model g0 := 0 and for each successive iteration b = 1,..., B solves (8.25).\\nThus,\\n(αb,cb) = argmin\\nα⩾0,c∈C\\nnX\\ni=1\\ne−yi gb−1(xi)\\n|    {z    }\\nw(b)\\ni\\ne−yi αc(xi) = argmin\\nα⩾0,c∈C\\nnX\\ni=1'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 334, 'page_label': '317'}, page_content='α⩾0,c∈C\\nnX\\ni=1\\ne−yi gb−1(xi)\\n|    {z    }\\nw(b)\\ni\\ne−yi αc(xi) = argmin\\nα⩾0,c∈C\\nnX\\ni=1\\nw(b)\\ni e−yiαc(xi),\\nwhere w(b)\\ni := exp{−yi gb−1(xi)}does not depend on αor c. It follows that\\n(αb,cb) = argmin\\nα⩾0,c∈C\\ne−α\\nnX\\ni=1\\nw(b)\\ni 1{c(xi) = yi}+ eα\\nnX\\ni=1\\nw(b)\\ni 1{c(xi) , yi}\\n= argmin\\nα⩾0,c∈C\\n(eα −e−α) ℓ(b)\\nτ (c) + e−α, (8.26)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 335, 'page_label': '318'}, page_content='318 Boosting\\nwhere\\nℓ(b)\\nτ (c) :=\\nPn\\ni=1 w(b)\\ni 1{c(xi) , yi}\\nPn\\ni=1 w(b)\\ni\\ncan be interpreted as the weighted zero–one training loss at iteration b.\\nFor any α ⩾0, the program (8.26) is minimized by a classifier c ∈C that minimizes\\nthis weighted training loss; that is,\\ncb(x) = argmin\\nc∈C\\nℓ(b)\\nτ . (8.27)\\nSubstituting (8.27) into (8.26) and solving for the optimal αgives\\nαb = 1\\n2 ln\\n 1 −ℓ(b)\\nτ (cb)\\nℓ(b)\\nτ (cb)\\n!\\n. (8.28)\\nThis gives the AdaBoost algorithm, summarized below.\\nAlgorithm 8.7.3: AdaBoost\\nInput: Training set τ= {(xi,yi)}n\\ni=1, and the number of boosting rounds B.\\nOutput: AdaBoost prediction function.\\n1 Set g0(x) ←0.\\n2 for i = 1 to n do\\n3 w(1)\\ni ←1/n\\n4 for b = 1 to B do\\n5 Fit a classifier cb on the training set τby solving\\ncb = argmin\\nc∈C\\nℓ(b)\\nτ (c) = argmin\\nc∈C\\nPn\\ni=1 w(b)\\ni 1{c(xi) , yi}\\nPn\\ni=1 w(b)\\ni\\n.\\n6 Set αb ←1\\n2 ln\\n 1 −ℓ(b)\\nτ (cb)\\nℓ(b)\\nτ (cb)\\n!\\n. // Update weights\\n7 for i = 1 to n do\\n8 w(b+1)\\ni ←w(b)\\ni exp{−yi αb cb(xi)}.\\n9 return gB(x) := PB\\nb=1 αb cb(x).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 335, 'page_label': '318'}, page_content='7 for i = 1 to n do\\n8 w(b+1)\\ni ←w(b)\\ni exp{−yi αb cb(xi)}.\\n9 return gB(x) := PB\\nb=1 αb cb(x).\\nAlgorithm 8.7.3 is quite intuitive. At the first step ( b = 1), AdaBoost assigns an equal\\nweight w(1)\\ni = 1/n to each training sample ( xi,yi) in the set τ = {(xi,yi)}n\\ni=1. Note that, in\\nthis case, the weighted zero–one training loss is equal to the regular zero–one training loss.\\nAt each successive step b >1, the weights of observations that were incorrectly classified\\nby the previous boosting prediction function gb are increased, and the weights of correctly\\nclassified observations are decreased. Due to the use of the weighted zero–one loss, the set\\nof incorrectly classified training samples will receive an extra weight and thus have a better\\nchance of being classified correctly by the next classifier cb+1. As soon as the AdaBoost\\nalgorithm finds the prediction function gB, the final classification is delivered via\\nsign\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nBX\\nb=1\\nαb cb(x)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 336, 'page_label': '319'}, page_content='Decision Trees and Ensemble Methods 319\\nThe step-size parameter αb found by the AdaBoost algorithm in Line 6 can be\\nviewed as an optimal step-size in the sense of training loss minimization. How-\\never, similar to the regression setting, one can slow down the AdaBoost algorithm\\nby setting αb to be a fixed (small) value αb = γ. As usual, when the latter is done in\\npractice, it is tackling the problem of overfitting.\\nWe consider an implementation of Algorithm 8.7.3 for a binary classification problem.\\nSpecifically, during all boosting rounds, we use simple decision trees of depth 1 (also called\\ndecision tree stumps stumps) as weak learners. The exponential and zero–one training losses as a\\nfunction of the number of boosting rounds are presented in Figure 8.11.\\nAdaBoost.py\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import zero_one_loss\\nimport numpy as np'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 336, 'page_label': '319'}, page_content='from sklearn.metrics import zero_one_loss\\nimport numpy as np\\ndef ExponentialLoss(y,yhat):\\nn = len (y)\\nloss = 0\\nfor i in range (n):\\nloss = loss+np.exp(-y[i]*yhat[i])\\nloss = loss/n\\nreturn loss\\n# create binary classification problem\\nnp.random.seed(100)\\nn_points = 100 # points\\nx, y = make_blobs(n_samples=n_points, n_features=5, centers=2,\\ncluster_std=20.0, random_state=100)\\ny[y==0]=-1\\n# AdaBoost implementation\\nBoostingRounds = 1000\\nn = len (x)\\nW = 1/n*np.ones(n)\\nLearner = []\\nalpha_b_arr = []\\nfor i in range (BoostingRounds):\\nclf = DecisionTreeClassifier(max_depth=1)\\nclf.fit(x,y, sample_weight=W)\\nLearner.append(clf)\\ntrain_pred = clf.predict(x)\\nerr_b = 0'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 337, 'page_label': '320'}, page_content='320 Boosting\\nfor i in range (n):\\nif (train_pred[i]!=y[i]):\\nerr_b = err_b+W[i]\\nerr_b = err_b/np. sum (W)\\nalpha_b = 0.5*np.log((1-err_b)/err_b)\\nalpha_b_arr.append(alpha_b)\\nfor i in range (n):\\nW[i] = W[i]*np.exp(-y[i]*alpha_b*train_pred[i])\\nyhat_boost = np.zeros( len (y))\\nfor j in range (BoostingRounds):\\nyhat_boost = yhat_boost+alpha_b_arr[j]*Learner[j].predict(x)\\nyhat = np.zeros(n)\\nyhat[yhat_boost>=0] = 1\\nyhat[yhat_boost<0] = -1\\nprint (\"AdaBoost Classifier exponential loss = \", ExponentialLoss(y,\\nyhat_boost))\\nprint (\"AdaBoost Classifier zero--one loss = \",zero_one_loss(y,yhat))\\nAdaBoost Classifier exponential loss = 0.004224013663777142\\nAdaBoost Classifier zero--one loss = 0.0\\n200 400 600 800 1 ,000\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nB\\nloss\\nexponential loss\\nzero-one loss\\nFigure 8.11: Exponential and zero–one training loss as a function of the number of boosting\\nrounds B for a binary classification problem.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 338, 'page_label': '321'}, page_content='Decision Trees and Ensemble Methods 321\\nFurther Reading\\nBreiman’s book on decision trees, [20], serves as a great starting point. Some additional\\nadvances can be found in [62, 96]. From the computational point of view, there exists\\nan efficient recursive procedure for tree pruning; see Chapters 3 and 10 in [20]. Several\\nadvantages and disadvantages of using decision trees are debated in [37, 55]. A detailed\\ndiscussion on bagging and random forests can be found in [21] and [23], respectively.\\nFreund and Schapire [44] provide the first boosting algorithm, the AdaBoost. While Ad-\\naBoost was developed in the context of the computational complexity of learning, it was\\nlater discovered by Friedman [45] that AdaBoost is a special case of an additive model.\\nIn addition, it was shown that for any di fferentiable loss function, there exists an e fficient\\nboosting procedure which mimics the gradient descent algorithm. The foundation of the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 338, 'page_label': '321'}, page_content='boosting procedure which mimics the gradient descent algorithm. The foundation of the\\nresulting gradient boosting method is detailed in [45]. Python packages that implement\\ngradient boosting include XGBoost and LightGBM.\\nExercises\\n1. Show that any training set τ = {(x,yi),i = 1,..., n}can be fitted via a tree with zero\\ntraining loss.\\n2. Suppose during the construction of a decision tree we wish to specify a constant re-\\ngional prediction function gw on the region Rw, based on the training data in Rw, say\\n{(x1,y1),..., (xk,yk)}. Show that gw(x) := k−1 Pk\\ni=1 yi minimizes the squared-error loss.\\n3. Using the program from Section 8.2.4, write a basic implementation of a decision tree\\nfor a binary classification problem. Implement the misclassification, Gini index, and en-\\ntropy impurity criteria to split nodes. Compare the results.\\n4. Suppose in the decision tree of Example 8.1, there are 3 blue and 2 red data points in'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 338, 'page_label': '321'}, page_content='4. Suppose in the decision tree of Example 8.1, there are 3 blue and 2 red data points in\\na certain tree region. Calculate the misclassification impurity, the Gini impurity, and the\\nentropy impurity. Repeat these calculations for 2 blue and 3 red data points.\\n5. Consider the procedure of finding the best splitting rule for a categorical variable with\\nk labels from Section 8.3.4. Show that one needs to consider 2k subsets of {1,..., k}to find\\nthe optimal partition of labels.\\n6. Reproduce Figure 8.6 using the following classification data.\\nfrom sklearn.datasets import make_blobs\\nX, y = make_blobs(n_samples=5000, n_features=10, centers=3,\\nrandom_state=10, cluster_std=10)\\n7. Prove (8.13); that is, show that\\nX\\nw∈W\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nnX\\ni=1\\n1{xi ∈Rw}Loss(yi,gw(xi))\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 = n ℓτ (g) .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 339, 'page_label': '322'}, page_content='322 Exercises\\n8. Suppose τ is a training set with n elements and τ∗, also of size n, is obtained from τ\\nby bootstrapping; that is, resampling with replacement. Show that for large n, τ∗does not\\ncontain a fraction of about e−1 ≈0.37 of the points from τ.\\n9. Prove Equation (8.17).\\n10. Consider the following training /test split of the data. Construct a random forest re-\\ngressor and identify the optimal subset size m in the sense of R2 score (see Remark 8.3).\\nimport numpy as np\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n# create regression problem\\nn_points = 1000 # points\\nx, y = make_friedman1(n_samples=n_points, n_features=15,\\nnoise=1.0, random_state=100)\\n# split to train/test set\\nx_train, x_test, y_train, y_test = \\\\\\ntrain_test_split(x, y, test_size=0.33, random_state=100)\\n11. Explain why bagging decision trees are a special case of random forests.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 339, 'page_label': '322'}, page_content='11. Explain why bagging decision trees are a special case of random forests.\\n12. Show that (8.28) holds.\\n13. Consider the following classification data and module imports:\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.metrics import zero_one_loss\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nX_train, y_train = make_blobs(n_samples=5000, n_features=10,\\ncenters=3, random_state=10, cluster_std=5)\\nUsing the gradient boosting algorithm with B = 100 rounds, plot the training loss as a\\nfunction of γ, for γ = 0.1,0.3,0.5,0.7,1. What is your conclusion regarding the relation\\nbetween B and γ?'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 340, 'page_label': '323'}, page_content='CHAPTER 9\\nDEEP LEARNING\\nIn this chapter, we show how one can construct a rich class of approximating func-\\ntions called neural networks. The learners belonging to the neural-network class of\\nfunctions have attractive properties that have made them ubiquitous in modern machine\\nlearning applications — their training is computationally feasible and their complexity\\nis easy to control and fine-tune.\\n9.1 Introduction\\nIn Chapter 2 we described the basic supervised learning task; namely, we wish to predict a\\nrandom output Y from a random inputX, using a prediction functiong : x 7→y that belongs\\nto a suitably chosen class of approximating functions G. More generally, we may wish to\\npredict a vector-valued output y using a prediction function g : x 7→y from class G.\\nIn this chapter y denotes the vector-valued output for a given input x. This di ffers\\nfrom our previous use (e.g., in Table 2.1), wherey denotes a vector of scalar outputs.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 340, 'page_label': '323'}, page_content='from our previous use (e.g., in Table 2.1), wherey denotes a vector of scalar outputs.\\nIn the machine learning context, the class Gis sometimes referred to as the hypothesis\\nspace or the universe of possible models, and the representational capacity of a hypothesis representational\\ncapacityspace Gis simply its complexity.\\nSuppose that we have a class of functions GL, indexed by a parameter L that controls\\nthe complexity of the class, so that GL ⊂ GL+1 ⊂ GL+2 ⊂ ···. In selecting a suitable\\nclass of functions, we have to be mindful of theapproximation–estimation tradeoff. On the ☞ 31\\none hand, the class GL must be complex (rich) enough to accurately represent the optimal\\nunknown prediction function g∗, which may require a very large L. On the other hand, the\\nlearners in the class GL must be simple enough to train with small estimation error and\\nwith minimal demands on computer memory, which may necessitate a small L.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 340, 'page_label': '323'}, page_content='with minimal demands on computer memory, which may necessitate a small L.\\nIn balancing these competing objectives, it helps if the more complex class GL+1 is\\neasily constructed from an already existing and simpler GL. The simpler class of functions\\nGL may itself be constructed by modifying an even simpler class GL−1, and so on.\\nA class of functions that permits such a natural hierarchical construction is the class of\\nneural networks. Conceptually, a neural network with L layers is a nonlinear parametric neural\\nnetworksregression model whose representational capacity can easily be controlled by L.\\n☞ 188323'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 341, 'page_label': '324'}, page_content='324 Introduction\\nAlternatively, in (9.3) we will define the output of a neural network as the repeated\\ncomposition of linear and (componentwise) nonlinear functions. As we shall see, this rep-\\nresentation of the output will provide a flexible class of nonlinear functions that can be\\neasily differentiated. As a result, the training of learners via gradient optimization methods\\ninvolves mostly standard matrix operations that can be performed very efficiently.☞412\\nHistorically, neural networks were originally intended to mimic the workings of the\\nhuman brain, with the network nodes modeling neurons and the network links modeling\\nthe axons connecting neurons. For this reason, rather than using the terminology of the\\nregression models in Chapter 5, we prefer to use a nomenclature inspired by the apparent\\nresemblance of neural networks to structures in the human brain.\\nWe note, however, that the attempts at building efficient machine learning algorithms by'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 341, 'page_label': '324'}, page_content='We note, however, that the attempts at building efficient machine learning algorithms by\\nmimicking the functioning of the human brain have been as unsuccessful as the attempts\\nat building flying aircraft by mimicking the flapping of birds’ wings. Instead, many ef-\\nfective machine algorithms have been inspired by age-old mathematical ideas for function\\napproximation. One such idea is the following fundamental result (see [119] for a proof).\\nTheorem 9.1: Kolmogorov (1957)\\nEvery continuous function g∗: [0,1]p 7→Rwith p ⩾2 can be written as\\ng∗(x) =\\n2p+1X\\nj=1\\nhj\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\npX\\ni=1\\nhi j(xi)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,\\nwhere {hj,hi j}is a set of univariate continuous functions that depend on g∗.\\nThis result tells us that any continuous high-dimensional map can be represented as\\nthe function composition of much simpler (one-dimensional) maps. The composition of\\nthe maps needed to compute the output g∗(x) for a given input x ∈Rp are depicted in'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 341, 'page_label': '324'}, page_content='the maps needed to compute the output g∗(x) for a given input x ∈Rp are depicted in\\nFigure 9.1, showing a directed graph or neural network with three layers, denoted as l =\\n0,1,2.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 342, 'page_label': '325'}, page_content='Deep Learning 325\\nhij\\nxi\\nxp\\nz j\\nh j\\n→ a j\\nz1\\nh1\\n→ a1\\nzq\\nhq\\n→ aq\\ng∗(x)\\nx1\\nhpq\\nh11\\nFigure 9.1: Every continuous function g∗: [0,1]p 7→Rcan be represented by a neural net-\\nwork with one hidden layer (l = 1), an input layer (l = 0), and an output layer (l = 2).\\nIn particular, each of the p components of the input x is represented as a node in the\\ninput layer (l = 0). In the hidden layer (l = 1) there are q := 2p + 1 nodes, each of which hidden layer\\nis associated with a pair of variables (z,a) with values\\nzj :=\\npX\\ni=1\\nhi j(xi) and aj := hj(zj).\\nA link between nodes ( zj,aj) and xi with weight hi j signifies that the value of zj depends\\non the value of xi via the function hi j. Finally, the output layer (l = 2) represents the value\\ng∗(x) = Pq\\nj=1 aj. Note that the arrows on the graph remind us that the sequence of the\\ncomputations is executed from left to right, or from the input layer l = 0 through to the\\noutput layer l = 2.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 342, 'page_label': '325'}, page_content='output layer l = 2.\\nIn practice, we do not know the collection of functions {hj,hi j}, because they depend\\non the unknown g∗. In the unlikely event that g∗ is linear, then all of the (2 p + 1)(p + 1)\\none-dimensional functions will be linear as well. However, in general, we should expect\\nthat each of the functions in {hj,hi j}is nonlinear.\\nUnfortunately, Theorem 9.1 only asserts the existence of {hj,hi j}, and does not tell us\\nhow to construct these nonlinear functions. One way out of this predicament is to replace\\nthese (2p + 1)(p + 1) unknown functions with a much larger number of known nonlinear\\nfunctions called activation functions.1 For example, a logistic activation function is activation\\nfunctions\\nS (z) = (1 + exp(−z))−1.\\nWe then hope that such a network, built from a su fficiently large number of activation\\nfunctions, will have similar representational capacity as the neural network in Figure 9.1\\nwith (2p + 1)(p + 1) functions.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 342, 'page_label': '325'}, page_content='with (2p + 1)(p + 1) functions.\\nIn general, we wish to use the simplest activation functions that will allow us to build\\na learner with large representational capacity and low training cost. The logistic function\\n1Activation functions derive their name from models of a neuron’s response when exposed to chemical\\nor electric stimuli.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 343, 'page_label': '326'}, page_content='326 Feed-Forward Neural Networks\\nis merely one possible choice for an activation function from among infinite possibilit-\\nies. Figure 9.2 shows a small selection of activation functions with di fferent regularity or\\nsmoothness properties.\\nHeaviside or unit step rectified linear unit (ReLU) logistic\\n-2 0 2\\n0\\n0.5\\n1\\n-2 0 2\\n0\\n1\\n2\\n3\\n-2 0 2\\n0\\n0.5\\n1\\n1{z ⩾0} z ×1{z ⩾0} (1 + exp(−z))−1\\nFigure 9.2: Some common activation functionsS (z) with their defining formulas and plots.\\nThe logistic function is an example of a sigmoid (that is, an S-shaped) function. Some\\nbooks define the logistic function as 2S (z) −1 (in terms of our definition).\\nIn addition to choosing the type and number of activation functions in a neural network,\\nwe can improve its representational capacity in another important way: introduce more\\nhidden layers. In the next section we explore this possibility in detail.\\n9.2 Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 343, 'page_label': '326'}, page_content='9.2 Feed-Forward Neural Networks\\nIn a neural network withL+1 layers, the zero or input layer (l = 0) encodes the input feature\\nvector x, and the last or output layer (l = L) encodes the (multivalued) output functiong(x).\\nThe remaining layers are called hidden layers. Each layer has a number of nodes, say pl\\nnodes for layer l = 0,..., L. In this notation, p0 is the dimension of the input feature vector\\nx and, for example, pL = 1 signifies that g(x) is a scalar output. All nodes in the hidden\\nlayers (l = 1,..., L −1) are associated with a pair of variables ( z,a), which we gather\\ninto pl-dimensional column vectors zl and al. In the so-called feed-forwardfeed-forward networks, the\\nvariables in any layer l are simple functions of the variables in the preceding layer l −1. In\\nparticular, zl and al−1 are related via the linear relation zl = Wl al−1 + bl, for some weight\\nmatrix Wl and bias vector bl.weight matrix'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 343, 'page_label': '326'}, page_content='matrix Wl and bias vector bl.weight matrix\\nbias vector Within any hidden layer l = 1,..., L −1, the components of the vectors zl and al\\nare related via al = Sl(zl), where Sl : Rpl 7→Rpl is a nonlinear multivalued function. All of\\nthese multivalued functions are typically of the form\\nSl(z) = [S (z1),..., S (zdim(z))]⊤, l = 1,..., L −1, (9.1)\\nwhere S is an activation function common to all hidden layers. The function SL : RpL−1 7→\\nRpL in the output layer is more general and its specification depends, for example, on\\nwhether the network is used for classification or for the prediction of a continuous output\\nY. A four-layer (L = 3) network is illustrated in Figure 9.3.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 344, 'page_label': '327'}, page_content='Deep Learning 327\\nInput layer Hidden layers Output layer\\nbias\\nweight\\nb1, j\\nz3,m\\nS3w1, ji\\nz2,k\\nS\\n→ a2,k\\nb2,k b3,m\\nz1, j\\nS\\n→ a1, j\\nw2,k j w3,mk g(x)xi\\nFigure 9.3: A neural network with L = 3: the l = 0 layer is the input layer, followed by two\\nhidden layers, and the output layer. Hidden layers may have different numbers of nodes.\\nThe output of this neural network is determined by the input vector x, (nonlinear)\\nfunctions {Sl}, as well as weight matrices Wl = [wl,i j] and bias vectors bl = [bl,j] for\\nl = 1,2,3.\\nHere, the (i, j)-th element of the weight matrix Wl = [wl,i j] is the weight that con-\\nnects the j-th node in the (l −1)-st layer with the i-th node in the l-th layer.\\nThe name given toL (the number of layers without the input layer) is thenetwork depth network depth\\nand maxl pl is called thenetwork width. While we mostly study networks that have an equal network width\\nnumber of nodes in the hidden layers ( p1 = ··· = pL−1), in general there can be di fferent'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 344, 'page_label': '327'}, page_content='number of nodes in the hidden layers ( p1 = ··· = pL−1), in general there can be di fferent\\nnumbers of nodes in each hidden layer.\\nThe output g(x) of a multiple-layer neural network is obtained from the input x via the\\nfollowing sequence of computations:\\nx|{z}\\na0\\n→W1 a0 + b1|       {z       }\\nz1\\n→S1(z1)|{z}\\na1\\n→W2 a1 + b2|       {z       }\\nz2\\n→S2(z2)|{z}\\na2\\n→···\\n→WL aL−1 + bL|          {z          }\\nzL\\n→SL(zL)|{z}\\naL\\n= g(x). (9.2)\\nDenoting the function z 7→Wl z + bl by Ml, the output g(x) can thus be written as the\\nfunction composition\\ng(x) = SL ◦ML ◦···◦ S2 ◦M2 ◦S1 ◦M1(x). (9.3)\\nThe algorithm for computing the output g(x) for an input x is summarized next. Note\\nthat we leave open the possibility that the activation functions{Sl}have different definitions'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 345, 'page_label': '328'}, page_content='328 Feed-Forward Neural Networks\\nfor each layer. In some cases, Sl may even depend on some or all of the already computed\\nz1,z2,... and a1,a2,... .\\nAlgorithm 9.2.1: Feed-Forward Propagation for a Neural Network\\ninput: Feature vector x; weights {wl,i j}, biases {bl,i}for each layer l = 1,..., L.\\noutput: The value of the prediction function g(x).\\n1 a0 ←x // the zero or input layer\\n2 for l = 1 to L do\\n3 Compute the hidden variable zl,i for each node i in layer l:\\nzl ←Wl al−1 + bl\\n4 Compute the activation function al,i for each node i in layer l:\\nal ←Sl(zl)\\n5 return g(x) ←aL // the output layer'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 346, 'page_label': '329'}, page_content='Deep Learning 329\\nExample 9.1 (Nonlinear Multi-Output Regression) Given the input x ∈Rp0 and an\\nactivation functionS : R7→R, the output g(x) := [g1(x),..., gp2 (x)]⊤of a nonlinear multi-\\noutput regression model can be computed via a neural network with: ☞ 214\\nz1 = W1 x + b1, where W1 ∈Rp1×p0 ,b1 ∈Rp1 ,\\na1,k = S (z1,k), k = 1,..., p1,\\ng(x) = W2 a1 + b2, where W2 ∈Rp2×p1 ,b2 ∈Rp2 ,\\nwhich is a neural network with one hidden layer and output function S2(z) = z. In the\\nspecial case where p1 = p2 = 1, b2 = 0,W2 = 1, and we collect all parameters into the\\nvector θ⊤= [b1,W1] ∈Rp0+1, the neural network can be interpreted as ageneralized linear\\nmodel with E[Y |X = x] = h([1,x⊤] θ) for some activation function h. ☞ 204\\nExample 9.2 (Multi-Logit Classification) Suppose that, for a classification problem,\\nan input x has to be classified into one ofc classes, labeled 0,..., c−1. We can perform the\\nclassification via a neural network with one hidden layer, with p1 = c nodes. In particular,\\nwe have'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 346, 'page_label': '329'}, page_content='we have\\nz1 = W1 x + b1, a1 = S1(z1),\\nwhere S1 is the softmax function: softmax\\nsoftmax : z 7→ exp(z)P\\nk exp(zk).\\nFor the output, we take g(x) = [g1(x),..., gc(x)]⊤ = a1, which can then be used as a\\npre-classifier of x. The actual classifier of x into one of the categories 0,1,..., c −1 is then ☞ 252\\nargmax\\nk ∈{0,...,c−1}\\ngk+1(x).\\nThis is equivalent to the multi-logit classifier in Section 7.5. Note, however, that there we ☞ 266\\nused a slightly di fferent notation, with ex instead of x and we have a reference class; see\\nExercise 13.\\nIn practical implementations, the softmax function can cause numerical over- and\\nunder-flow errors when either one of the exp( zk) happens to be extremely large orP\\nk exp(zk) happens to be very small. In such cases we can exploit the invariance\\nproperty (Exercise 1):\\nsoftmax(z) = softmax(z + c ×1) for any constant c.\\nUsing this property, we can compute softmax(z) with greater numerical stability via\\nsoftmax(z −maxk{zk}×1).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 346, 'page_label': '329'}, page_content='softmax(z −maxk{zk}×1).\\nWhen neural networks are used for classification into c classes and the number of out-\\nput nodes is c −1, then the gi(x) may be viewed as nonlinear discriminant functions. ☞ 260'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 347, 'page_label': '330'}, page_content='330 Feed-Forward Neural Networks\\nExample 9.3 (Density Estimation) Estimating the density f of some random feature\\nX ∈Ris the prototypical unsupervised learning task, which we tackled in Section 4.5.2 us-\\ning Gaussian mixture models. We can view a Gaussian mixture model withp1 components☞137\\nand a common scale parameter σ >0 as a neural network with two hidden layers, similar\\nto the one on Figure 9.3. In particular, if the activation function in the first hidden layer,\\nS1, is of the form (9.1) with S (z) := exp(−z2/(2σ2))/\\n√\\n2πσ2, then the density value g(x) is\\ncomputed via:\\nz1 = W1 x + b1, a1 = S1(z1),\\nz2 = W2 a1 + b2, a2 = S2(z2),\\ng(x) = a⊤\\n1 a2,\\nwhere W1 = 1 is a p1 ×1 column vector of ones,W2 = O is a p1 ×p1 matrix of zeros, andS2\\nis the softmax function. We identify the column vector b1 with the p1 location parameters,\\n[µ1,...,µ p1 ]⊤ of the Gaussian mixture and b2 ∈Rp1 with the p1 weights of the mixture.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 347, 'page_label': '330'}, page_content='[µ1,...,µ p1 ]⊤ of the Gaussian mixture and b2 ∈Rp1 with the p1 weights of the mixture.\\nNote the unusual activation function of the output layer — it requires the value of a1 from\\nthe first hidden layer and a2 from the second hidden layer.\\nThere are a number of key design characteristics of a feed-forward network. First, we\\nneed to choose the activation function(s). Second, we need to choose the loss function for\\nthe training of the network. As we shall explain in the next section, the most common\\nchoices are the ReLU activation function and the cross-entropy loss. Crucially, we need\\nto carefully construct the network architecture — the number of connections among thenetwork\\narchitecture nodes in different layers and the overall number of layers of the network.\\nFor example, if the connections from one layer to the next are pruned (called sparse\\nconnectivity) and the links share the same weight values {wl,i j}(called parameter sharing)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 347, 'page_label': '330'}, page_content='connectivity) and the links share the same weight values {wl,i j}(called parameter sharing)\\nfor all {(i, j) : |i −j|= 0,1,... }, then the weight matrices will be sparse and Toeplitz.☞379\\nIntuitively, the parameter sharing and sparse connectivity can speed up the training of\\nthe network, because there are fewer parameters to learn, and the Toeplitz structure permits\\nquick computation of the matrix-vector products in Algorithm 9.2.1. An important example\\nof such a network is the convolution neural network (CNN), in which some or all of theconvolution\\nneural\\nnetwork\\nnetwork layers encode the linear operation of convolution:\\nWl al−1 = wl ∗al−1,\\nwhere [ x ∗y]i := P\\nk xkyi−k+1. As discussed in Example A.10, a convolution matrix is a☞380\\nspecial type of sparse Toeplitz matrix, and its action on a vector of learning parameters can\\nbe evaluated quickly via the fast Fourier transform.☞394\\nCNNs are particularly suited to image processing problems, because their convolution'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 347, 'page_label': '330'}, page_content='CNNs are particularly suited to image processing problems, because their convolution\\nlayers closely mimic the neurological properties of the visual cortex. In particular, the\\ncortex partitions the visual field into many small regions and assigns a group of neurons to\\nevery such region. Moreover, some of these groups of neurons respond only to the presence\\nof particular features (for example, edges).\\nThis neurological property is naturally modeled via convolution layers in the neural\\nnetwork. Specifically, suppose that the input image is given by anm1 ×m2 matrix of pixels.\\nNow, define a k ×k matrix (sometimes called a kernel, where k is generally taken to be 3\\nor 5). Then, the convolution layer output can be calculated using the discrete convolution'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 348, 'page_label': '331'}, page_content='Deep Learning 331\\nof all possible k ×k input matrix regions and the kernel matrix; (see Example A.10). In\\nparticular, by noting that there are (m1 −k +1) ×(m2 −k +1) possible regions in the original\\nimage, we conclude that the convolution layer output size is ( m1 −k + 1) ×(m2 −k + 1).\\nIn practice, we frequently define several kernel matrices, giving an output layer of size\\n(m1 −k + 1) ×(m2 −k + 1) ×(the number of kernels). Figure 9.4 shows a 5×5 input image\\nand a 2 ×2 kernel with a 4 ×4 output matrix. An example of using a CNN for image\\nclassification is given in Section 9.5.2.\\nFigure 9.4: An example 5×5 input image and a 2×2 kernel. The kernel is applied to every\\n2 ×2 region of the original image.\\n9.3 Back-Propagation\\nThe training of neural networks is a major challenge that requires both ingenuity and much\\nexperimentation. The algorithms for training neural networks with great depth are collect-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 348, 'page_label': '331'}, page_content='experimentation. The algorithms for training neural networks with great depth are collect-\\nively referred to asdeep learning methods. One of the simplest and most effective methods deep learning\\nfor training is via steepest descent and its variations. ☞ 412Steepest descent requires computation of the gradient with respect to all bias vectors\\nand weight matrices. Given the potentially large number of parameters (weight and bias\\nterms) in a neural network, we need to find an efficient method to calculate this gradient.\\nTo illustrate the nature of the gradient computations, let θ= {Wl,bl}be a column vec-\\ntor of length dim( θ) = PL\\nl=1(pl−1 pl + pl) that collects all the weight parameters (number-\\ning PL\\nl=1 pl−1 pl) and bias parameters (numbering PL\\nl=1 pl) of a multiple-layer network with\\ntraining loss:\\nℓτ(g(·|θ)) := 1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi |θ)).\\nWriting Ci(θ) := Loss(yi,g(xi |θ)) for short (using C for cost), we have\\nℓτ(g(·|θ)) = 1\\nn\\nnX\\ni=1\\nCi(θ), (9.4)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 348, 'page_label': '331'}, page_content='ℓτ(g(·|θ)) = 1\\nn\\nnX\\ni=1\\nCi(θ), (9.4)\\nso that obtaining the gradient of ℓτ requires computation of ∂Ci/∂θ for every i. For ac-\\ntivation functions of the form (9.1), define Dl as the diagonal matrix with the vector of\\nderivatives\\nS′(z) := [S ′(zl,1),..., S ′(zl,pl )]⊤\\ndown its main diagonal; that is,\\nDl := diag(S ′(zl,1),..., S ′(zl,pl )), l = 1,..., L −1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 349, 'page_label': '332'}, page_content='332 Back-Propagation\\nThe following theorem provides us with the formulas needed to compute the gradient of a\\ntypical Ci(θ).\\nTheorem 9.2: Gradient of Training Loss\\nFor a given (input, output) pair ( x,y), let g(x |θ) be the output of Algorithm 9.2.1,\\nand let C(θ) = Loss(y,g(x |θ)) be an almost-everywhere di fferentiable loss func-\\ntion. Suppose {zl,al}L\\nl=1 are the vectors obtained during the feed-forward propagation\\n(a0 = x,aL = g(x |θ)). Then, we have for l = 1,..., L:\\n∂C\\n∂Wl\\n= δl a⊤\\nl−1 and ∂C\\n∂bl\\n= δl,\\nwhere δl := ∂C/∂zl is computed recursively for l = L,..., 2:\\nδl−1 = Dl−1W⊤\\nl δl with δL = ∂SL\\n∂zL\\n∂C\\n∂g . (9.5)\\nProof: The scalar value C is obtained from the transitions (9.2), followed by the mapping\\ng(x |θ) 7→Loss(y,g(x |θ)). Using the chain rule (see Appendix B.1.2), we have☞400\\nδL = ∂C\\n∂zL\\n= ∂g(x)\\n∂zL\\n∂C\\n∂g(x) = ∂SL\\n∂zL\\n∂C\\n∂g .\\nRecall that the vector/vector derivative of a linear mapping z 7→Wz is given by W⊤; see'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 349, 'page_label': '332'}, page_content='∂zL\\n∂C\\n∂g .\\nRecall that the vector/vector derivative of a linear mapping z 7→Wz is given by W⊤; see\\n(B.5). It follows that, since zl = Wl al−1 + bl and al = S(zl), the chain rule gives☞399\\n∂zl\\n∂zl−1\\n= ∂al−1\\n∂zl−1\\n∂zl\\n∂al−1\\n= Dl−1W⊤\\nl .\\nHence, the recursive formula (9.5):\\nδl−1 = ∂C\\n∂zl−1\\n= ∂zl\\n∂zl−1\\n∂C\\n∂zl\\n= Dl−1W⊤\\nl δl, l = L,..., 3,2.\\nUsing the {δl}, we can now compute the derivatives with respect to the weight matrices\\nand the biases. In particular, applying the “scalar /matrix” di fferentiation rule (B.10) to\\nzl = Wl al−1 + bl gives:\\n∂C\\n∂Wl\\n= ∂C\\n∂zl\\n∂zl\\n∂Wl\\n= δl a⊤\\nl−1, l = 1,..., L\\nand\\n∂C\\n∂bl\\n= ∂zl\\n∂bl\\n∂C\\n∂zl\\n= δl, l = 1,..., L.\\n□\\nFrom the theorem we can see that for each pair (x,y) in the training set, we can compute the\\ngradient ∂C/∂θin a sequential manner, by computing δL,..., δ1. This procedure is called\\nback-propagationback-\\npropagation\\n. Since back-propagation mostly involves simple matrix multiplication, it'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 350, 'page_label': '333'}, page_content='Deep Learning 333\\ncan be efficiently implemented using dedicated computing hardware such as graphical pro-\\ncessor units (GPUs) and other parallel computing architecture. Note also that many matrix\\ncomputations that run in quadratic time can be replaced with linear-time componentwise\\nmultiplication. Specifically, multiplication of a vector with a diagonal matrix is equivalent\\nto componentwise multiplication:\\nA|{z}\\ndiag(a)\\nb = a ⊙b.\\nConsequently, we can write δl−1 = Dl−1W⊤\\nl δl as: δl−1 = S′(zl−1) ⊙W⊤\\nl δl, l = L,..., 3,2.\\nWe now summarize the back-propagation algorithm for the computation of a typical\\n∂C/∂θ. In the following algorithm, Lines 1 to 5 are the feed-forward part of the algorithm,\\nand Lines 7 to 10 are the back-propagation part of the algorithm.\\nAlgorithm 9.3.1: Computing the Gradient of a Typical C(θ)\\ninput: Training example (x,y), weight matrices and bias vectors {Wl,bl}L\\nl=1 =: θ,\\nactivation functions {Sl}L\\nl=1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 350, 'page_label': '333'}, page_content='l=1 =: θ,\\nactivation functions {Sl}L\\nl=1.\\noutput: The derivatives with respect to all weight matrices and bias vectors.\\n1 a0 ←x\\n2 for l = 1,..., L do // feed-forward\\n3 zl ←Wl al−1 + bl\\n4 al ←Sl(zl)\\n5 δL ←∂SL\\n∂zL\\n∂C\\n∂g\\n6 z0 ←0 // arbitrary assignment needed to finish the loop\\n7 for l = L,..., 1 do // back-propagation\\n8 ∂C\\n∂bl\\n←δl\\n9 ∂C\\n∂Wl\\n←δl a⊤\\nl−1\\n10 δl−1 ←S′(zl−1) ⊙W⊤\\nl δl\\n11 return ∂C\\n∂Wl\\nand ∂C\\n∂bl\\nfor all l = 1,..., L and the value g(x) ←aL (if needed)\\nNote that for the gradient of C(θ) to exist at every point, we need the activation func-\\ntions to be differentiable everywhere. This is the case, for example, for the logistic activa-\\ntion function in Figure 9.2. It is not the case for the ReLU function, which is differentiable\\neverywhere, except at z = 0. However, in practice, the kink of the ReLU function at z = 0\\nis unlikely to trip the back-propagation algorithm, because rounding errors and the finite-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 350, 'page_label': '333'}, page_content='is unlikely to trip the back-propagation algorithm, because rounding errors and the finite-\\nprecision computer arithmetic make it extremely unlikely that we will need to evaluate the\\nReLU at precisely z = 0. This is the reason why in Theorem 9.2 we merely required that\\nC(θ) is almost-everywhere differentiable.\\nIn spite of its kink at the origin, the ReLU has an important advantage over the logistic\\nfunction. While the derivative of the logistic function decays exponentially fast to zero as\\nwe move away from the origin, a phenomenon referred to as saturation, the derivative of saturation\\nthe ReLU function is always unity for positivez. Thus, for large positivez, the derivative of\\nthe logistic function does not carry any useful information, but the derivative of the ReLU\\ncan help guide a gradient optimization algorithm. The situation for the Heaviside function\\nin Figure 9.2 is even worse, because its derivative is completely noninformative for any'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 350, 'page_label': '333'}, page_content='in Figure 9.2 is even worse, because its derivative is completely noninformative for any\\nz , 0. In this respect, the lack of saturation of the ReLU function for z > 0 makes it a\\ndesirable activation function for training a network via back-propagation.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 351, 'page_label': '334'}, page_content='334 Back-Propagation\\nFinally, note that to obtain the gradient ∂ℓτ/∂θof the training loss, we simply need to\\nloop Algorithm 9.3.1 over all the n training examples, as follows.\\nAlgorithm 9.3.2: Computing the Gradient of the Training Loss\\ninput: Training set τ= {(xi,yi)}n\\ni=1, weight matrices and bias vectors\\n{Wl,bl}L\\nl=1 =: θ, activation functions {Sl}L\\nl=1.\\noutput: The gradient of the training loss.\\n1 for i = 1,..., n do // loop over all training examples\\n2 Run Algorithm 9.3.1 with input (xi,yi) to compute\\nn ∂Ci\\n∂Wl\\n,∂Ci\\n∂bl\\noL\\nl=1\\n3 return ∂C\\n∂Wl\\n= 1\\nn\\nPn\\ni=1\\n∂Ci\\n∂Wl\\nand ∂C\\n∂bl\\n= 1\\nn\\nPn\\ni=1\\n∂Ci\\n∂bl\\nfor all l = 1,..., L\\nExample 9.4 (Squared-Error and Cross-Entropy Loss) The back-propagation Al-\\ngorithm 9.3.1 requires a formula for δL in line 5. In particular, to execute line 5 we need to\\nspecify both a loss function and an SL that defines the output layer: g(x |θ) = aL = SL(zL).\\nFor instance, in the multi-logit classification of inputs x into pL categories labeled☞266'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 351, 'page_label': '334'}, page_content='For instance, in the multi-logit classification of inputs x into pL categories labeled☞266\\n0,1,..., (pL −1), the output layer is defined via the softmax function:\\nSL : zL 7→ exp(zL)PpL\\nk=1 exp(zL,k).\\nIn other words, g(x |θ) is a probability vector such that its (y+1)-st component gy+1(x |θ) =\\ng(y |θ,x) is the estimate or prediction of the true conditional probability f (y |x). Combin-\\ning the softmax output with the cross-entropy loss, as was done in (7.17), yields:☞267\\nLoss( f (y |x),g(y |θ,x)) = −ln g(y |θ,x)\\n= −ln gy+1(x |θ)\\n= −zy+1 + ln PpL\\nk=1 exp(zk).\\nHence, we obtain the vector δL with components (k = 1,..., pL)\\nδL,k = ∂\\n∂zk\\n\\x10\\n−zy+1 + ln PpL\\nk=1 exp(zk)\\n\\x11\\n= gk(x |θ) −1{y = k −1}.\\nNote that we can remove a node from the final layer of the multi-logit network, be-\\ncause g1(x |θ) (which corresponds to the y = 0 class) can be eliminated, using the fact\\nthat g1(x |θ) = 1 −PpL\\nk=2 gk(x |θ). For a numerical comparison, see Exercise 13.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 351, 'page_label': '334'}, page_content='that g1(x |θ) = 1 −PpL\\nk=2 gk(x |θ). For a numerical comparison, see Exercise 13.\\nAs another example, in nonlinear multi-output regression (see Example 9.1), the out-\\nput function SL is typically of the form (9.1), so that ∂SL/∂z = diag(S ′\\nL(z1),..., S ′\\nL(zpL )).\\nCombining the output g(x |θ) = SL(zL) with the squared-error loss yields:\\nLoss(y,g(x |θ)) = ∥y −g(x |θ)∥2 =\\npLX\\nj=1\\n(yj −gj(x |θ))2.\\nHence, line 5 in Algorithm 9.3.1 simplifies to:\\nδL = ∂SL\\n∂z\\n∂C\\n∂g = S′\\nL(zL) ⊙2(g(x |θ) −y).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 352, 'page_label': '335'}, page_content='Deep Learning 335\\n9.4 Methods for Training\\nNeural networks have been studied for a long time, yet it is only recently that there have\\nbeen sufficient computational resources to train them e ffectively. The training of neural\\nnetworks requires minimization of a training loss, ℓτ(g(·|θ)) = 1\\nn\\nPn\\ni=1 Ci(θ), which is typ-\\nically a di fficult high-dimensional optimization problem with multiple local minima. We\\nnext consider a number of simple training methods.\\nIn this section, the vectors δt and 1t use the notation of Section B.3.2 and should not\\nbe confused with the derivative δand the prediction function g, respectively.\\n9.4.1 Steepest Descent\\nIf we can compute the gradient of ℓτ(g(·|θ)) via back-propagation, then we can apply the\\nsteepest descent algorithm, which reads as follows. Starting from a guessθ1, we iterate the ☞ 412\\nfollowing step until convergence:\\nθt+1 = θt −αt ut, t = 1,2,..., (9.6)\\nwhere ut := ∂ℓτ\\n∂θ (θt) and αt is the learning rate learning rate.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 352, 'page_label': '335'}, page_content='where ut := ∂ℓτ\\n∂θ (θt) and αt is the learning rate learning rate.\\nObserve that, rather than operating directly on the weights and biases, we operate in-\\nstead on θ := {Wl,bl}L\\nl=1 — a column vector of length PL\\nl=1(pl−1 pl + pl) that stores all the\\nweight and bias parameters. The advantage of organizing the computations in this way is\\nthat we can easily compute the learning rate αt; for example, via the Barzilai–Borwein\\nformula in (B.26). ☞ 413\\nAlgorithm 9.4.1: Training via Steepest Descent\\ninput: Training set τ= {(xi,yi)}n\\ni=1, initial weight matrices and bias vectors\\n{Wl,bl}L\\nl=1 =: θ1, activation functions {Sl}L\\nl=1.\\noutput: The parameters of the trained learner.\\n1 t ←1, δ←0.1 ×1, ut−1 ←0, α←0.1 // initialization\\n2 while stopping condition is not met do\\n3 compute the gradient ut = ∂ℓτ\\n∂θ (θt) using Algorithm 9.3.2\\n4 1←ut −ut−1\\n5 if δ⊤1>0 then // check if Hessian is positive-definite\\n6 α←δ⊤1\\x0e∥1∥2 // Barzilai-Borwein\\n7 else\\n8 α←2 ×α // failing positivity, do something heuristic'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 352, 'page_label': '335'}, page_content='6 α←δ⊤1\\x0e∥1∥2 // Barzilai-Borwein\\n7 else\\n8 α←2 ×α // failing positivity, do something heuristic\\n9 δ←−αut\\n10 θt+1 ←θt + δ\\n11 t ←t + 1\\n12 return θt as the minimizer of the training loss\\nTypically, we initialize the algorithm with small random values for θ1, while being\\ncareful to avoid saturating the activation function. For example, in the case of the ReLU'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 353, 'page_label': '336'}, page_content='336 Methods for Training\\nactivation function, we will use small positive values to ensure that its derivative is not\\nzero. A zero derivative of the activation function prevents the propagation of information\\nuseful for computing a good search direction.\\nRecall that computation of the gradient of the training loss via Algorithm 9.3.2 requires\\naveraging over all training examples. When the size n of the training set τn is too large,\\ncomputation of the gradient ∂ℓτn /∂θvia Algorithm 9.3.2 may be too costly. In such cases,\\nwe may employ the stochastic gradient descentstochastic\\ngradient\\ndescent\\nalgorithm. In this algorithm, we view the\\ntraining loss as an expectation that can be approximated via Monte Carlo sampling. In\\nparticular, if K is a random variable with distributionP[K = k] = 1/n for k = 1,..., n, then\\nwe can write\\nℓτ(g(·|θ)) = 1\\nn\\nnX\\nk=1\\nLoss(yk,g(xk |θ)) = ELoss(yK,g(xK |θ)).\\nWe can thus approximate ℓτ(g(·|θ)) via a Monte Carlo estimator using N iid copies of K:\\nbℓτ(g(·|θ)) := 1'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 353, 'page_label': '336'}, page_content='bℓτ(g(·|θ)) := 1\\nN\\nNX\\ni=1\\nLoss(yKi ,g(xKi |θ)).\\nThe iid Monte Carlo sample K1,..., KN is called a minibatchminibatch (see also Exercise 3). Typic-\\nally, n ≫N so that the probability of observing ties in a minibatch of size N is negligible.\\nFinally, note that if the learning rate of the stochastic gradient descent algorithm sat-\\nisfies the conditions in (3.30), then the stochastic gradient descent algorithm is simply a☞107\\nversion of the stochastic approximation Algorithm 3.4.5.\\n9.4.2 Levenberg–Marquardt Method\\nSince a neural network with squared-error loss is a special type of nonlinear regression\\nmodel, it is possible to train it using classical nonlinear least-squares minimization meth-\\nods, such as the Levenberg–Marquardtalgorithm.☞415\\nFor simplicity of notation, suppose that the output of the net for an input x is a scalar\\ng(x). For a given input parameter θ of dimension d = dim(θ), the Levenberg–Marquardt\\nAlgorithm B.3.3 requires computation of the following vector of outputs:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 353, 'page_label': '336'}, page_content='Algorithm B.3.3 requires computation of the following vector of outputs:\\ng(τ|θ) := [g(x1 |θ),..., g(xn |θ)]⊤,\\nas well as the n ×d matrix of Jacobi, G, of g at θ. To compute these quantities, we can\\nagain use the back-propagation Algorithm 9.3.1, as follows.\\nAlgorithm 9.4.2: Output for Training via Levenberg–Marquardt\\ninput: Training set τ= {(xi,yi)}n\\ni=1, parameter θ.\\noutput: Vector g(τ|θ) and matrix of Jacobi G for use in Algorithm B.3.3.\\n1 for i = 1,..., n do // loop over all training examples\\n2 Run Algorithm 9.3.1 with input (xi,yi) (using ∂C\\n∂g = 1 in line 5) to compute\\ng(xi |θ) and ∂g(xi |θ)\\n∂θ .\\n3 g(τ|θ) ←[g(x1 |θ),..., g(xn |θ)]⊤\\n4 G ←\\nh∂g(x1 |θ)\\n∂θ ,··· ,∂g(xn |θ)\\n∂θ\\ni⊤\\n5 return g(τ|θ) and G'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 354, 'page_label': '337'}, page_content='Deep Learning 337\\nThe Levenberg–Marquardt algorithm is not suitable for networks with a large number\\nof parameters, because the cost of the matrix computations becomes prohibitive. For in-\\nstance, obtaining the Levenberg–Marquardt search direction in (B.28) usually incurs an\\nO(d3) cost. In addition, the Levenberg–Marquardt algorithm is applicable only when we\\nwish to train the network using the squared-error loss. Both of these shortcomings are\\nmitigated to an extent with the quasi-Newton or adaptive gradient methods described next.\\n9.4.3 Limited-Memory BFGS Method\\nAll the methods discussed so far have beenfirst-order optimization methods, that is, meth-\\nods that only use the gradient vectorut := ∂ℓτ\\n∂θ (θt) at the current (and/or immediate past) can-\\ndidate solution θt. In trying to design a more e fficient second-order optimization method,\\nwe may be tempted to use Newton’s method with a search direction: ☞ 410\\n−H−1\\nt ut,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 354, 'page_label': '337'}, page_content='we may be tempted to use Newton’s method with a search direction: ☞ 410\\n−H−1\\nt ut,\\nwhere Ht is the d ×d matrix of second-order partial derivatives of ℓτ(g(·|θ)) at θt.\\nThere are two problems with this approach. First, while the computation of ut via Al-\\ngorithm 9.3.2 typically costs O(d), the computation of Ht costs O(d2). Second, even if we\\nhave somehow computedHt very fast, computing the search directionH−1\\nt ut still incurs an\\nO(d3) cost. Both of these considerations make Newton’s method impractical for large d.\\nInstead, a practical alternative is to use a quasi-Newton method, in which we directly quasi-newton\\nmethodaim to approximate H−1\\nt via a matrix Ct that satisfies the secant condition:\\n☞ 411Ct 1t = δt,\\nwhere δt := θt −θt−1 and 1t := ut −ut−1.\\nAn ingenious formula that generates a suitable sequence of approximating matrices\\n{Ct}(each satisfying the secant condition) is the BFGS updating formula (B.23), which\\ncan be written as the recursion (see Exercise 9):\\nCt =\\n\\x10\\nI −υt 1tδ⊤'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 354, 'page_label': '337'}, page_content='can be written as the recursion (see Exercise 9):\\nCt =\\n\\x10\\nI −υt 1tδ⊤\\nt\\n\\x11⊤\\nCt−1\\n\\x10\\nI −υt 1tδ⊤\\nt\\n\\x11\\n+ υt δtδ⊤\\nt , υ t := (1⊤\\nt δt)−1. (9.7)\\nThis formula allows us to update Ct−1 to Ct and then compute Ct ut in O(d2) time. While\\nthis quasi-Newton approach is better than the O(d3) cost of Newton’s method, it may be\\nstill too costly in large-scale applications.\\nInstead, an approximate or limited memory BFGS limited memory\\nbfgs\\nupdating can be achieved in O(d)\\ntime. The idea is to store a few of the most recent pairs{δt,1t}in order to evaluate its action\\non a vector ut without explicitly constructing and storing Ct in computer memory. This is\\npossible, because updating C0 to C1 in (9.7) requires only the pair δ1,11, and similarly\\ncomputing Ct from C0 only requires the history of the updates δ1,11 ..., δt,1t, which can\\nbe shown as follows.\\nDefine the matrices At,..., A0 via the backward recursion ( j = 1,..., t):\\nAt := I, Aj−1 :=\\n\\x10\\nI −υj 1jδ⊤\\nj\\n\\x11\\nAj,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 354, 'page_label': '337'}, page_content='At := I, Aj−1 :=\\n\\x10\\nI −υj 1jδ⊤\\nj\\n\\x11\\nAj,\\nand observe that all matrix vector products: Aj u =: qj,for j = 0,..., t can be computed\\nefficiently via the backward recursion starting with qt = u:\\nτj := δ⊤\\nj qj, qj−1 = qj −υjτj 1j, j = t,t −1,..., 1. (9.8)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 355, 'page_label': '338'}, page_content='338 Methods for Training\\nIn addition to {qj}, we will make use of the vectors {rj}defined via the recursion:\\nr0 := C0 q0, rj = rj−1 + υj\\n\\x10\\nτj −1⊤\\nj rj−1\\n\\x11\\nδj, j = 1,..., t. (9.9)\\nAt the final iteration t, the BFGS updating formula (9.7) can be rewritten in the form:\\nCt = A⊤\\nt−1Ct−1At−1 + υt δtδ⊤\\nt .\\nBy iterating the recursion (9.7) backwards to C0, we can write:\\nCt = A⊤\\n0 C0A0 +\\ntX\\nj=1\\nυj A⊤\\nj δjδ⊤\\nj Aj,\\nthat is, we can expressCt in terms of the initialC0 and the entire history of all BFGS values\\n{δj,1j}, as claimed. Further, with the {qj,rj}computed via (9.8) and (9.9), we can write:\\nCt u = A⊤\\n0 C0 q0 +\\ntX\\nj=1\\nυj\\n\\x10\\nδ⊤\\nj qj\\n\\x11\\nA⊤\\nj δj\\n= A⊤\\n0 r0 + υ1τ1A⊤\\n1 δ1 +\\ntX\\nj=2\\nυjτjA⊤\\nj δj\\n= A⊤\\n1\\n\\x02\\x00I −υ1δ11⊤\\n1\\n\\x01 r0 + υ1τ1δ1\\n\\x03 +\\ntX\\nj=2\\nυjτjA⊤\\nj δj.\\nHence, from the definition of the {rj}in (9.9), we obtain\\nCtu = A⊤\\n1 r1 +\\ntX\\nj=2\\nυjτjA⊤\\nj δj\\n= A⊤\\n2 r2 +\\ntX\\nj=3\\nυjτjA⊤\\nj δj\\n= ··· = A⊤\\nt rt + 0 = rt.\\nGiven C0 and the history of all recent BFGS values{δj,1j}h\\nj=1, the computation of the quasi-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 355, 'page_label': '338'}, page_content='Given C0 and the history of all recent BFGS values{δj,1j}h\\nj=1, the computation of the quasi-\\nNewton search direction d = −Ch u can be accomplished via the recursions (9.8) and (9.9)\\nas summarized in Algorithm 9.4.3.\\nNote that if C0 is a diagonal matrix, say the identity matrix, then C0 q is cheap to\\ncompute and the cost of running Algorithm 9.4.3 is O(h d). Thus, for a fixed length of the\\nBFGS history, the cost of the limited-memory BFGS updating grows linearly ind, making\\nit a viable optimization algorithm in large-scale applications.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 356, 'page_label': '339'}, page_content='Deep Learning 339\\nAlgorithm 9.4.3: Limited-Memory BFGS Update\\ninput: BFGS history list {δj,1j}h\\nj=1, initial C0, and input u.\\noutput: d = −Ch u, where Ct = \\x00I −υt δt1⊤\\nt\\n\\x01Ct−1\\n\\x10\\nI −υt 1tδ⊤\\nt\\n\\x11\\n+ υt δtδ⊤\\nt .\\n1 q ←u\\n2 for i = h,h −1,..., 1 do // backward recursion to compute A0 u\\n3 υi ←\\n\\x10\\nδ⊤\\ni 1i\\n\\x11−1\\n4 τi ←δ⊤\\ni q\\n5 q ←q −υiτi 1i\\n6 q ←C0 q // compute C0(A0 u)\\n7 for i = 1,..., h do // compute recursion (9.9)\\n8 q ←q + υi(τi −1⊤\\ni q) δi\\n9 return d ←−q, the value of −Ch u\\nIn summary, a quasi-Newton algorithm with limited-memory BFGS updating reads as\\nfollows.\\nAlgorithm 9.4.4: Quasi-Newton Minimization with Limited-Memory BFGS\\ninput: Training set τ= {(xi,yi)}n\\ni=1, initial weight matrices and bias vectors\\n{Wl,bl}L\\nl=1 =: θ1, activation functions {Sl}L\\nl=1, and history parameter h.\\noutput: The parameters of the trained learner.\\n1 t ←1, δ←0.1 ×1, ut−1 ←0 // initialization\\n2 while stopping condition is not met do\\n3 Compute ℓvalue = ℓτ(g(·|θt)) and ut = ∂ℓτ\\n∂θ (θt) via Algorithm 9.3.2.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 356, 'page_label': '339'}, page_content='3 Compute ℓvalue = ℓτ(g(·|θt)) and ut = ∂ℓτ\\n∂θ (θt) via Algorithm 9.3.2.\\n4 1←ut −ut−1\\n5 Add (δ,1) to the BFGS history as the newest BFGS pair.\\n6 if the number of pairs in the BFGS history is greater than h then\\n7 remove the oldest pair from the BFGS history\\n8 Compute d via Algorithm 9.4.3 using the BFGS history, C0 = I, and ut.\\n9 α←1\\n10 while ℓτ(g(·|θt + αd)) ⩾ℓvalue + 10−4αd⊤ut do\\n11 α←α/1.5 // line-search along quasi-Newton direction\\n12 δ←αd\\n13 θt+1 ←θt + δ\\n14 t ←t + 1\\n15 return θt as the minimizer of the training loss\\n9.4.4 Adaptive Gradient Methods\\nRecall that the limited-memory BFGS method in the previous section determines a search\\ndirection using the recent history of previously computed gradients{ut}and input paramet-\\ners {θt}. This is because the BFGS pairs{δt,1t}can be easily constructed from the identities:\\nδt = θt −θt−1 and 1t = ut −ut−1. In other words, using only past gradient computations and'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 356, 'page_label': '339'}, page_content='δt = θt −θt−1 and 1t = ut −ut−1. In other words, using only past gradient computations and\\nwith little extra computation, it is possible to infer some of the second-order information'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 357, 'page_label': '340'}, page_content='340 Methods for Training\\ncontained in the Hessian matrix of ℓτ(θ). In addition to the BFGS method, there are other\\nways in which we can exploit the history of past gradient computations.\\nOne approach is to use the normal approximation method, in which the Hessian of ℓτ☞414\\nat θt is approximated via\\nbHt = γI + 1\\nh\\ntX\\ni=t−h+1\\nuiu⊤\\ni , (9.10)\\nwhere ut−h+1,..., ut are the h most recently computed gradients andγis a tuning parameter\\n(for example, γ= 1/h). The search direction is then given by\\n−bH−1\\nt ut,\\nwhich can be computed quickly in O(h2 d) time either using the QR decomposition (Exer-\\ncises 5 and 6), or the Sherman–Morrison Algorithm A.6.1. This approach requires that we☞373\\nstore the last h gradient vectors in memory.\\nAnother approach that completely bypasses the need to invert a Hessian approximation\\nis the Adaptive Gradient or AdaGradAdaGrad method, in which we only store the diagonal of bHt\\nand use the search direction:\\n−diag(bHt)−1/2ut.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 357, 'page_label': '340'}, page_content='and use the search direction:\\n−diag(bHt)−1/2ut.\\nWe can avoid storing any of the gradient history by instead using the slightly di fferent\\nsearch direction2\\n−ut\\n.p\\nvt + γ×1,\\nwhere the vector vt is updated recursively via\\nvt =\\n \\n1 −1\\nh\\n!\\nvt−1 + 1\\nh ut ⊙ut.\\nWith this updating of vt, the difference between the vector vt + γ×1 and the diagonal of\\nthe Hessian bHt will be negligible.\\nA more sophisticated version of AdaGrad is the adaptive moment estimation or AdamAdam\\nmethod, in which we not only average the vectors{vt}, but also average the gradient vectors\\n{ut}, as follows.\\nAlgorithm 9.4.5: Updating of Search Direction at Iteration t via Adam\\ninput: ut, but−1, vt−1, θt, and parameters (α,hv,hu), equal to, e.g., (10−3,103,10).\\noutput: but, vt, θt+1.\\n1 but ←\\n\\x10\\n1 −1\\nhu\\n\\x11\\nbut−1 + 1\\nhu\\nut\\n2 vt ←\\n\\x10\\n1 −1\\nhv\\n\\x11\\nvt−1 + 1\\nhv\\nut ⊙ut\\n3 u∗\\nt ←but\\n.\\x10\\n1 −(1 −h−1\\nu )t\\n\\x11\\n4 v∗\\nt ←vt\\n.\\x10\\n1 −(1 −h−1\\nv )t\\n\\x11\\n5 θt+1 ←θt −αu∗\\nt\\n.\\x10p\\nv∗\\nt + 10−8 ×1\\n\\x11\\n6 return but, vt, θt+1\\n2Here we divide two vectors componentwise.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 358, 'page_label': '341'}, page_content='Deep Learning 341\\nYet another computationally cheap approach is the momentum method, in which the momentum\\nmethodsteepest descent iteration (9.6) is modified to\\nθt+1 = θt −αt ut + γδt,\\nwhere δt = θt −θt−1 and γis a tuning parameter. This strategy frequently performs better\\nthan the “vanilla” steepest descent method, because the search direction is less likely to\\nchange abruptly.\\nNumerical experience suggests that the vanilla steepest-descent Algorithm 9.4.1 and\\nthe Levenberg–Marquardt Algorithm B.3.3 are e ffective for networks with shallow archi-\\ntectures, but not for networks with deep architectures. In comparison, the stochastic gradi-\\nent descent method, the limited-memory BFGS Algorithm 9.4.4, or any of the adaptive\\ngradient methods in this section, can frequently handle networks with many hidden lay-\\ners (provided that any tuning parameters and initialization values are carefully chosen via\\nexperimentation).\\n9.5 Examples in Python'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 358, 'page_label': '341'}, page_content='experimentation).\\n9.5 Examples in Python\\nIn this section we provide two numerical examples in Python. In the first example, we\\ntrain a neural network with the stochastic gradient descent method using the polynomial\\nregression data from Example 2.1, and without using any specialized Python packages. ☞ 26\\nIn the second example, we consider a realistic application of a neural network to image\\nrecognition and classification. Here we use the specialized open-source Python package\\nPytorch.\\n9.5.1 Simple Polynomial Regression\\nConsider again the polynomial regression data set depicted in Figure 2.4. We use a network\\nwith architecture\\n[p0,p1,p2,p3] = [1,20,20,1].\\nIn other words, we have two hidden layers with 20 neurons, resulting in a learner with a\\ntotal of dim(θ) = 481 parameters. To implement such a neural network, we first import the\\nnumpy and the matplotlib packages, then read the regression problem data and define\\nthe feed-forward neural network layers.\\nNeuralNetPurePython.py'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 358, 'page_label': '341'}, page_content=\"the feed-forward neural network layers.\\nNeuralNetPurePython.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n#%%\\n# import data\\ndata = np.genfromtxt( 'polyreg.csv ',delimiter= ',')\\nX = data[:,0].reshape(-1,1)\\ny = data[:,1].reshape(-1,1)\\n# Network setup\\np = [X.shape[1],20,20,1] # size of layers\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 359, 'page_label': '342'}, page_content='342 Examples in Python\\nL = len (p)-1 # number of layers\\nNext, the initialize method generates random initial weight matrices and bias vec-\\ntors {Wl,bl}L\\nl=1. Specifically, all parameters are initialized with values distributed according\\nto the standard normal distribution.\\ndef initialize(p, w_sig = 1):\\nW, b = [[]]* len (p), [[]]* len (p)\\nfor l in range (1,len (p)):\\nW[l]= w_sig * np.random.randn(p[l], p[l-1])\\nb[l]= w_sig * np.random.randn(p[l], 1)\\nreturn W,b\\nW,b = initialize(p) # initialize weight matrices and bias vectors\\nThe following code implements the ReLU activation function from Figure 9.2 and the\\nsquared error loss. Note that these functions return both the function values and the corres-\\nponding gradients.\\ndef RELU(z,l): # RELU activation function: value and derivative\\nif l == L: return z, np.ones_like(z)\\nelse :\\nval = np.maximum(0,z) # RELU function element -wise\\nJ = np.array(z>0, dtype = float ) # derivative of RELU\\nelement -wise\\nreturn val, J\\ndef loss_fn(y,g):'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 359, 'page_label': '342'}, page_content='element -wise\\nreturn val, J\\ndef loss_fn(y,g):\\nreturn (g - y)**2, 2 * (g - y)\\nS = RELU\\nNext, we implement the feed-forward and backward-propagation Algorithm 9.3.1.\\nHere, we have implemented Algorithm 9.3.2 inside the backward-propagation loop.\\ndef feedforward(x,W,b):\\na, z, gr_S = [0]*(L+1), [0]*(L+1), [0]*(L+1)\\na[0] = x.reshape(-1,1)\\nfor l in range (1,L+1):\\nz[l] = W[l] @ a[l-1] + b[l] # affine transformation\\na[l], gr_S[l] = S(z[l],l) # activation function\\nreturn a, z, gr_S\\ndef backward(W,b,X,y):\\nn =len (y)\\ndelta = [0]*(L+1)\\ndC_db, dC_dW = [0]*(L+1), [0]*(L+1)\\nloss=0'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 360, 'page_label': '343'}, page_content='Deep Learning 343\\nfor i in range (n): # loop over training examples\\na, z, gr_S = feedforward(X[i,:].T, W, b)\\ncost, gr_C = loss_fn(y[i], a[L]) # cost i and gradient wrt g\\nloss += cost/n\\ndelta[L] = gr_S[L] @ gr_C\\nfor l in range (L,0,-1): # l = L,...,1\\ndCi_dbl = delta[l]\\ndCi_dWl = delta[l] @ a[l-1].T\\n# ---- sum up over samples ----\\ndC_db[l] = dC_db[l] + dCi_dbl/n\\ndC_dW[l] = dC_dW[l] + dCi_dWl/n\\n# -----------------------------\\ndelta[l-1] = gr_S[l-1] * W[l].T @ delta[l]\\nreturn dC_dW, dC_db, loss\\nAs explained in Section 9.4, it is sometimes more convenient to collect all the weight\\nmatrices and bias vectors {Wl,bl}L\\nl=1 into a single vector θ. Consequently, we code two\\nfunctions that map the weight matrices and the bias vectors into a single parameter vector,\\nand vice versa.\\ndef list2vec(W,b):\\n# converts list of weight matrices and bias vectors into\\n# one column vector\\nb_stack = np.vstack([b[i] for i in range (1,len (b))] )\\nW_stack = np.vstack(W[i].flatten().reshape(-1,1) for i in range'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 360, 'page_label': '343'}, page_content='W_stack = np.vstack(W[i].flatten().reshape(-1,1) for i in range\\n(1,len (W)))\\nvec = np.vstack([b_stack, W_stack])\\nreturn vec\\n#%%\\ndef vec2list(vec, p):\\n# converts vector to weight matrices and bias vectors\\nW, b = [[]]* len (p),[[]]* len (p)\\np_count = 0\\nfor l in range (1,len (p)): # construct bias vectors\\nb[l] = vec[p_count:(p_count+p[l])].reshape(-1,1)\\np_count = p_count + p[l]\\nfor l in range (1,len (p)): # construct weight matrices\\nW[l] = vec[p_count:(p_count + p[l]*p[l-1])].reshape(p[l], p[\\nl-1])\\np_count = p_count + (p[l]*p[l-1])\\nreturn W, b\\nFinally, we run the stochastic gradient descent for 104 iterations using a minibatch of\\nsize 20 and a constant learning rate of αt = 0.005.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 361, 'page_label': '344'}, page_content='344 Examples in Python\\nbatch_size = 20\\nlr = 0.005\\nbeta = list2vec(W,b)\\nloss_arr = []\\nn = len (X)\\nnum_epochs = 10000\\nprint (\"epoch | batch loss\")\\nprint (\"----------------------------\")\\nfor epoch in range (1,num_epochs+1):\\nbatch_idx = np.random.choice(n,batch_size)\\nbatch_X = X[batch_idx].reshape(-1,1)\\nbatch_y=y[batch_idx].reshape(-1,1)\\ndC_dW, dC_db, loss = backward(W,b,batch_X,batch_y)\\nd_beta = list2vec(dC_dW,dC_db)\\nloss_arr.append(loss.flatten()[0])\\nif (epoch==1 or np.mod(epoch,1000)==0):\\nprint (epoch,\": \",loss.flatten()[0])\\nbeta = beta - lr*d_beta\\nW,b = vec2list(beta,p)\\n# calculate the loss of the entire training set\\ndC_dW, dC_db, loss = backward(W,b,X,y)\\nprint (\"entire training set loss = \",loss.flatten()[0])\\nxx = np.arange(0,1,0.01)\\ny_preds = np.zeros_like(xx)\\nfor i in range (len (xx)):\\na, _, _ = feedforward(xx[i],W,b)\\ny_preds[i], = a[L]\\nplt.plot(X,y, \\'r.\\', markersize = 4,label = \\'y\\')\\nplt.plot(np.array(xx), y_preds, \\'b\\',label = \\'fit\\')\\nplt.legend()\\nplt.xlabel( \\'x\\')\\nplt.ylabel( \\'y\\')'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 361, 'page_label': '344'}, page_content=\"plt.plot(np.array(xx), y_preds, 'b',label = 'fit')\\nplt.legend()\\nplt.xlabel( 'x')\\nplt.ylabel( 'y')\\nplt.show()\\nplt.plot(np.array(loss_arr), 'b')\\nplt.xlabel( 'iteration ')\\nplt.ylabel( 'Training Loss ')\\nplt.show()\\nepoch | batch loss\\n----------------------------\\n1 : 158.6779278688539\\n1000 : 54.52430507401445\\n2000 : 38.346572088604965\\n3000 : 31.02036319180713\\n4000 : 22.91114276931535\\n5000 : 27.75810262906341\\n6000 : 22.296907007032928\\n7000 : 17.337367420038046\\n8000 : 19.233689945334195\\n9000 : 39.54261478969857\\n10000 : 14.754724387604416\\nentire training set loss = 28.904957963612727\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 362, 'page_label': '345'}, page_content='Deep Learning 345\\nThe left panel of Figure 9.5 shows a trained neural network with a training loss of\\napproximately 28 .9. As seen from the right panel of Figure 9.5, the algorithm initially\\nmakes rapid progress until it settles down into a stationary regime after 400 iterations.\\n0.00\\n 0.25\\n 0.50\\n 0.75\\n 1.00\\ninput u\\n0\\n20\\n40output y\\nﬁt\\ny\\n0\\n 500\\n 1000\\n 1500\\n 2000\\niteration\\n0\\n100\\n200\\n300Batch Loss\\nFigure 9.5: Left panel: The fitted neural network with training loss of ℓτ(gτ) ≈28.9. Right\\npanel: The evolution of the estimated loss,bℓτ(gτ(·|θ)), over the steepest-descent iterations.\\n9.5.2 Image Classification\\nIn this section, we will use the package Pytorch, which is an open-source machine learn-\\ning library for Python. Pytorch can easily exploit any graphics processing unit (GPU)\\nfor accelerated computation. As an example, we consider the Fashion-MNIST data set\\nfrom https://www.kaggle.com/zalando-research/fashionmnist. The Fashion-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 362, 'page_label': '345'}, page_content='from https://www.kaggle.com/zalando-research/fashionmnist. The Fashion-\\nMNIST data set contains 28 ×28 gray-scale images of clothing. Our task is to classify\\neach image according to its label. Specifically, the labels are: T-Shirt, Trouser, Pullover,\\nDress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle Boot. Figure 9.6 depicts a typical\\nankle boot in the left panel and a typical dress in the right panel. To start with, we import\\nthe required libraries and load the Fashion-MNIST data set.\\nFigure 9.6: Left: an ankle boot. Right: a dress.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 363, 'page_label': '346'}, page_content=\"346 Examples in Python\\nImageClassificationPytorch.py\\nimport torch\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom PIL import Image\\nimport torch.nn.functional as F\\n#################################################################\\n# data loader class\\n#################################################################\\nclass LoadData(Dataset):\\ndef __init__(self, fName, transform=None):\\ndata = pd.read_csv(fName)\\nself.X = np.array(data.iloc[:, 1:], dtype=np.uint8).reshape\\n(-1, 1, 28, 28)\\nself.y = np.array(data.iloc[:, 0])\\ndef __len__(self):\\nreturn len (self.X)\\ndef __getitem__(self, idx):\\nimg = self.X[idx]\\nlbl = self.y[idx]\\nreturn (img, lbl)\\n# load the image data\\ntrain_ds = LoadData( 'fashionmnist/fashion-mnist_train.csv ')\\ntest_ds = LoadData( 'fashionmnist/fashion-mnist_test.csv ')\\n# set labels dictionary\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 363, 'page_label': '346'}, page_content=\"test_ds = LoadData( 'fashionmnist/fashion-mnist_test.csv ')\\n# set labels dictionary\\nlabels = {0 : 'T-Shirt ', 1 : 'Trouser ', 2 : 'Pullover ',\\n3 : 'Dress ', 4 : 'Coat ', 5 : 'Sandal ', 6 : 'Shirt ',\\n7 : 'Sneaker ', 8 : 'Bag', 9 : 'Ankle Boot '}\\nSince an image input data is generally memory intensive, it is important to partition\\nthe data set into (mini-)batches. The code below defines a batch size of 100 images and\\ninitializes the Pytorch data loader objects. These objects will be used for efficient iteration\\nover the data set.\\n# load the data in batches\\nbatch_size = 100\\ntrain_loader = torch.utils.data.DataLoader(dataset=train_ds,\\nbatch_size=batch_size,\\nshuffle=True)\\ntest_loader = torch.utils.data.DataLoader(dataset=test_ds,\\nbatch_size=batch_size,\\nshuffle=True)\\nNext, to define the network architecture in Pytorch all we need to do is define an\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 364, 'page_label': '347'}, page_content='Deep Learning 347\\ninstance of the torch.nn.Moduleclass. Choosing a network architecture with good gen-\\neralization properties can be a difficult task. Here, we use a network with two convolution\\nlayers (defined in the cnn_layerblock), a 3×3 kernel, and three hidden layers (defined in\\nthe flat_layerblock). Since there are ten possible output labels, the output layer has ten\\nnodes. More specifically, the first and the second convolution layers have 16 and 32 output\\nchannels. Combining this with the definition of the 3 ×3 kernel, we conclude that the size\\nof the first flat hidden layer should be:\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nsecond convolution layer\\nz                      }|                      {\\n(28 −3 + 1)|        {z        }\\nfirst convolution layer\\n−3 + 1\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n2\\n×32 = 18432,\\nwhere the multiplication by 32 follows from the fact that the second convolution layer has\\n32 output channels. Having said that, the flat_fts variable determines the number of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 364, 'page_label': '347'}, page_content='32 output channels. Having said that, the flat_fts variable determines the number of\\noutput layers of the convolution block. This number is used to define the size of the first\\nhidden layer of the flat_layerblock. The rest of the hidden layers have 100 neurons and\\nwe use the ReLU activation function for all layers. Finally, note that the forwardmethod\\nin the CNNclass implements the forward pass.\\n# define the network\\nclass CNN(nn.Module):\\ndef __init__(self):\\nsuper (CNN, self).__init__()\\nself.cnn_layer = nn.Sequential(\\nnn.Conv2d(1, 16, kernel_size=3, stride=(1,1)),\\nnn.ReLU(),\\nnn.Conv2d(16, 32, kernel_size=3, stride=(1,1)),\\nnn.ReLU(),\\n)\\nself.flat_fts = (((28-3+1)-3+1)**2)*32\\nself.flat_layer = nn.Sequential(\\nnn.Linear(self.flat_fts, 100),\\nnn.ReLU(),\\nnn.Linear(100, 100),\\nnn.ReLU(),\\nnn.Linear(100, 100),\\nnn.ReLU(),\\nnn.Linear(100, 10))\\ndef forward(self, x):\\nout = self.cnn_layer(x)\\nout = out.view(-1, self.flat_fts)\\nout = self.flat_layer(out)\\nreturn out'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 364, 'page_label': '347'}, page_content='out = self.cnn_layer(x)\\nout = out.view(-1, self.flat_fts)\\nout = self.flat_layer(out)\\nreturn out\\nNext, we specify how the network will be trained. We choose the device type, namely,\\nthe central processing unit (CPU) or the GPU (if available), the number of training itera-\\ntions (epochs), and the learning rate. Then, we create an instance of the proposed convolu-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 365, 'page_label': '348'}, page_content=\"348 Examples in Python\\ntion network and send it to the predefined device (CPU or GPU). Note how easily one can\\nswitch between the CPU or the GPU without major changes to the code.\\nIn addition to the specifications above, we need to choose an appropriate loss function\\nand training algorithm. Here, we use the cross-entropy loss and the Adam adaptive gradi-☞267\\nent Algorithm 9.4.5. Once these parameters are set, the learning proceeds to evaluate the\\ngradient of the loss function via the back-propagation algorithm.\\n# learning parameters\\nnum_epochs = 50\\nlearning_rate = 0.001\\n#device = torch.device ( 'cpu ') # use this to run on CPU\\ndevice = torch.device ( 'cuda ') # use this to run on GPU\\n#instance of the Conv Net\\ncnn = CNN()\\ncnn.to(device=device)\\n#loss function and optimizer\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\\n# the learning loop\\nlosses = []\\nfor epoch in range (1,num_epochs+1):\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 365, 'page_label': '348'}, page_content='# the learning loop\\nlosses = []\\nfor epoch in range (1,num_epochs+1):\\nfor i, (images, labels) in enumerate (train_loader):\\nimages = Variable(images. float ()).to(device=device)\\nlabels = Variable(labels).to(device=device)\\noptimizer.zero_grad()\\noutputs = cnn(images)\\nloss = criterion(outputs, labels)\\nloss.backward()\\noptimizer.step()\\nlosses.append(loss.item())\\nif (epoch==1 or epoch % 10 == 0):\\nprint (\"Epoch : \", epoch, \", Training Loss: \", loss.item())\\n# evaluate on the test set\\ncnn. eval ()\\ncorrect = 0\\ntotal = 0\\nfor images, labels in test_loader:\\nimages = Variable(images. float ()).to(device=device)\\noutputs = cnn(images)\\n_, predicted = torch. max (outputs.data, 1)\\ntotal += labels.size(0)\\ncorrect += (predicted.cpu() == labels). sum ()\\nprint (\"Test Accuracy of the model on the 10,000 training test images\\n: \", (100 * correct.item() / total),\"%\")\\n# plot'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 366, 'page_label': '349'}, page_content=\"Deep Learning 349\\nplt.rc( 'text ', usetex=True)\\nplt.rc( 'font ', family= 'serif ',size=20)\\nplt.tight_layout()\\nplt.plot(np.array(losses)[10: len (losses)])\\nplt.xlabel(r '{iteration} ',fontsize=20)\\nplt.ylabel(r '{Batch Loss} ',fontsize=20)\\nplt.subplots_adjust(top=0.8)\\nplt.show()\\nEpoch : 1 , Training Loss: 0.412550151348114\\nEpoch : 10 , Training Loss: 0.05452106520533562\\nEpoch : 20 , Training Loss: 0.07233225554227829\\nEpoch : 30 , Training Loss: 0.01696968264877796\\nEpoch : 40 , Training Loss: 0.0008199119474738836\\nEpoch : 50 , Training Loss: 0.006860652007162571\\nTest Accuracy of the model on the 10,000 training test images: 91.02 %\\nFinally, we evaluate the network performance using the test data set. A typical mini-\\nbatch loss as a function of iteration is shown in Figure 9.7 and the proposed neural network\\nachieves about 91% accuracy on the test set.\\n0\\n 200\\n 400\\n 600\\n 800\\n 1000\\niteration\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nBatch Loss\\nFigure 9.7: The batch loss history.\\nFurther Reading\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 366, 'page_label': '349'}, page_content='1000\\niteration\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nBatch Loss\\nFigure 9.7: The batch loss history.\\nFurther Reading\\nA popular book written by some of the pioneers of deep learning is [53]. For an excellent\\nand gentle introduction to the intuition behind neural networks, we recommend [94]. A\\nsummary of many effective gradient descent methods for training of deep networks is given\\nin [105]. An early resource on the limited-memory BFGS method is [81], and a more recent\\nresource includes [13], which makes recommendations on the best choice for the length of\\nthe BFGS history (that is, the value of the parameter h).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 367, 'page_label': '350'}, page_content='350 Exercises\\nExercises\\n1. Show that the softmax function\\nsoftmax : z 7→ exp(z)P\\nk exp(zk)\\nsatisfies the invariance property:\\nsoftmax(z) = softmax(z + c ×1), for any constant c.\\n2. Projection pursuitprojection\\npursuit\\nis a network with one hidden layer that can be written as:\\ng(x) = S (ω⊤x),\\nwhere S is a univariate smoothing cubic spline . If we use squared-error loss with τn =☞235\\n{yi,xi}n\\ni=1, we need to minimize the training loss:\\n1\\nn\\nnX\\ni=1\\n\\x00yi −S (ω⊤xi)\\x012\\nwith respect to ωand all cubic smoothing splines. This training of the network is typically\\ntackled iteratively in a manner similar to the EM algorithm. In particular, we iterate ( t =☞139\\n1,2,... ) the following steps until convergence.\\n(a) Given the missing data ωt, compute the splineS t by training a cubic smoothing spline\\non {yi,ω⊤\\nt xi}. The smoothing coe fficient of the spline may be determined as part of\\nthis step.\\n(b) Given the spline function S t, compute the next projection vector ωt+1 via iterative'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 367, 'page_label': '350'}, page_content='this step.\\n(b) Given the spline function S t, compute the next projection vector ωt+1 via iterative\\nreweighted least squares:iterative\\nreweighted\\nleast squares ωt+1 = argmin\\nβ\\n(et −Xβ)⊤Σt (et −Xβ), (9.11)\\nwhere\\net,i := ω⊤\\nt xi + yi −S t(ω⊤\\nt xi)\\nS ′\\nt (ω⊤\\nt xi) , i = 1,..., n\\nis the adjusted response, and Σ1/2\\nt = diag(S ′\\nt (ω⊤\\nt x1),..., S ′\\nt (ω⊤\\nt xn)) is a diagonal mat-\\nrix.\\nApply Taylor’s Theorem B.1 to the function S t and derive the iterative reweighted☞400\\nleast-squares optimization program (9.11).\\n3. Suppose that in the stochastic gradient descent method we wish to repeatedly draw☞336\\nminibatches of size N from τn, where we assume that N ×m = n for some large integer m.\\nInstead of repeatedly resampling from τn, an alternative is to reshuffle τn via a random per-\\nmutation Π and then advance sequentially through the reshu ffled training set to construct☞115\\nm non-overlapping minibatches. A single traversal of such a reshuffled training set is called'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 367, 'page_label': '350'}, page_content='m non-overlapping minibatches. A single traversal of such a reshuffled training set is called\\nan epochepoch . The following pseudo-code describes the procedure.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 368, 'page_label': '351'}, page_content='Deep Learning 351\\nAlgorithm 9.5.1: Stochastic Gradient Descent with Reshuffling\\ninput: Training set τn = {(xi,yi)}n\\ni=1, initial weight matrices and bias vectors\\n{Wl,bl}L\\nl=1 →θ1, activation functions {Sl}L\\nl=1, learning rates {α1,α2,... }.\\noutput: The parameters of the trained learner.\\n1 t ←1 and epoch ←0\\n2 while stopping condition is not met do\\n3 Draw U1,..., Un\\niid\\n∼U(0,1).\\n4 Let Π be the permutation of {1,..., n}that satisfies UΠ1 <··· <UΠn .\\n5 (xi,yi) ←(xΠi ,yΠi ) for i = 1,..., n // reshuffle τn\\n6 for j = 1,..., m do\\n7 bℓτ ←1\\nN\\nPjN\\ni=( j−1)N+1 Loss(yi,g(xi |θ))\\n8 θt+1 ←θt −αt\\n∂bℓτ\\n∂θ (θt)\\n9 t ←t + 1\\n10 epoch ←epoch + 1 // number of reshuffles or epochs\\n11 return θt as the minimizer of the training loss\\nWrite Python code that implements the stochastic gradient descent with data reshu ffling,\\nand use it to train the neural net in Section 9.5.1. ☞ 341\\n4. Denote the pdf of the N(0,Σ) distribution by φΣ(·), and let\\nD(µ0,Σ0 |µ1,Σ1) =\\nZ\\nRd\\nφΣ0 (x −µ0) ln φΣ0 (x −µ0)\\nφΣ1 (x −µ1) dx'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 368, 'page_label': '351'}, page_content='D(µ0,Σ0 |µ1,Σ1) =\\nZ\\nRd\\nφΣ0 (x −µ0) ln φΣ0 (x −µ0)\\nφΣ1 (x −µ1) dx\\nbe the Kullback–Leibler divergence between the densities of the N(µ0,Σ0) and N(µ1,Σ1) ☞ 42\\ndistributions on Rd. Show that\\n2D(µ0,Σ0 |µ1,Σ1) = tr(Σ−1\\n1 Σ0) −ln |Σ−1\\n1 Σ0|+ (µ1 −µ0)⊤Σ−1\\n1 (µ1 −µ0) −d.\\nHence, deduce the formula in (B.22).\\n5. Suppose that we wish to compute the inverse and log-determinant of the matrix\\nIn + UU⊤,\\nwhere U is an n ×h matrix with h ≪n. Show that\\n(In + UU⊤)−1 = In −QnQ⊤\\nn ,\\nwhere Qn contains the first n rows of the (n + h) ×h matrix Q in the QR factorization of ☞ 375\\nthe (n + h) ×h matrix: \"U\\nIh\\n#\\n= QR.\\nIn addition, show that ln |In + UU⊤|= Ph\\ni=1 ln r2\\nii, where {rii}are the diagonal elements of\\nthe h ×h matrix R.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 369, 'page_label': '352'}, page_content='352 Exercises\\n6. Suppose that\\nU = [u0,u1,..., uh−1],\\nwhere all u ∈ Rn are column vectors and we have computed ( In + UU⊤)−1 via the QR\\nfactorization method in Exercise 5. If the columns of matrix U are updated to\\n[u1,..., uh−1,uh],\\nshow that the inverse ( In + UU⊤)−1 can be updated in O(h n) time (rather than computed\\nfrom scratch in O(h2 n) time). Deduce that the computing cost of updating the Hessian\\napproximation (9.10) is the same as that for the limited-memory BFGS Algorithm 9.4.3.\\nIn your solution you may use the following facts from [29]. Suppose we are given the\\nQ and R factors in the QR factorization of a matrix A ∈Rn×h. If a row/column is added to\\nmatrix A, then the Q and R factors need not be recomputed from scratch (in O(h2 n) time),\\nbut can be updated e fficiently in O(h n) time. Similarly, if a row/column is removed from\\nmatrix A, then the Q and R factors can be updated in O(h2) time.\\n7. Suppose that U ∈Rn×h has its k-th column v replaced with w, giving the updated eU.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 369, 'page_label': '352'}, page_content='7. Suppose that U ∈Rn×h has its k-th column v replaced with w, giving the updated eU.\\n(a) If e ∈Rh denotes the unit-length vector such that ek = ∥e∥= 1 and\\nr±:=\\n√\\n2\\n2 U⊤(w −v) +\\n√\\n2 ∥w −v∥2\\n4 e ±\\n√\\n2\\n2 e,\\nshow that\\neU⊤eU = U⊤U + r+r⊤\\n+ −r−r⊤\\n−.\\n[Hint: You may find Exercise 16 in Chapter 6 useful.]☞247\\n(b) Let B := (Ih + U⊤U)−1. Use the Woodbury identity (A.15) to show that☞371\\n(In + eUeU⊤)−1 = In −eU\\n\\x10\\nB−1 + r+r⊤\\n+ −r−r⊤\\n−\\n\\x11−1 eU⊤.\\n(c) Suppose that we have stored B in computer memory. Use Algorithm 6.8.1 and parts\\n(a) and (b) to write pseudo-code that updates (In+UU⊤)−1 to (In+eUeU⊤)−1 in O((n+h)h)\\ncomputing time.\\n8. Equation (9.7) gives the rank-two BFGS update of the inverse Hessian Ct−1 to Ct. In-\\nstead of using a two-rank update, we can consider a one-rank update, in which Ct−1 is\\nupdated to Ct by the general rank-one formula:\\nCt = Ct−1 + υt rt r⊤\\nt .\\nFind values for the scalar υt and vector rt, such that Ct satisfies the secant condition Ct1t =\\nδt.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 369, 'page_label': '352'}, page_content='Find values for the scalar υt and vector rt, such that Ct satisfies the secant condition Ct1t =\\nδt.\\n9. Show that the BFGS formula (B.23) can be written as:\\nC ←\\n\\x10\\nI −υ1δ⊤\\x11⊤\\nC\\n\\x10\\nI −υ1δ⊤\\x11\\n+ υδδ⊤,\\nwhere υ:= (1⊤δ)−1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 370, 'page_label': '353'}, page_content='Deep Learning 353\\n10. Show that the BFGS formula (B.23) is the solution to the constrained optimization\\nproblem:\\nCBFGS = argmin\\nA subject to A1= δ,A = A⊤\\nD(0,C |0,A),\\nwhere Dis the Kullback–Leibler discrepancy defined in (B.22). On the other hand, show\\nthat the DFP formula (B.24) is the solution to the constrained optimization problem:\\nCDFP = argmin\\nA subject to A1= δ,A = A⊤\\nD(0,A |0,C).\\n11. Consider again the logistic regression model in Exercise 5.18, which used iterative ☞ 213\\nreweighted least squares for training the learner. Repeat all the computations, but this\\ntime using thelimited-memory BFGS Algorithm 9.4.4. Which training algorithm converges\\nfaster to the optimal solution?\\n12. Download the seeds_dataset.txtdata set from the book’s GitHub site, which con-\\ntains 210 independent examples. The categorical output (response) here is the type of wheat\\ngrain: Kama, Rosa, and Canadian (encoded as 1, 2, and 3), so that c = 3. The seven con-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 370, 'page_label': '353'}, page_content='grain: Kama, Rosa, and Canadian (encoded as 1, 2, and 3), so that c = 3. The seven con-\\ntinuous features (explanatory variables) are measurements of the geometrical properties of\\nthe grain (area, perimeter, compactness, length, width, asymmetry coe fficient, and length\\nof kernel groove). Thus, x ∈R7 (which does not include the constant feature 1) and the\\nmulti-logit pre-classifier in Example 9.2 can be written as g(x) = softmax(Wx + b), where ☞ 329\\nW ∈R3×7 and b ∈R3. Implement and train this pre-classifier on the firstn = 105 examples\\nof the seeds data set using, for example, Algorithm 9.4.1. Use the remaining n′= 105\\nexamples in the data set to estimate the generalization risk of the learner using the cross-\\nentropy loss. [Hint: Use the cross-entropy loss formulas from Example 9.4.]\\n13. In Exercise 12 above, we train the multi-logit classifier using a weight matrixW ∈R3×7\\nand bias vectorb ∈R3. Repeat the training of the multi-logit model, but this time keepingz1'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 370, 'page_label': '353'}, page_content='and bias vectorb ∈R3. Repeat the training of the multi-logit model, but this time keepingz1\\nas an arbitrary constant (say z1 = 0), and thus setting c = 0 to be a “reference” class. This\\nhas the e ffect of removing a node from the output layer of the network, giving a weight\\nmatrix W ∈R2×7 and bias vector b ∈R2 of smaller dimensions than in (7.16). ☞ 267\\n14. Consider again Example 9.4, where we used a softmax output function SL in con- ☞ 334\\njunction with the cross-entropy loss: C(θ) = −ln gy+1(x |θ).Find formulas for ∂C\\n∂g and ∂SL\\n∂zL\\n.\\nHence, verify that:\\n∂SL\\n∂zL\\n∂C\\n∂g = g(x |θ) −ey+1,\\nwhere ei is the unit length vector with an entry of 1 in the i-th position.\\n15. Derive the formula (B.25) for a diagonal Hessian update in a quasi-Newton method ☞ 412\\nfor minimization. In other words, given a current minimizer xt of f (x), a diagonal matrix\\nC of approximating the Hessian of f , and a gradient vector u = ∇f (xt), find the solution\\nto the constrained optimization program:\\nmin\\nA'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 370, 'page_label': '353'}, page_content='to the constrained optimization program:\\nmin\\nA\\nD(xt,C |xt −Au,A)\\nsubject to: A1⩾δ, A is diagonal,\\nwhere Dis the Kullback–Leibler distance defined in (B.22) (see Exercise 4).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 371, 'page_label': '354'}, page_content='354 Exercises\\n16. Consider again the Python implementation of the polynomial regression in Sec-\\ntion 9.5.1, where the stochastic gradient descent was used for training.\\nUsing the polynomial regression data set, implement and run the following four altern-\\native training methods:\\n(a) the steepest-descent Algorithm 9.4.1;\\n(b) the Levenberg–Marquardt Algorithm B.3.3, in conjunction with Algorithm 9.4.2 for☞415\\ncomputing the matrix of Jacobi;\\n(c) the limited-memory BFGS Algorithm 9.4.4;\\n(d) the Adam Algorithm 9.4.5, which uses past gradient values to determine the next\\nsearch direction.\\nFor each training algorithm, using trial and error, tune any algorithmic parameters so that\\nthe network training is as fast as possible. Comment on the relative advantages and disad-\\nvantages of each training/optimization method. For example, comment on which optimiz-\\nation method makes rapid initial progress, but gets trapped in a suboptimal solution, and'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 371, 'page_label': '354'}, page_content='ation method makes rapid initial progress, but gets trapped in a suboptimal solution, and\\nwhich method is slower, but more consistent in finding good optima.\\n17. Consider again the Pytorch code in Section 9.5.2. Repeat all the computations, but\\nthis time using the momentum method for training of the network. Comment on which\\nmethod is preferable: the momentum or the Adam method?'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 372, 'page_label': '355'}, page_content='APPENDIX A\\nLINEAR ALGEBRA AND FUNCTIONAL\\nANALYSIS\\nThe purpose of this appendix is to review some important topics in linear algebra\\nand functional analysis. We assume that the reader has some familiarity with matrix\\nand vector operations, including matrix multiplication and the computation of determ-\\ninants.\\nA.1 Vector Spaces, Bases, and Matrices\\nLinear algebra is the study of vector spaces and linear mappings. Vectors are, by defini-\\ntion, elements of some vector space vector spaceVand satisfy the usual rules of addition and scalar\\nmultiplication, e.g.,\\nif x ∈V and y ∈V, then αx + βy ∈V for all α,β ∈R (or C).\\nWe will be dealing mostly with vectors in the Euclidean vector space Rn for some n. That\\nis, we view the points of Rn as objects that can be added up and multiplied with a scalar,\\ne.g., (x1,x2) + (y1,y2) = (x1 + y1,x2 + y2) for points in R2. Sometimes it is convenient to\\nwork with the complex vector space Cn instead of Rn; see also Section A.3.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 372, 'page_label': '355'}, page_content='work with the complex vector space Cn instead of Rn; see also Section A.3.\\nVectors v1,..., vk are called linearly independent linearly\\nindependent\\nif none of them can be expressed as\\na linear combination of the others; that is, if α1v1 + ··· + αnvn = 0, then it must hold that\\nαi = 0 for all i = 1,..., n.\\nDefinition A.1: Basis of a Vector Space\\nA set of vectors B= {v1,..., vn}is called a basis basisof the vector space Vif every\\nvector x ∈V can be written as a unique linear combination of the vectors in B:\\nx = α1 v1 + ··· + αn vn.\\nThe (possibly infinite) number n is called the dimension dimensionof V.\\n355'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 373, 'page_label': '356'}, page_content='356 Vector Spaces, Bases, and Matrices\\nUsing a basis Bof V, we can thus represent each vector x ∈V as a row or column of\\nnumbers\\n[α1,...,α n] or\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nα1\\n...\\nαn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (A.1)\\nTypically, vectors in Rn are represented via the standard basisstandard basis , consisting of unit\\nvectors (points) e1 = (1,0,..., 0),..., en = (0,0,..., 0,1). As a consequence, any point\\n(x1,..., xn) ∈Rn can be represented, using the standard basis, as a row or column vec-\\ntor of the form (A.1) above, with αi = xi,i = 1,..., n. We will also write [x1,x2,..., xn]⊤,\\nfor the corresponding column vector, where ⊤denotes the transposetranspose .\\nTo avoid confusion, we will use the convention from now on that a generic vectorx\\nis always represented via the standard basis as a column vector. The corresponding\\nrow vector is denoted by x⊤.\\nA matrix can be viewed as an array of m rows and n columns that defines a linearmatrix'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 373, 'page_label': '356'}, page_content='A matrix can be viewed as an array of m rows and n columns that defines a linearmatrix\\ntransformation from Rn to Rm (or for complex matrices, from Cn to Cm). The matrix is saidlinear\\ntransformation to be square if m = n. If a1,a2,..., an are the columns of A, that is, A = [a1,a2,..., an],\\nand if x = [x1,..., xn]⊤, then Ax = x1 a1 + ··· + xn an. In particular, the standard basis\\nvector ek is mapped to the vectorak, k = 1,..., n. We sometimes use the notationA = [ai j],\\nto denote a matrix whose (i, j)-th element is ai j. When we wish to emphasize that a matrix\\nA is real-valued withm rows and n columns, we write A ∈Rm×n. The rankrank of a matrix is the\\nnumber of linearly independent rows or, equivalently, the number of linearly independent\\ncolumns.\\nExample A.1 (Linear Transformation) Take the matrix\\nA =\\n\" 1 1\\n−0.5 −2\\n#\\n.\\nIt transforms the two basis vectors [1,0]⊤and [0,1]⊤, shown in red and blue in the left panel'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 373, 'page_label': '356'}, page_content='#\\n.\\nIt transforms the two basis vectors [1,0]⊤and [0,1]⊤, shown in red and blue in the left panel\\nof Figure A.1, to the vectors [1 ,−0.5]⊤ and [1,−2]⊤, shown on the right panel. Similarly,\\nthe points on the unit circle are transformed to an ellipse.\\n-1 -0.5 0 0.5 1\\nx\\n-1\\n-0.5\\n0\\n0.5\\n1\\ny\\n-2 0 2\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\nFigure A.1: A linear transformation of the unit circle.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 374, 'page_label': '357'}, page_content='Linear Algebra and Functional Analysis 357\\nSuppose A = [a1,..., an], where the A= {ai}form a basis of Rn. Take any vector x =\\n[x1,..., xn]⊤\\nE with respect to the standard basis E(we write subscript Eto stress this). Then\\nthe representation of this vector with respect to Ais simply\\ny = A−1 x,\\nwhere A−1 is the inverse inverseof A; that is, the matrix such that AA−1 = A−1A = In, where\\nIn is the n-dimensional identity matrix. To see this, note that A−1 ai gives the i-th unit\\nvector representation, for i = 1,..., n, and recall that each vector in Rn is a unique linear\\ncombination of these basis vectors.\\nExample A.2 (Basis Representation) Consider the matrix\\nA =\\n\"1 2\\n3 4\\n#\\nwith inverse A−1 =\\n\"−2 1\\n3/2 −1/2\\n#\\n. (A.2)\\nThe vector x = [1,1]⊤\\nE in the standard basis has representation y = A−1 x = [−1,1]⊤\\nAin the\\nbasis consisting of the columns of A. Namely,\\nAy = −\\n\"1\\n3\\n#\\n+\\n\"2\\n4\\n#\\n=\\n\"1\\n1\\n#\\n.\\nThe transpose transposeof a matrix A = [ai j] is the matrixA⊤= [aji]; that is, the (i, j)-th element'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 374, 'page_label': '357'}, page_content='of A⊤ is the ( j,i)-th element of A. The trace traceof a square matrix is the sum of its diagonal\\nelements. A useful result is the following cyclic property.\\nTheorem A.1: Cyclic Property\\nThe trace is invariant under cyclic permutations: tr(ABC) = tr(BCA) = tr(CAB).\\nProof: It suffices to show that tr( DE) is equal to tr( ED) for any m ×n matrix D = [di j]\\nand n ×m matrix E = [ei j]. The diagonal elements of DE are Pn\\nj=1 di j eji,i = 1,..., m and\\nthe diagonal elements of ED are Pm\\ni=1 eji di j, j = 1,..., n. They sum up to the same numberPm\\ni=1\\nPn\\nj=1 di j eji. □\\nA square matrix has an inverse if and only if its columns (or rows) are linearly in-\\ndependent. This is the same as the matrix being of full rank; that is, its rank is equal to\\nthe number of columns. An equivalent statement is that its determinant is not zero. The\\ndeterminant determinantof an n ×n matrix A = [ai,j] is defined as\\ndet(A) :=\\nX\\nπ\\n(−1)ζ(π)\\nnY\\ni=1\\naπi,i, (A.3)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 374, 'page_label': '357'}, page_content='det(A) :=\\nX\\nπ\\n(−1)ζ(π)\\nnY\\ni=1\\naπi,i, (A.3)\\nwhere the sum is over all permutations π= (π1,...,π n) of (1,..., n), and ζ(π) is the num-\\nber of pairs ( i, j) for which i < j and πi > πj. For example, ζ(2,3,4,1) = 3 for the pairs\\n(1,4),(2,4),(3,4). The determinant of a diagonal matrix diagonal matrix— a matrix with only zero ele-\\nments off the diagonal — is simply the product of its diagonal elements.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 375, 'page_label': '358'}, page_content='358 Vector Spaces, Bases, and Matrices\\nGeometrically, the determinant of a square matrix A = [a1,..., an] is the (signed)\\nvolume of the parallelepiped ( n-dimensional parallelogram) defined by the columns\\na1,..., an; that is, the set of points x = Pn\\ni=1 αi ai, where 0 ⩽αi ⩽1,i = 1,..., n.\\nThe easiest way to compute a determinant of a general matrix is to apply simple op-\\nerations to the matrix that potentially reduce its complexity (as in the number of non-zero\\nelements, for example), while retaining its determinant:\\n• Adding a multiple of one column (or row) to another, does not change the determin-\\nant.\\n• Multiplying a column (or row) with a number multiplies the determinant by the same\\nnumber.\\n• Swapping two rows changes the sign of the determinant.\\nBy applying these rules repeatedly one can reduce any matrix to a diagonal matrix.\\nIt follows then that the determinant of the original matrix is equal to the product of the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 375, 'page_label': '358'}, page_content='It follows then that the determinant of the original matrix is equal to the product of the\\ndiagonal elements of the resulting diagonal matrix multiplied by a known constant.\\nExample A.3 (Determinant and Volume) Figure A.2 illustrates how the determinant\\nof a matrix can be viewed as a signed volume, which can be computed by repeatedly apply-\\ning the first rule above. Here, we wish to compute the area of red parallelogram determined\\nby the matrix A given in (A.2). In particular, the corner points of the parallelogram corres-\\npond to the vectors [0,0]⊤,[1,3]⊤,[2,4]⊤, and [3,7]⊤.\\n0 0.5 1 1.5 2 2.5 3\\n-2\\n0\\n2\\n4\\n6\\n8\\nFigure A.2: The volume of the red parallelogram can be obtained by a number of shear\\noperations that do not change the volume.\\nAdding −2 times the first column of A to the second column gives the matrix\\nB =\\n\"1 0\\n3 −2\\n#\\n,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 376, 'page_label': '359'}, page_content='Linear Algebra and Functional Analysis 359\\ncorresponding to the blue parallelogram. The linear operation that transforms the red to the\\nblue parallelogram can be thought of as a succession of two linear transformations. The\\nfirst is to transform the coordinates of points on the red parallelogram (in standard basis)\\nto the basis formed by the columns of A. Second, relative to this new basis, we apply the\\nmatrix B above. Note that the input of this matrix is with respect to the new basis, whereas\\nthe output is with respect to the standard basis. The matrix for the combined operation is\\nnow\\nBA−1 =\\n\"1 0\\n3 −2\\n# \"−2 1\\n3/2 −1/2\\n#\\n=\\n\"−2 1\\n−9 4\\n#\\n,\\nwhich maps [1 ,3]⊤ to [1,3]⊤ (does not change) and [2 ,4]⊤ to [0,−2]⊤. We say that we\\napply a shear shearin the direction [1,3]⊤. The significance of such an operation is that a shear\\ndoes not alter the volume of the parallelogram . The second (blue) parallelogram has an'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 376, 'page_label': '359'}, page_content='does not alter the volume of the parallelogram . The second (blue) parallelogram has an\\neasier form, because one of the sides is parallel to the y-axis. By applying another shear,\\nin the direction [0,−2]⊤, we can obtain a simple (green) rectangle, whose volume is 2. In\\nmatrix terms, we add 3/2 times the second column of B to the first column of B, to obtain\\nthe matrix\\nC =\\n\"1 0\\n0 −2\\n#\\n,\\nwhich is a diagonal matrix, whose determinant is −2, corresponding to the volume 2 of all\\nthe parallelograms.\\nTheorem A.2 summarizes a number of useful matrix rules for the concepts that we have\\ndiscussed so far. We leave the proofs, which typically involves “writing out” the equations,\\nas an exercise for the reader; see also [116].\\nTheorem A.2: Useful Matrix Rules\\n1. ( AB)⊤= B⊤A⊤\\n2. ( AB)−1 = B−1A−1\\n3. ( A−1)⊤= (A⊤)−1 =: A−⊤\\n4. det( AB) = det(A) det(B)\\n5. x⊤Ax = tr \\x00Axx⊤\\x01\\n6. det( A) = Q\\ni aii if A = [ai j] is triangular'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 376, 'page_label': '359'}, page_content='4. det( AB) = det(A) det(B)\\n5. x⊤Ax = tr \\x00Axx⊤\\x01\\n6. det( A) = Q\\ni aii if A = [ai j] is triangular\\nNext, consider an n ×p matrix A for which the matrix inverse fails to exist. That is, A\\nis either non-square ( n , p) or its determinant is 0. Instead of the inverse, we can use its\\nso-called pseudo-inverse, which always exists.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 377, 'page_label': '360'}, page_content='360 Inner Product\\nDefinition A.2: Moore–Penrose Pseudo-Inverse\\nThe Moore–Penrose pseudo-inverseMoore–Penrose\\npseudo-inverse\\nof a real matrix A ∈Rn×p is defined as the\\nunique matrix A+ ∈Rp×n that satisfies the conditions:\\n1. AA+A = A\\n2. A+AA+ = A+\\n3. ( AA+)⊤= AA+\\n4. ( A+A)⊤= A+A\\nWe can write A+ explicitly in terms of A when A has a full column or row rank. For\\nexample, we always have\\nA⊤AA+ = A⊤(AA+)⊤= ((AA+)A)⊤= (A)⊤= A⊤. (A.4)\\nIf A has a full column rank p, then ( A⊤A)−1 exists, so that from (A.4) it follows that\\nA+ = (A⊤A)−1A⊤. This is referred to as the left pseudo-inverseleft\\npseudo-inverse\\n, as A+A = Ip. Similarly, if\\nA has a full row rank n, that is, (AA⊤)−1 exists, then it follows from\\nA+AA⊤= (A+A)⊤A⊤= (A(A+A))⊤= A⊤\\nthat A+ = A⊤(AA⊤)−1. This is the right pseudo-inverseright\\npseudo-inverse\\n, as AA+ = In. Finally, ifA is of full\\nrank and square, then A+ = A−1.\\nA.2 Inner Product\\nThe (Euclidean) inner productinner product of two real vectors x = [x1,..., xn]⊤ and y = [y1,..., yn]⊤'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 377, 'page_label': '360'}, page_content='is defined as the number\\n⟨x,y⟩=\\nnX\\ni=1\\nxi yi = x⊤y.\\nHere x⊤y is the matrix multiplication of the (1 ×n) matrix x⊤ and the ( n ×1) matrix y.\\nThe inner product induces a geometry on the linear spaceRn, allowing for the definition of\\nlength, angle, and so on. The inner product satisfies the following properties:\\n1. ⟨αx + βy,z⟩= α⟨x,z⟩+ β⟨y,z⟩;\\n2. ⟨x,y⟩= ⟨y,x⟩;\\n3. ⟨x,x⟩⩾0;\\n4. ⟨x,x⟩= 0 if and only if x = 0.\\nVectors x and y are called perpendicular (or orthogonalorthogonal ) if ⟨x,y⟩= 0. The Euclidean\\nnormEuclidean norm (or length) of a vector x is defined as\\n||x||=\\nq\\nx2\\n1 + ··· + x2\\nn =\\np\\n⟨x,x⟩.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 378, 'page_label': '361'}, page_content='Linear Algebra and Functional Analysis 361\\nIf x and y are perpendicular, then Pythagoras’ theorem Pythagoras’\\ntheorem\\nholds:\\n||x + y||2 = ⟨x + y, x + y⟩= ⟨x,x⟩+ 2 ⟨x,y⟩+ ⟨y,y⟩= ||x||2 + ||y||2. (A.5)\\nA basis {v1,..., vn}of Rn in which all the vectors are pairwise perpendicular and have\\nnorm 1 is called an orthonormal orthonormal(short for orthogonal and normalized) basis. For example,\\nthe standard basis is orthonormal.\\nTheorem A.3: Orthonormal Basis Representation\\nIf {v1,..., vn}is an orthonormal basis ofRn, then any vector x ∈Rn can be expressed\\nas\\nx = ⟨x,v1⟩v1 + ··· + ⟨x,vn⟩vn. (A.6)\\nProof: Observe that, because the {vi}form a basis, there exist unique α1,...,α n such that\\nx = α1v1 + ··· + αnvn. By the linearity of the inner product and the orthonormality of the\\n{vi}it follows that ⟨x,vj⟩= ⟨P\\ni αivi,vj⟩= αj. □\\nAn n ×n matrix V whose columns form an orthonormal basis is called an orthogonal\\nmatrix orthogonal\\nmatrix\\n.1 Note that for an orthogonal matrix V = [v1,..., vn], we have\\nV⊤V ='),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 378, 'page_label': '361'}, page_content='matrix orthogonal\\nmatrix\\n.1 Note that for an orthogonal matrix V = [v1,..., vn], we have\\nV⊤V =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nv⊤\\n1\\nv⊤\\n2\\n...\\nv⊤\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n[v1,v2,..., vn] =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nv⊤\\n1 v1 v⊤\\n1 v2 ... v⊤\\n1 vn\\n... ... ... ...\\nv⊤\\nn v1 v⊤\\nn v2 ... v⊤\\nn vn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb = In.\\nHence, V−1 = V⊤. Note also that an orthogonal transformation is length preserving length\\npreserving\\n; that\\nis, Vx has the same length as x. This follows from\\n||Vx||2 = ⟨Vx,Vx⟩= x⊤V⊤Vx = x⊤x = ||x||2.\\nA.3 Complex Vectors and Matrices\\nInstead of the vector space Rn of n-dimensional real vectors, it is sometimes useful to\\nconsider the vector space Cn of n-dimensional complex vectors. In this case the adjoint adjoint\\nor conjugate transpose operation (∗) replaces the transpose operation ( ⊤). This involves\\nthe usual transposition of the matrix or vector with the additional step that any complex\\nnumber z = x + i y is replaced by its complex conjugate z = x −i y. For example, if\\nx =\\n\"a1 + i b1\\na2 + i b2\\n#\\nand A ='),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 378, 'page_label': '361'}, page_content='x =\\n\"a1 + i b1\\na2 + i b2\\n#\\nand A =\\n\"a11 + i b11 a12 + i b12\\na21 + i b21 a22 + i b22\\n#\\n,\\nthen\\nx∗= [a1 −i b1, a2 −i b2] and A∗=\\n\"a11 −i b11 a21 −i b21\\na12 −i b12 a22 −i b22\\n#\\n.\\n1The qualifier “orthogonal” for such matrices has been fixed by history. A better term would have been\\n“orthonormal”.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 379, 'page_label': '362'}, page_content='362 Orthogonal Projections\\nThe (Euclidean) inner product of x and y (viewed as column vectors) is now defined as\\n⟨x,y⟩= y∗x =\\nnX\\ni=1\\nxi yi,\\nwhich is no longer symmetric: ⟨x,y⟩= ⟨y,x⟩. Note that this generalizes the real-valued\\ninner product. The determinant of a complex matrix A is defined exactly as in (A.3). As a\\nconsequence, det(A∗) = det(A).\\nA complex matrix is said to be Hermitian or self-adjoint if A∗= A, and unitary ifHermitian\\nunitary A∗A = I (that is, if A∗ = A−1). For real matrices “Hermitian” is the same as “symmetric”,\\nand “unitary” is the same as “orthogonal”.\\nA.4 Orthogonal Projections\\nLet {u1,..., uk}be a set of linearly independent vectors in Rn. The set\\nV= Span {u1,..., uk}= {α1u1 + ··· + αkuk, α1,...,α k ∈R},\\nis called the linear subspace spanned by {u1,..., uk}. The orthogonal complement of V,linear subspace\\northogonal\\ncomplement\\ndenoted by V⊥, is the set of all vectors w that are orthogonal to V, in the sense that'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 379, 'page_label': '362'}, page_content='complement\\ndenoted by V⊥, is the set of all vectors w that are orthogonal to V, in the sense that\\n⟨w,v⟩= 0 for all v ∈V. The matrix P such that Px = x, for all x ∈V, and Px = 0, for all\\nx ∈V⊥is called the orthogonal projection matrixorthogonal\\nprojection\\nmatrix\\nonto V. Suppose that U = [u1,..., uk]\\nhas full rank, in which case U⊤U is an invertible matrix. The orthogonal projection matrix\\nP onto V= Span {u1,..., uk}is then given by\\nP = U(U⊤U)−1U⊤.\\nNamely, since PU = U, the matrix P projects any vector in Vonto itself. Moreover, P\\nprojects any vector in V⊥onto the zero vector. Using the pseudo-inverse, it is possible to\\nspecify the projection matrix also for the case where U is not of full rank, leading to the\\nfollowing theorem.\\nTheorem A.4: Orthogonal Projection\\nLet U = [u1,..., uk]. Then, the orthogonal projection matrix P onto V= Span{u1,\\n..., uk}is given by\\nP = U U+, (A.7)\\nwhere U+ is the (right) pseudo-inverse of U.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 379, 'page_label': '362'}, page_content='..., uk}is given by\\nP = U U+, (A.7)\\nwhere U+ is the (right) pseudo-inverse of U.\\nProof: By Property 1 of Definition A.2 we have PU = UU+U = U, so that P projects any\\nvector in Vonto itself. Moreover, P projects any vector in V⊥onto the zero vector. □\\nNote that in the special case where u1,..., uk above form an orthonormal basis of V,\\nthen the projection onto Vis very simple to describe, namely we have\\nPx = UU⊤x =\\nkX\\ni=1\\n⟨x,ui⟩ui. (A.8)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 380, 'page_label': '363'}, page_content='Linear Algebra and Functional Analysis 363\\nFor any point x ∈Rn, the point in Vthat is closest to x is its orthogonal projection Px,\\nas the following theorem shows.\\nTheorem A.5: Orthogonal Projection and Minimal Distance\\nLet {u1,..., uk}be an orthonormal basis of subspace Vand let P be the orthogonal\\nprojection matrix onto V. The solution to the minimization program\\nmin\\ny∈V\\n∥x −y∥2\\nis y = Px. That is, Px ∈V is closest to x.\\nProof: We can write each point y ∈V as y = Pk\\ni=1 αi ui. Consequently,\\n∥x −y∥2 =\\n\\x1c\\nx −\\nkX\\ni=1\\nαi ui, x −\\nkX\\ni=1\\nαi ui\\n\\x1d\\n= ∥x∥2 −2\\nkX\\ni=1\\nαi ⟨x,ui⟩+\\nkX\\ni=1\\nα2\\ni .\\nMinimizing this with respect to the {αi}gives αi = ⟨x,ui⟩,i = 1,..., k. In view of (A.8),\\nthe optimal y is thus Px. □\\nA.5 Eigenvalues and Eigenvectors\\nLet A be an n ×n matrix. If Av = λv for some number λand non-zero vector v, then λis\\ncalled an eigenvalue of A with eigenvector v. eigenvalue\\neigenvectorIf (λ,v) is an (eigenvalue, eigenvector) pair, the matrix λI −A maps any multiple of v'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 380, 'page_label': '363'}, page_content='eigenvectorIf (λ,v) is an (eigenvalue, eigenvector) pair, the matrix λI −A maps any multiple of v\\nto the zero vector. Consequently, the columns of λI −A are linearly dependent, and hence\\nits determinant is 0. This provides a way to identify the eigenvalues, namely as the r ⩽n\\ndifferent roots λ1,...,λ r of the characteristic polynomial characteristic\\npolynomial\\ndet(λI −A) = (λ−λ1)α1 ··· (λ−λr)αr ,\\nwhere α1 + ··· + αr = n. The integer αi is called the algebraic multiplicity of λi. The algebraic\\nmultiplicityeigenvectors that correspond to an eigenvalueλi lie in the kernel or null space of the matrix\\nnull spaceλiI −A; that is, the linear space of vectors v such that (λiI −A)v = 0. This space is called\\nthe eigenspace of λi. Its dimension, di ∈{1,..., n}, is called the geometric multiplicity of geometric\\nmultiplicityλi. It always holds that di ⩽αi. If P\\ni di = n, then we can construct a basis for Rn consisting\\nof eigenvectors, as illustrated next.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 380, 'page_label': '363'}, page_content='i di = n, then we can construct a basis for Rn consisting\\nof eigenvectors, as illustrated next.\\nExample A.4 (Linear Transformation (cont.)) We revisit the linear transformation in\\nFigure A.1, where\\nA =\\n\" 1 1\\n−1/2 −2\\n#\\n.\\nThe characteristic polynomial is ( λ−1)(λ+ 2) + 1/2, with roots λ1 = −1/2 −\\n√\\n7/2 ≈\\n−1.8229 and λ2 = −1/2 +\\n√\\n7/2 ≈0.8229. The corresponding unit eigenvectors are v1 ≈\\n[0.3339,−0.9426]⊤ and v2 ≈[0.9847,−0.1744]⊤. The eigenspace corresponding to λ1 is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 381, 'page_label': '364'}, page_content='364 Eigenvalues and Eigenvectors\\nV1 = Span {v1}= {βv1 : β∈R}and the eigenspace corresponding to λ2 is V2 = Span {v2}.\\nThe algebraic and geometric multiplicities are 1 in this case. Any pair of vectors taken\\nfrom V1 and V2 forms a basis for R2. Figure A.3 shows how v1 and v2 are transformed to\\nAv1 ∈V1 and Av2 ∈V2, respectively.\\n-2 0 2\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\nFigure A.3: The dashed arrows are the unit eigenvectorsv1 (blue) and v2 (red) of matrix A.\\nTheir transformed values Av1 and Av2 are indicated by solid arrows.\\nA matrix for which the algebraic and geometric multiplicities of all its eigenvalues\\nare the same is called semi-simple. This is equivalent to the matrix being diagonalizable,semi-simple\\ndiagonalizable meaning that there is a matrix V and a diagonal matrix D such that\\nA = VDV−1.\\nTo see that this so-called eigen-decompositioneigen-\\ndecomposition\\nholds, suppose A is a semi-simple matrix\\nwith eigenvalues\\nλ1,...,λ 1|     {z     }\\nd1\\n,··· ,λr,...,λ r|     {z     }\\ndr\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 381, 'page_label': '364'}, page_content='with eigenvalues\\nλ1,...,λ 1|     {z     }\\nd1\\n,··· ,λr,...,λ r|     {z     }\\ndr\\n.\\nLet D be the diagonal matrix whose diagonal elements are the eigenvalues of A, and let V\\nbe a matrix whose columns are linearly independent eigenvectors corresponding to these\\neigenvalues. Then, for each (eigenvalue, eigenvector) pair (λ,v), we have Av = λv. Hence,\\nin matrix notation, we have A V= VD, and so A = VDV−1.\\nA.5.1 Left- and Right-Eigenvectors\\nThe eigenvector as defined in the previous section is called aright-eigenvector, as it lies on\\nthe right of A in the equation Av = λv.\\nIf A is a complex matrix with an eigenvalueλ, then the eigenvalue’s complex conjugate\\nλ is an eigenvalue of A∗. To see this, define B := λI −A and B∗ := λI −A∗. Since λ is\\nan eigenvalue, we have det( B) = 0. Applying the identity det( B) = det(B∗), we see that'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 382, 'page_label': '365'}, page_content='Linear Algebra and Functional Analysis 365\\ntherefore det(B∗) = 0, and hence that λ is an eigenvalue of A∗. Let w be an eigenvector\\ncorresponding to λ. Then, A∗w = λw or, equivalently,\\nw∗A = λw∗.\\nFor this reason, we call w∗ the left-eigenvector left-\\neigenvector\\nof A for eigenvalue λ. If v is a (right-) ei-\\ngenvector of A, then its adjoint v∗ is usually not a left-eigenvector, unless A∗A = AA∗\\n(such matrices are called normal; normal matrixa real symmetric matrix is normal). However, the im-\\nportant property holds that left- and right-eigenvectors belonging to different eigenvalues\\nare orthogonal. Namely, ifw∗is a left-eigenvalue ofλ1 and v a right-eigenvalue ofλ2 , λ1,\\nthen\\nλ1w∗v = w∗Av = λ2w∗v,\\nwhich can only be true if w∗v = 0.\\nTheorem A.6: Schur Triangulation\\nFor any complex matrix A, there exists a unitary matrix U such that T = U−1AU is\\nupper triangular.\\nProof: The proof is by induction on the dimension n of the matrix. Clearly, the statement'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 382, 'page_label': '365'}, page_content='Proof: The proof is by induction on the dimension n of the matrix. Clearly, the statement\\nis true for n = 1, as A is simply a complex number and we can take U equal to 1. Suppose\\nthat the result is true for dimension n. We wish to show that it also holds for dimension\\nn + 1. Any matrix A always has at least one eigenvalue λwith eigenvector v, normalized\\nto have length 1. Let U be any unitary matrix whose first column is v. Such a matrix can\\nalways be constructed2. As U is unitary, the first row ofU−1 is v∗, and U−1AU is of the form\\n\" v∗\\n∗\\n#\\nA\\nh\\nv ∗\\ni\\n|   {z   }\\nU\\n=\\n\" λ ∗\\n0 B\\n#\\n,\\nfor some matrix B. By the induction hypothesis, there exists a unitary matrix W and an\\nupper triangular matrix T such that W−1BW = T. Now, define\\nV :=\\n\" 1 0⊤\\n0 W\\n#\\n.\\nThen,\\nV−1 \\x10\\nU−1AU\\n\\x11\\nV =\\n\" 1 0⊤\\n0 W−1\\n#\" λ ∗\\n0 B\\n#\" 1 0⊤\\n0 W\\n#\\n=\\n\" λ ∗\\n0 W−1BW\\n#\\n=\\n\" λ ∗\\n0 T\\n#\\n,\\nwhich is upper triangular of dimension n + 1. Since UV is unitary, this completes the\\ninduction, and hence the result is true for all n. □'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 382, 'page_label': '365'}, page_content='induction, and hence the result is true for all n. □\\nThe theorem above can be used to prove an important property of Hermitian matrices,\\ni.e., matrices for which A∗= A.\\n2After specifying v we can complete the rest of the unitary matrix via the Gram–Schmidt procedure, for\\nexample; see Section A.6.4.'),\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a73b03",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b6450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embedding = OllamaEmbeddings(model = \"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb558bc",
   "metadata": {},
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c728a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vectordb = Chroma.from_documents(documents = final_docs,embedding = embedding , persist_directory= \"./Chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1e9288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19315faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa12bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
